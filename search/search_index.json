{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GEOG3300 and AGRI3003 - Semester 1 2025","text":""},{"location":"#welcome","title":"Welcome","text":"<p>This is the online course book for Advanced Spatial Analysis (3300) and Decisions from Data in Agriculture (3003).</p> <p>There are rapid changes in data science and technology/sensors that are driving change across the environmental and agricultural sectors. This unit focuses on student learning through practical and applied laboratories, using Python, and collecting field data using uncrewed aerial vehicles (UAVs). Students will develop skills to manipulate, transform, analyse, and visualise big spatial and non-spatial data to provide solutions to the big landscape and agricultural questions.</p> <p>This course will teach students foundational skills and knowledge required to implement spatial and non-spatial data analysis workflows programmatically, to handle and analyse \u201cbig\u201d data, and to operate in cloud-based environments.</p> <p>There are three overarching themes are:</p> <ul> <li> <p>Programming: analysis doesn\u2019t scale without code, programming skills are required to use many software packages and tools for spatial data analysis.</p> </li> <li> <p>Big data: various sensors are generating databases that are growing in size and variety. Increasingly specialist software and data formats are required to handle big data.</p> </li> <li> <p>Cloud-based environments: it\u2019s easier to move the question to the data than to make big data mobile. Data and computer processing are often located on remote computers - we need to know how to get there. </p> </li> </ul>"},{"location":"#computer-labs","title":"Computer labs","text":"<p>The material and concepts will be delivered in computer workshops. In the first session, the instructor will introduce the week's content and explain key concepts. Then for the reminder of the first session and the second session students will work through practical examples and practice exercises in a Jupyter notebook environment with support from instructors. </p>"},{"location":"course-outline/","title":"Course outline","text":""},{"location":"course-outline/#week-1","title":"Week 1","text":"<p>This week's lab will provide an introduction to programming concepts in Python:</p> <ul> <li>Python programs</li> <li>data types and structures</li> <li>variables</li> <li>control flow, loops, and conditional execution</li> <li>classes and objects</li> </ul> <p>It will demonstrate these concepts by:</p> <p>1) creating a Python program that runs as an AI-powered farm assistant using a large language model.</p> <p> </p> <p>2) running a short Python program that loads crop yield data collected by harvesters from disk and visualises this data on interactive maps.</p> <p> </p> <p>3) practice exercises working with Python data types, data structures and control flow.</p> <p> </p> <p>4) practice exercises that demonstrates how to setup Google Colab to run large language models via Hugging Face.</p> <p> </p>"},{"location":"course-outline/#week-2","title":"Week 2","text":"<p>These labs introduce data visualisation concepts, selecting different figures to explore a dataset's structure and relationships between variables, using colour to highlight features and patterns, and generating maps from spatial data. You will be working with crop yield data sampled by a header on a harvester from a field in the Western Australia and satellite images from the same field. </p> <p>You do not need a GPU instance in Google Colab for these labs.</p> <p>1) introduction to data visualisation and creating interactive figures.</p> <p> </p> <p>2) geospatial data visualisation of raster and vector data.</p> <p> </p> <p>3) practice exercises generating interactive visualisations using Plotly Express from an Our World in Data article: To protect the world\u2019s wildlife we must improve crop yields \u2013 especially across Africa. </p> <p> </p> <p>4) practice exercises reading GeoTIFF files storing Sentinel-2 images captured near York, Western Australia in a mix of cloudy and clear sky conditions. Create a new Google Colab notebook and exectute the following code snippet to read the files. </p> <pre><code>import os\nimport subprocess\n\nif \"data_lab-2_geotiffs_practice\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_geotiffs_practice.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-2_geotiffs_practice.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_geotiffs_practice\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-2_geotiffs_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</code></pre>"},{"location":"course-outline/#week-3","title":"Week 3","text":"<p>These labs will provide an introduction to file formats for tabular, vector, and raster data; best practice for reading and writing spatial and non-spatial data in Python programs; navigating directories and file systems; and, identifying suitable file formats for different tasks, use cases, contexts (e.g. big data / web environments). You will be working with a range of datasets including satellite images, climate reanalysis products, and crop yield data collected by a harvester from a field in Western Australia.</p> <p>You do not need a GPU instance in Google Colab for these labs.</p> <p>1) data I/O and file formats.</p> <p> </p> <p>2) geospatial data I/O.</p> <p> </p> <p>3) practice exercises reading and writing vector data to different formats working with flood maps generated by the European Commission Copernicus Emergency Management Service (EMS) - Rapid Mapping Activations.</p> <p> </p> <p>4) demonstration of partial reads from cloud native geospatial file formats.</p> <p> </p>"},{"location":"course-outline/#week-4","title":"Week 4","text":"<p>These labs focus on data wrangling and data transformation. You will implement a range of data transformation and geoprocessing operations including map algebra, spatial and non-spatial data subsetting, zonal statistics, and combining datasets. One lab will focus on raster data and build a data processing workflow to convert satellite images into flood maps following a tropical cyclone event in Fiji. The second lab will turn a satellite image into a vector-tabular dataset of field boundaries with a crop type label and field-average spectral reflectance values. This demonstrates how we can process satellite images into a format ready for machine learning model development. </p> <p>You do not need a GPU instance in Google Colab for these labs.</p> <p>1) data wrangling: raster data.</p> <p> </p> <p>2) data wrangling: vector and tabular data</p> <p> </p> <p>3) practice exercises performing a range of geometry operations to generate zones for zonal statistics operations to compute summary statistics describing areas flooded and not flooded after a tropical cyclone in event in a satellite image. </p> <p> </p>"},{"location":"course-outline/#week-5","title":"Week 5","text":"<p>An introduction to machine learning concepts and workflows, how to implement a machine learning workflow in Python, and considerations for machine learning with spatial data. You will implement two machine learning tasks. First, you will use the AgriFieldNet Competition Dataset\u00a0(Radiant Earth Foundation and IDinsight, 2022)\u00a0dataset to develop a crop type classifier for fields in India using Sentinel-2 remote sensing data. Second, you will develop a model to predict maize crop yields in Uganda with Sentinel-2 remote sensing data using the replication data from\u00a0Lobell et al.\u00a0(2019).</p> <p>You do not need a GPU instance in Google Colab for these labs.</p> <p>1) crop type classification</p> <p> </p> <p>2) crop yield prediction</p> <p> </p>"},{"location":"course-outline/#week-6","title":"Week 6","text":"<p>An introduction to computer-to-computer communication, client-server architectures, HTTP requests / responses, and web APIs. You will learn how to use Python to make requests to web APIs, use web APIs to download data into their programs, and scrape data from websites. You apply these skills to make a weather data application for a field in Western Australia that sources data from an open weather web API. You will also learn how to use the SpatioTemporal Asset Catalog (STAC) specification and STAC APIs to define and automate searches for geospatial data that meets certain query conditions from various cloud providers.</p> <p>You do not need a GPU instance in Google Colab for these labs.</p> <p>1) web APIs, client-server architectures, and cloud computing</p> <p> </p> <p>2) SpatioTemporal Asset Collections - retrieving geospatial data from the cloud</p> <p> </p>"},{"location":"course-outline/#week-7","title":"Week 7","text":"<p>An introduction to web mapping technologies, highlighting their strengths and limitations for particular contexts, and providing some recipes for requesting web map data using Python.</p> <p> </p>"},{"location":"course-outline/#week-8","title":"Week 8","text":"<p>An introduction to using UAV images and demonstrating the process to generate science-ready UAV data for answering agricultural questions.</p> <p> </p>"},{"location":"course-outline/#week-9","title":"Week 9","text":"<p>An introduction to analysing high-resolution UAV data over a cotton trial in New South Wales.</p> <p> </p>"},{"location":"ai/example-uses/","title":"Example uses of AI tools","text":"<p>These are some examples of prompts and ways to use AI tools. They are designed to give you ideas of how to use AI tools to support and enhance your studies. </p>"},{"location":"ai/example-uses/#ai-for-gis","title":"AI for GIS","text":"<p>Use the LLM to help you identify GIS operations to solve geospatial problems</p> <p>Prompt</p> <p>I am a geospatial analyst working for a precision agriculture technology consultancy. I have two drone images of the same field captured on different dates. How can I co-register these images for accurate within-field change analysis? </p> <p>Can you consider the strengths and weaknesses of different co-registration methods and outline software tools that support these operations?</p> <p>Use thinking models</p> <p>This task requires identifying different co-registration methods and reasoning about their strengths and weakness. \"Thinking\" models which implement chain-of-thought reasoning and reflect on generated responses are useful for these comparative tasks. The model's \"thinking\" process will provide a slower and more thoughtful consideration of each co-registration method. </p> Response (Gemini 2.0 Flash Thinking) <p>Co-registering drone images captured on different dates is crucial for accurate within-field change analysis in precision agriculture. Misalignment can lead to false change detection, skewing your analysis of vegetation health, growth patterns, or treatment effectiveness. Let's explore the methods, their strengths and weaknesses, and relevant software tools.</p> <p>Co-registration Methods for Drone Imagery</p> <p>Co-registration aims to spatially align two or more images so that corresponding pixels represent the same ground location. For drone images, especially those of agricultural fields, several methods are applicable, broadly categorized as:</p> <ol> <li> <p>Ground Control Point (GCP) Based Co-registration:</p> </li> <li> <p>Method Description: This is often considered the most accurate and reliable method, especially when high precision is required. It relies on accurately measured ground control points (GCPs) visible in both drone images. GCPs are physical markers placed in the field with known coordinates (typically obtained using a survey-grade GPS or RTK). During image processing, these GCPs are identified in each image and used to geometrically transform one image to match the spatial reference of the other.</p> </li> </ol> <p>Strengths:</p> <ul> <li>High Accuracy: When GCPs are accurately surveyed and well-distributed, this method provides the highest possible geometric accuracy, crucial for precise change detection.</li> <li>Reliability: GCPs provide absolute spatial reference, making the co-registration robust and less susceptible to image distortions or content changes.</li> <li>Control over Transformation: You can select the type of transformation (e.g., rigid, affine, polynomial) based on the expected distortions and field characteristics.</li> </ul> <p>Weaknesses:</p> <ul> <li>Labor-Intensive: Requires physical placement and accurate surveying of GCPs, which can be time-consuming and expensive. Field Access Required: Necessitates access to the field on both image acquisition dates to establish or verify GCP locations (if permanent GCPs are not used).</li> <li>GCP Visibility: GCPs must be clearly visible in both drone images, which may require careful marker design and placement, and could be obstructed by vegetation in later image dates.</li> <li> <p>Not always practical for large areas or repeat visits: Placing and surveying GCPs for every image acquisition, especially for large fields or frequent monitoring, can become logistically challenging.</p> </li> <li> <p>Feature-Based (Image-to-Image) Co-registration:</p> </li> <li> <p>Method Description: This method directly aligns images based on automatically detected and matched features within the images themselves. It doesn't rely on external GCPs. Common techniques include:</p> </li> <li>Feature Detection and Matching: Algorithms like SIFT (Scale-Invariant Feature Transform), SURF (Speeded-Up Robust Features), or ORB (Oriented FAST and Rotated BRIEF) are used to identify distinctive features (keypoints) in both images. These features are then matched based on their descriptors.</li> <li>Transformation Estimation: Once enough feature matches are found, a geometric transformation (e.g., affine, projective) is estimated that best aligns the matched features between the images. Robust estimators (like RANSAC) are often used to minimize the influence of outlier matches.</li> <li>Image Resampling: One image is then resampled and warped according to the estimated transformation to align with the reference image.</li> </ul> <p>Strengths:</p> <ul> <li>GCP-Free: Eliminates the need for GCPs, significantly reducing fieldwork and time. Automated: The process is largely automated, making it more efficient for large datasets.</li> <li>Suitable for areas without GCPs: Applicable even if GCPs were not collected during image acquisition or are impractical.</li> <li>Can adapt to image content changes: Feature-based methods can sometimes be more resilient to slight changes in scene content (e.g., vegetation growth) compared to methods solely relying on initial georeferencing.</li> </ul> <p>Weaknesses:</p> <ul> <li>Lower Accuracy (generally): Typically less accurate than GCP-based methods, as image features are not as spatially precise as surveyed GCPs. Accuracy depends heavily on image quality, feature distinctiveness, and the quality of feature matching.</li> <li>Potential for Misregistration: In areas with repetitive patterns, textureless surfaces, or significant changes between image dates (e.g., major crop growth, tillage), feature matching can be unreliable, leading to misregistration.</li> <li>Transformation Limitations: Feature-based methods often assume a global transformation model, which might not perfectly correct for localized distortions present in drone imagery (especially from lower-cost drones or in windy conditions).</li> <li> <p>Parameter Tuning: Feature detection and matching algorithms often have parameters that need to be tuned for optimal performance based on image characteristics.</p> </li> <li> <p>Sensor Model/Direct Georeferencing Based Co-registration:</p> </li> <li> <p>Method Description: Modern drones often incorporate high-precision GPS/GNSS and Inertial Measurement Units (IMUs) for direct georeferencing. This means the drone's position and orientation are recorded directly during flight. If both datasets were processed using direct georeferencing (or PPK/RTK post-processing), the images should ideally be already well-aligned based on the sensor model and trajectory data. Co-registration in this case becomes more about fine-tuning.</p> </li> </ul> <p>Strengths:</p> <ul> <li>Fast and Efficient: If direct georeferencing is accurate, co-registration can be very fast as the initial alignment is already good.</li> <li>Reduced or Eliminated GCPs (potentially): High-accuracy direct georeferencing (like PPK/RTK) can significantly reduce or even eliminate the need for GCPs in some cases, though GCPs are still recommended for validation and to further improve accuracy, especially for large projects or demanding accuracy requirements. Good Starting Point: Provides a good initial geometric correction, simplifying further refinement if needed.</li> </ul> <p>Weaknesses:</p> <ul> <li>Accuracy Dependent on Sensor Quality: The accuracy is directly tied to the quality of the drone's GPS/IMU system and the processing method (direct georeferencing vs. PPK/RTK). Lower-cost drones might have less accurate sensors.</li> <li>Atmospheric and Environmental Factors: GPS signals can be affected by atmospheric conditions and obstructions. Still may require refinement: Even with direct georeferencing, there can be residual distortions and misalignments. Fine-tuning using feature-based methods or minimal GCPs might still be beneficial for very high-accuracy change analysis.</li> <li>Reliability of Initial Processing: Relies on the accuracy and quality of the initial photogrammetric processing steps that generated the orthomosaics or orthophotos in the first place.</li> </ul> <p>Choosing the Right Method:</p> <p>The best co-registration method depends on several factors:</p> <ul> <li>Required Accuracy: For very precise within-field change analysis (e.g., centimeter-level accuracy), GCP-based methods are generally preferred. For less stringent accuracy requirements or preliminary assessments, feature-based methods or direct georeferencing refinement might suffice.</li> <li>Availability of GCPs: If GCPs were collected and accurately surveyed, GCP-based methods are a strong choice. If not, you'll need to rely on feature-based or sensor model approaches.</li> <li>Drone and Sensor Quality: Drones with high-precision GPS/IMU systems and processed with PPK/RTK offer better initial georeferencing, potentially reducing the need for extensive co-registration.</li> <li>Image Quality and Content: Image quality (resolution, sharpness, radiometry) and the presence of distinct features significantly impact the success of feature-based methods. Changes in vegetation or scene content between dates can also affect feature matching.</li> <li>Time and Resource Constraints: GCP-based methods are more time and resource-intensive. Feature-based and direct georeferencing refinement are typically faster and require less fieldwork.</li> </ul> <p>Software Tools for Co-registration</p> <p>Many geospatial software packages support image co-registration. Here are some prominent examples, categorized by type:</p> <p>GIS and Remote Sensing Software:</p> <p>ArcGIS (Esri):</p> <ul> <li>Georeferencing Toolset: Robust tools for GCP-based georeferencing, including various transformation models, residual analysis, and accuracy assessment.</li> <li>Image Analysis Extension: Supports feature-based image registration using techniques like automatic tie point generation and image warping.</li> <li>Workflow: Can integrate GCP-based and feature-based methods.</li> <li>Pros: Widely used in geospatial industry, comprehensive tools, excellent visualization and analysis capabilities.</li> <li>Cons: Commercial software, can be expensive, complex interface for some users.</li> </ul> <p>QGIS (Open Source):</p> <ul> <li>Georeferencer GDAL plugin: For GCP-based georeferencing, supports various transformations, allows GCP management, and accuracy assessment.</li> <li>Plugins like \"Feature Alignment\" or scripting with Python/GDAL/OpenCV: Can be used to implement feature-based co-registration (more advanced, might require programming skills).</li> <li>Pros: Free and open source, versatile, large community support, integrates well with other geospatial libraries.</li> <li>Cons: Feature-based tools might require more manual setup and scripting compared to commercial software, potentially steeper learning curve for complex tasks.</li> </ul> <p>ENVI (Harris Geospatial Solutions):</p> <ul> <li>Image Registration Module: Dedicated module for both GCP-based and feature-based image registration. Offers automatic tie point generation, various transformation models, and accuracy metrics.</li> <li>Pros: Specialized for remote sensing image analysis, powerful image processing capabilities, good for handling large datasets.</li> <li>Cons: Commercial software, can be expensive, interface might be less intuitive for general GIS tasks compared to ArcGIS.</li> </ul> <p>ERDAS IMAGINE (Hexagon Geospatial):</p> <ul> <li>Geometric Correction Module: Provides tools for GCP-based georeferencing and image registration, including various transformation options and accuracy reporting.</li> <li>Auto Sync Module: Supports automated feature-based image registration.</li> <li>Pros: Long-standing remote sensing software with strong image processing and photogrammetry capabilities.</li> <li>Cons: Commercial software, can be expensive, interface can be perceived as older and less modern compared to some newer software.</li> </ul> <p>Photogrammetry Software (Often used for drone data processing):</p> <p>Agisoft Metashape (formerly PhotoScan):</p> <ul> <li>Alignment Step: As part of the photogrammetric workflow, Metashape inherently performs image alignment, which can be refined with GCPs or tie points. It's excellent at feature-based alignment for orthomosaic generation and can be used for co-registration of already orthorectified images.</li> <li>Pros: Widely used for drone data processing, strong feature-based alignment capabilities, good for generating orthomosaics and DEMs.</li> <li>Cons: Commercial software, primarily focused on 3D reconstruction workflow, dedicated co-registration tools might be less extensive compared to pure GIS/RS software for specific registration tasks.</li> </ul> <p>Pix4Dmapper/Pix4Dmatic:</p> <ul> <li>Similar to Metashape: Pix4D products also perform robust image alignment as part of their processing pipeline. Can be used for co-registration and refinement with GCPs or manual tie points.</li> <li>Pros: Popular drone data processing software, good for large datasets, Pix4Dmatic is designed for faster processing of very large datasets.</li> <li>Cons: Commercial software, primarily focused on 3D reconstruction workflow, dedicated co-registration tools might be within the broader processing flow rather than standalone tools.</li> </ul> <p>Programming Libraries (For Custom Solutions and Automation):</p> <p>GDAL/OGR (Geospatial Data Abstraction Library):</p> <ul> <li>Command-line tools and libraries (C++, Python, etc.): Powerful for geospatial data manipulation, including raster warping and transformation. Can be used to implement GCP-based and feature-based co-registration workflows through scripting.</li> <li>Pros: Open source, very versatile, widely used in geospatial development, excellent for batch processing and automation.</li> <li>Cons: Requires programming skills, might have a steeper learning curve for users without programming background, requires assembling workflows from individual functions.</li> </ul> <p>OpenCV (Open Source Computer Vision Library): * Python, C++, Java interfaces: Extensive library for computer vision tasks, including feature detection, feature matching, and image transformations. Can be used to build custom feature-based co-registration workflows. * Pros: Open source, highly optimized for computer vision tasks, large community, very flexible for custom algorithm development. * Cons: Requires programming skills, focused on computer vision primitives, requires more effort to build complete geospatial workflows compared to dedicated GIS/RS software.</p> <p>Workflow Outline for Co-registration and Change Analysis:</p> <p>Pre-processing:</p> <ul> <li>Radiometric Calibration: Ensure both images are radiometrically calibrated to minimize the effects of sensor variations and atmospheric conditions on change detection.</li> <li>Orthorectification: If your images are not already orthorectified, process them to generate orthomosaics/orthophotos. This is crucial for accurate geometric alignment and spatial measurements.</li> <li>Resolution Matching (Optional): If the images have significantly different resolutions, you might consider resampling the higher-resolution image to match the lower resolution to avoid artifacts during change analysis.</li> <li>Choose Co-registration Method: Select the most appropriate method based on accuracy requirements, data availability (GCPs), drone/sensor quality, and time constraints (as discussed earlier).</li> </ul> <p>Perform Co-registration:</p> <p>GCP-Based:</p> <ul> <li>Load GCP coordinates into the software.</li> <li>Identify and mark GCPs in both images.</li> <li>Select a suitable transformation model.</li> <li>Run the georeferencing/co-registration tool.</li> </ul> <p>Feature-Based:</p> <ul> <li>Select feature detection and matching algorithms in the software.</li> <li>Run the automatic feature matching process.</li> <li>Review and refine matches (remove outliers if necessary).</li> <li>Select a transformation model and run the image registration tool.</li> </ul> <p>Sensor Model Refinement:</p> <p>If direct georeferencing was used initially, load both orthomosaics into software. Consider using feature-based refinement or adding a few well-distributed check points (even without full GCP survey) to fine-tune the alignment.</p> <p>Accuracy Assessment:</p> <ul> <li>Visual Inspection: Overlay the co-registered images and visually inspect for misalignment, especially in areas with distinct features. \"Flicker\" or \"swipe\" tools in GIS software are helpful for this.</li> </ul> <p>Quantitative Accuracy Assessment:</p> <ul> <li>RMS Error (RMSE): If using GCPs or check points, examine the Root Mean Square Error (RMSE) reported by the software. Lower RMSE values indicate better accuracy.</li> <li>Check Points (Independent): If possible, use independent check points (locations not used in the co-registration process) to assess accuracy.</li> <li>Feature Overlay Analysis: Examine the alignment of features like field boundaries, roads, or drainage ditches after co-registration.</li> <li>Change Analysis: Once you are satisfied with the co-registration accuracy:</li> </ul> <p>Perform your desired change analysis: This could involve:</p> <ul> <li>Band Math/Vegetation Indices Differencing: Calculate differences in NDVI, other vegetation indices, or individual bands to highlight changes in vegetation health, biomass, etc.</li> <li>Image Classification Change Detection: Classify land cover types in both images and analyze changes in classes over time.</li> <li>Object-Based Change Detection: Identify and track changes in individual objects within the field.</li> <li>Interpretation and Validation:  Interpret the change analysis results in the context of agricultural practices, environmental conditions, and your specific research questions. Validate your findings with field observations or other data sources if possible.</li> </ul> <p>Key Considerations for Precision Agriculture:</p> <ul> <li>Within-Field Variability: Agricultural fields often exhibit high within-field variability. Accurate co-registration is essential to ensure that change analysis truly reflects temporal changes and not just spatial misalignment.</li> <li>Sub-Pixel Accuracy: For many precision agriculture applications, achieving sub-pixel co-registration accuracy is desirable to minimize noise and maximize the reliability of change detection.</li> <li>Documentation: Document your co-registration method, parameters, accuracy assessment results, and software tools used for reproducibility and transparency.</li> </ul> <p>By carefully selecting the appropriate co-registration method, utilizing robust software tools, and rigorously assessing accuracy, you can effectively co-register your drone images and perform reliable within-field change analysis for precision agriculture applications. Remember to consider the strengths and weaknesses of each method in the context of your specific project requirements and data characteristics.</p> <p>Prompt</p> <p>I have a series of spatial points with crop yield measurements from a yield monitor on a harvester as a CSV file, a GeoTIFF file storing monthly NDVI values and GeoPackage file storing a field boundary. </p> <p>What GIS operations do I need to perform to select all the yield monitor points within the field and to extract the monthly NDVI time-series at each point.</p> <p>Help the model generate more accurate and relevant responses</p> <p>The more detail you provide the LLM in your prompt, the more closely the LLM will engage with the specifics of your task and generate a more relevant and useful response. </p> <p>The prompt above is a good start as it describes the formats of the input data. However, it could be improved by specifying what output format you'd like (e.g. structure of a table, visualisations or file format to save the data). You could also ask the model to provide a step by step explantion to ensure it considers each step in the workflow in-turn.</p> Response (Gemini 2.0 Flash) <p>Load Data</p> <ul> <li>Yield Monitor Data: Load the CSV file containing the yield monitor data into a GeoDataFrame. Ensure the data includes latitude and longitude columns or a geometry column representing the point locations.</li> <li>NDVI Data: Load the GeoTIFF file containing monthly NDVI values as a raster dataset.</li> <li>Field Boundary: Load the GeoPackage file and extract the field boundary polygon.</li> </ul> <p>Spatial Selection</p> <ul> <li>Reproject (if necessary): Ensure that the yield monitor data and the field boundary are in the same coordinate reference system (CRS). If not, reproject one of the datasets to match the other.</li> <li>Spatial Join: Perform a spatial join between the yield monitor points and the field boundary polygon. Use the 'intersects' or 'within' spatial predicate to select only the points that fall within the field boundary. This will create a new GeoDataFrame containing only the yield points within the field.</li> </ul> <p>Time-Series Extraction</p> <ul> <li>Reproject (if necessary): Ensure that the yield monitor points (now filtered to be within the field) and the NDVI raster are in the same CRS. If not, reproject one of the datasets to match the other.</li> <li>Extract NDVI Values: For each point in the filtered yield monitor data, extract the corresponding NDVI value from each monthly layer in the NDVI raster. This will create a new DataFrame or extend the existing GeoDataFrame with columns representing the NDVI values for each month.</li> </ul> <p>Python Code Example (using geopandas and rasterio)</p> <pre><code>import geopandas as gpd\nimport rasterio\n\n# Load data\nyield_data = gpd.read_file(\"yield_monitor.csv\")  # Replace with your file name\nndvi_raster = rasterio.open(\"monthly_ndvi.tif\")  # Replace with your file name\nfield_boundary = gpd.read_file(\"field_boundary.gpkg\", layer=\"field_layer\")  # Replace with your file name and layer name\n\n# Reproject if necessary (ensure yield_data and field_boundary have the same CRS)\nyield_data = yield_data.to_crs(field_boundary.crs)\n\n# Spatial join\nyield_within_field = gpd.sjoin(yield_data, field_boundary, how=\"inner\", predicate=\"intersects\")\n\n# Reproject if necessary (ensure yield_within_field and ndvi_raster have the same CRS)\nyield_within_field = yield_within_field.to_crs(ndvi_raster.crs)\n\n# Extract NDVI time-series\nfor i in range(1, ndvi_raster.count + 1):\n    ndvi_values = [ndvi_raster.read(i).item(int(x), int(y)) for x, y in zip(yield_within_field.geometry.x, yield_within_field.geometry.y)]\n    yield_within_field[f\"NDVI_month_{i}\"] = ndvi_values\n\n# Save or use the resulting GeoDataFrame\nprint(yield_within_field.head())\n</code></pre> <p>Explanation</p> <ul> <li>The code first loads the yield data, NDVI raster, and field boundary into appropriate data structures.</li> <li>It then reprojects the yield data to match the CRS of the field boundary (if necessary) and performs a spatial join to select points within the field.</li> <li>Next, it reprojects the filtered yield data to match the CRS of the NDVI raster (if necessary).</li> <li>It iterates through each band (month) in the NDVI raster, extracts the NDVI value at each point location, and adds it as a new column in the GeoDataFrame.</li> <li>The resulting GeoDataFrame contains the yield data for points within the field, along with their corresponding monthly NDVI values.</li> </ul> <p>Additional Considerations</p> <ul> <li>Point in Polygon: The spatial join operation effectively performs a \"point-in-polygon\" test to determine which points are inside the field boundary.</li> <li>Raster Value Extraction: The code extracts the NDVI value at the exact pixel location of each point. You can explore other extraction methods (e.g., bilinear interpolation) if needed.</li> <li>Data Cleaning: Check for missing or invalid values in both the yield data and the NDVI time-series.</li> <li>Analysis: Once you have the combined data, you can perform further analysis, such as correlating yield with NDVI trends or building predictive models.</li> </ul> <p>Prompt</p> <p>I have a dataset of tree locations represented as spatial points and field boundaries in a separate polygon layer. How can I count the number of trees within each field? </p> <p>Please provide a step by step explanation. </p> Response (Gemini 2.0 Flash) <p>You can count the number of trees within each field using a spatial join or intersection operation.  Here's a step-by-step explanation, applicable to most GIS software (like QGIS, ArcGIS, or PostGIS):</p> <p>Prepare your data:</p> <ul> <li>Tree Points: Ensure your tree locations are in a point layer (e.g., shapefile, GeoJSON, or a spatial database table). Each point should represent a single tree. The crucial part is that these points have a defined coordinate system (CRS).</li> <li>Field Polygons: Make sure your field boundaries are in a polygon layer (e.g., shapefile, GeoJSON, or a spatial database table). Each polygon represents a single field. This layer must be in the same coordinate system as your tree point layer. If they aren't, you'll need to reproject one of the layers to match the other before proceeding. Consistent CRS is absolutely essential for accurate spatial analysis.</li> </ul> <p>Perform the Spatial Join (or Intersection):</p> <p>This is the core step. The specific tool name might vary slightly depending on your GIS software, but the concept is the same.  You're essentially asking the GIS to determine which trees fall within which fields.</p> <p>QGIS: Use the \"Join attributes by location\" tool (found in the Vector menu under \"Spatial join\").</p> <ul> <li>Target layer: Your field polygons.</li> <li>Join layer: Your tree points.</li> <li>Join type: \"One to many\" (because one field can contain multiple trees).</li> <li>Output layer: A new polygon layer. This new layer will have all the attributes of your original field polygons, plus information about the trees within each field (specifically, the count).</li> </ul> <p>ArcGIS: Use the \"Spatial Join\" tool.</p> <ul> <li>Target Features: Your field polygons.</li> <li>Join Features: Your tree points.</li> <li>Join Type: \"One to Many\".</li> <li>Match Option: \"Within\" (or \"Contains,\" depending on how you define \"within\").</li> <li>Output Feature Class: A new feature class (polygon layer).</li> </ul> <p>PostGIS (SQL): Use a spatial query with the ST_Contains or ST_Intersects function. Here's a basic example:</p> <pre><code>CREATE TABLE field_tree_counts AS\nSELECT\n    fields.field_id,  -- Replace with your field's unique ID column\n    fields.field_name, -- Replace with your field's name column (optional)\n    COUNT(trees.tree_id) AS tree_count -- Replace with your tree's unique ID column\nFROM\n    fields_table AS fields, -- Replace with your fields table name\n    trees_table AS trees  -- Replace with your trees table name\nWHERE\n    ST_Contains(fields.geom, trees.geom) -- Or ST_Intersects(fields.geom, trees.geom)\nGROUP BY\n    fields.field_id, fields.field_name; -- Group by field ID and name\n</code></pre> <p>View the Results:</p> <p>The output from the spatial join will be a new polygon layer. This layer will have the attributes of your original field polygons. Crucially, it will also have a new attribute (usually named something like tree_count or COUNT) that represents the number of trees within each field.</p> <p>Open the attribute table of this new layer to see the counts.</p> <p>(Optional) Further Analysis:</p> <p>You can now use this new layer for further analysis, like calculating statistics (average tree density per field), visualizing the results (e.g., choropleth maps showing tree density), or exporting the data to other formats.</p> <p>Key Considerations:</p> <ul> <li>Coordinate Systems (CRS): Double-check that your tree and field data are in the same CRS. This is the most common source of errors in spatial analysis.</li> <li>\"Within\" vs. \"Contains\": In some GIS software, you might have the option of \"Within\" or \"Contains.\" \"Within\" means the tree's entire geometry must be within the field. \"Contains\" means the field contains the tree. Usually, \"Within\" is the more appropriate choice for this type of analysis.</li> <li>Data Cleaning: Before performing the spatial join, it's a good idea to check your data for any errors, like overlapping polygons or invalid geometries. Cleaning your data can prevent unexpected results.</li> <li>Performance: If you have a very large dataset, the spatial join can be computationally intensive. Consider using spatial indexing or optimizing your query (if using a spatial database) to improve performance.</li> </ul> <p>By following these steps, you should be able to accurately count the number of trees within each field in your dataset.  Remember to adapt the instructions slightly depending on the specific GIS software you are using.</p> <p>Describe a geospatial problem and your datasets and instruct the LLM to give you detailed instructions on how to solve the problem using desktop GIS (e.g. QGIS / ArcGIS)</p> <p>Prompt</p> <p>I have a series of spatial points with crop yield measurements from a yield monitor on a harvester as a CSV file, a GeoTIFF file storing monthly NDVI values and GeoPackage file storing a field boundary. I need to select all the yield monitor points within the field and to extract the monthly NDVI time-series at each point. </p> <p>Can you provide detailed step-by-step instructions to do this using QGIS?</p> Response (Gemini 2.0 Flash) <p>Here's a detailed step-by-step guide on how to select yield points within a field boundary and extract monthly NDVI time series at each point using QGIS:</p> <p>Load Your Data:</p> <p>Yield Points (CSV):</p> <ul> <li>Open QGIS.</li> <li>Go to Layer -&gt; Add Layer -&gt; Add Delimited Text Layer.</li> <li>Browse to your CSV file.</li> <li>Ensure the correct delimiter (comma in your case) is selected.</li> <li>Specify the X and Y fields (longitude and latitude).</li> <li>Set the CRS (Coordinate Reference System) to match your other data. This is crucial. If you're unsure, check the properties of your GeoTIFF or field boundary. Common CRS for agricultural data include UTM zones or geographic coordinates (WGS 84).</li> <li>Click Add. The points should now appear on the map.</li> </ul> <p>NDVI GeoTIFF:</p> <ul> <li>Go to Layer -&gt; Add Layer -&gt; Add Raster Layer.</li> <li>Browse to your GeoTIFF file.</li> <li>Click Add. The NDVI raster should now be displayed. It's likely that you will need to repeat this step for each monthly NDVI GeoTIFF you want to extract data from.</li> </ul> <p>Field Boundary (GeoPackage):</p> <ul> <li>Go to Layer -&gt; Add Layer -&gt; Add Vector Layer.</li> <li>Browse to your GeoPackage file.</li> <li>Select the field boundary layer.</li> <li>Click Add. The field boundary should now be visible.</li> </ul> <p>Select Yield Points within the Field:</p> <ul> <li>Go to Vector -&gt; Geoprocessing Tools -&gt; Clip.</li> <li>Input layer: Your yield points layer.</li> <li>Overlay layer: Your field boundary layer.</li> <li>Output layer: Choose a location and name for the new clipped points layer (e.g., yield_points_in_field.shp).</li> <li>Click Run. This will create a new point layer containing only the yield points that fall within the field boundary.</li> </ul> <p>Extract NDVI Values at Each Point (for one month):</p> <ul> <li>Go to Plugins -&gt; Manage and Install Plugins.</li> <li>Search for and install the \"Point sampling tool\" plugin if you don't have it already.</li> <li>Go to Plugins -&gt; Point sampling tool -&gt; Point sampling tool.</li> <li>Input point layer: Your clipped yield points layer (yield_points_in_field).</li> <li>Input raster layer: Your NDVI GeoTIFF (for the specific month).</li> <li>Output layer: Choose a location and name for the output (e.g., yield_points_ndvi_month1.shp).</li> <li>Click Run. This will create a new point layer. The attribute table of this new layer will contain the NDVI value (from the raster) at each point. The name of the new field may be the same as the raster file name.</li> </ul> <p>Repeat for all months:</p> <p>Repeat step 3 for each of your monthly NDVI GeoTIFF files. Make sure to create a new output file for each month (e.g., yield_points_ndvi_month2.shp, yield_points_ndvi_month3.shp, etc.).</p> <p>Combine Monthly Data (Optional but usually needed):</p> <p>You now have multiple point layers, each containing the NDVI values for a different month.  To create a single table with the time series data, you can use the \"Join attributes by location\" tool (Vector -&gt; Data Management Tools -&gt; Join attributes by location) or the \"Merge vector layers\" tool (Vector -&gt; Data Management Tools -&gt; Merge vector layers).  The latter is easier to use, but requires some data manipulation to get it into a time series format.</p> <p>Using \"Merge vector layers\" (Recommended):</p> <ul> <li>Merge all the monthly point layers into a single layer.</li> <li>Open the attribute table of the merged layer. You'll now have all the monthly NDVI values as different columns.</li> <li>Export the attribute table to a CSV file. This will be easier to work with for time series analysis.</li> <li>Using \"Join attributes by location\" (More complex):</li> <li>You can join each month's NDVI data to the original yield points layer. This will create separate layers for each month.</li> <li>You can then merge these layers or export the attribute tables to create your time series dataset.</li> </ul> <p>Time Series Analysis (Outside QGIS):</p> <p>Once you have your data in a CSV file, you'll likely want to perform time series analysis or visualization. You can use tools like R, Python (with libraries like Pandas and Matplotlib), or even Excel for this.</p> <p>Important Considerations:</p> <ul> <li>Coordinate Systems (CRS): Absolutely critical. Make sure all your layers (yield points, NDVI rasters, and field boundary) are in the same CRS. If they're not, you must reproject them before doing any spatial analysis.</li> <li>NDVI Raster Alignment: Ensure your monthly NDVI rasters align spatially. If they are slightly misaligned, you might get incorrect NDVI values at your points.</li> <li>Data Volume: If you have a very large dataset, these operations might take some time. Consider using a spatial database (like PostGIS) for better performance.</li> <li>QGIS Versions: The menu options might be slightly different depending on your QGIS version, but the general workflow should be the same.</li> </ul> <p>This comprehensive guide should help you extract the NDVI time series for your yield points. Remember to double-check your CRS at every step!  If you encounter specific errors, please provide the error message and details about your data, and I'll do my best to assist you further.</p> <p>Screen share with Google AI Studio and ask it questions on where to find tools and features in the QGIS user interface as you work in a desktop GIS </p> <p>Using Google AI Studio you can screen share QGIS with Gemini and verbally ask questions for help with completing tasks and navigating the user interface; for example, where to find processing tools and how to configure them for GIS operations.</p> <p>Screen share of QGIS with Google AI Studio to get help with merging polygons.</p>"},{"location":"ai/example-uses/#ai-for-programming-and-data-analysis","title":"AI for programming and data analysis","text":"<p>Use LLMs for programming from writing boilerplate code, generating examples and helping solve coding problems</p> <p>Prompt</p> <p>Can you write a Python function to merge three tables using a common ID column?</p> Response (Gemini 2.0 Flash) <pre><code>import pandas as pd\n\ndef merge_tables(table1, table2, table3, common_id):\n    \"\"\"\n    Merges three pandas DataFrames using a common ID column.\n\n    Args:\n        table1: The first pandas DataFrame.\n        table2: The second pandas DataFrame.\n        table3: The third pandas DataFrame.\n        common_id: The name of the common ID column (string).\n\n    Returns:\n        A new pandas DataFrame containing the merged data, or None if an error occurs.\n        Prints informative messages if an issue is encountered.\n    \"\"\"\n\n    try:\n        # Check if the common ID column exists in all tables\n        if common_id not in table1.columns or common_id not in table2.columns or common_id not in table3.columns:\n            print(f\"Error: Common ID column '{common_id}' not found in all tables.\")\n            return None\n\n        # Merge the tables\n        merged_table = pd.merge(table1, table2, on=common_id, how='outer')  # Outer join to keep all rows\n        merged_table = pd.merge(merged_table, table3, on=common_id, how='outer')\n\n        return merged_table\n\n    except Exception as e:\n        print(f\"An error occurred during merging: {e}\")\n        return None\n\n\n# Example Usage:\n# Create sample DataFrames (replace with your actual data)\ndata1 = {'ID': [1, 2, 3, 4], 'Value1': ['A', 'B', 'C', 'D']}\ntable1 = pd.DataFrame(data1)\n\ndata2 = {'ID': [2, 3, 4, 5], 'Value2': ['E', 'F', 'G', 'H']}\ntable2 = pd.DataFrame(data2)\n\ndata3 = {'ID': [1, 3, 5, 6], 'Value3': ['I', 'J', 'K', 'L']}\ntable3 = pd.DataFrame(data3)\n\ncommon_id_column = 'ID'\n\nmerged_data = merge_tables(table1, table2, table3, common_id_column)\n\nif merged_data is not None:\n    print(merged_data)  # Print the merged DataFrame\n    # You can now save it to a new file, perform further operations, etc.\n    # For example, to save to a CSV:\n    # merged_data.to_csv(\"merged_data.csv\", index=False)\n</code></pre> <p>Explanation and Improvements:</p> <ul> <li>Error Handling: The code includes a try...except block to catch potential errors during the merge process. This makes the function more robust. It also checks if the common_id column actually exists in all tables before attempting the merge.</li> <li>Clearer Error Messages: The print() statements provide more informative messages if there's an issue, making debugging easier.</li> <li>Outer Join: The how='outer' parameter in pd.merge() performs an outer join. This is crucial. An outer join ensures that all rows from all tables are included in the merged table. If a particular ID is missing in one of the tables, the corresponding columns for that table will have NaN (Not a Number) values. This prevents data loss.</li> <li>Example Usage: The example code demonstrates how to use the function with sample DataFrames. Replace these sample DataFrames with your actual data. It also shows how to save the result to a CSV file.</li> <li>Conciseness: The code is more concise and readable.</li> <li>Flexibility: The function is more flexible, as it can handle cases where one or more of the DataFrames might not contain all the IDs present in the others.</li> </ul> <p>How to Use with Your Data:</p> <p>Import pandas: </p> <pre><code>import pandas as pd\n</code></pre> <p>Read your tables: Use pd.read_csv(), pd.read_excel(), or other appropriate pandas functions to read your three tables into pandas DataFrames. For example:</p> <pre><code>table1 = pd.read_csv(\"table1.csv\")\ntable2 = pd.read_csv(\"table2.csv\")\ntable3 = pd.read_csv(\"table3.csv\")\n</code></pre> <p>Call the function:</p> <pre><code>merged_data = merge_tables(table1, table2, table3, \"ID\")  # Replace \"ID\" with your actual common ID column name\n</code></pre> <p>Handle the result:</p> <pre><code>if merged_data is not None:\n    print(merged_data)\n    merged_data.to_csv(\"merged_result.csv\", index=False) # Save to a CSV\n</code></pre> <p>Remember to replace \"table1.csv\", \"table2.csv\", \"table3.csv\", and \"ID\" with the actual paths to your files and the name of your common ID column.`</p> <p>Prompt</p> <p>Can you write Python code snippet to open a NetCDF file and print information about the file\u2019s contents?</p> <p>Use LLMs to help you work with different file formats</p> <p>A wide range of file formats are used to store and transfer geospatial and agricultural data (e.g. see the list of raster file formats supported by GDAL). It's not always obvious how data is stored in different file formats or how to read data from them. </p> <p>LLMs can be a useful tool to give you quick examples of how to read data from a new format and provide an introduction to working with file formats that might be more accessible than their technical documentation. </p> <p>Often, there is more than one way to read data from a file format. You could improve this prompt by asking the model to provide examples of different ways to open a NetCFD file and comment on their strengths and weaknesses. </p> Response (Gemini 2.0 Flash) <pre><code>import netCDF4  # Import the netCDF4 library\n\ndef print_netcdf_info(filepath):\n    \"\"\"Opens a NetCDF file and prints information about its contents.\n\n    Args:\n        filepath: The path to the NetCDF file (string).\n    \"\"\"\n\n    try:\n        # Open the NetCDF file in read mode ('r')\n        with netCDF4.Dataset(filepath, 'r') as nc_file:  # Use 'with' for automatic closing\n\n            print(f\"NetCDF file: {filepath}\")\n            print(\"=\" * 40)\n\n            # Print global attributes (metadata)\n            print(\"\\nGlobal Attributes:\")\n            for attr_name in nc_file.ncattrs():\n                print(f\"  {attr_name}: {nc_file.getncattr(attr_name)}\")\n\n            # Print dimensions\n            print(\"\\nDimensions:\")\n            for dim_name, dim in nc_file.dimensions.items():\n                print(f\"  {dim_name}: {len(dim)}\")\n\n            # Print variables\n            print(\"\\nVariables:\")\n            for var_name, var in nc_file.variables.items():\n                print(f\"  {var_name}:\")\n                print(f\"    Dimensions: {var.dimensions}\")\n                print(f\"    Shape: {var.shape}\")\n                print(f\"    Data type: {var.dtype}\")\n                print(f\"    Attributes:\")\n                for attr_name in var.ncattrs():\n                    print(f\"      {attr_name}: {var.getncattr(attr_name)}\")\n                # Print a few values (optional - for small variables only!)\n                if var.size &lt; 100:  # Avoid printing huge arrays\n                    print(f\"    Values: {var[:]}\")  # [:] gets all data\n\n    except FileNotFoundError:\n        print(f\"Error: NetCDF file not found at {filepath}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage:\nfilepath = \"your_netcdf_file.nc\"  # Replace with the actual path to your NetCDF file\nprint_netcdf_info(filepath)\n\n# Example of reading data from a specific variable:\ndef read_netcdf_variable(filepath, variable_name):\n    try:\n        with netCDF4.Dataset(filepath, 'r') as nc_file:\n            if variable_name in nc_file.variables:\n                variable_data = nc_file.variables[variable_name][:]  # Read all data\n                return variable_data\n            else:\n                print(f\"Variable '{variable_name}' not found in the NetCDF file.\")\n                return None\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Example usage of reading a variable\ntemperature_data = read_netcdf_variable(filepath, \"temperature\") # Replace \"temperature\" with your variable name\nif temperature_data is not None:\n    print(temperature_data.shape) # Print the shape of the data\n    print(temperature_data) # Print the data (optional)\n</code></pre> <p>Key Improvements and Explanations:</p> <ul> <li>netCDF4 Library: The code uses the netCDF4 library, which is the recommended way to work with NetCDF files in Python. You'll likely need to install it: pip install netCDF4</li> <li>Error Handling: The <code>try...except</code> block handles potential errors, such as the file not being found or other issues during file processing. This makes the code more robust.</li> <li><code>with</code> Statement: The <code>with netCDF4.Dataset(...) as nc_file:</code> construct ensures that the NetCDF file is automatically closed, even if errors occur. This is best practice.</li> <li>Clear Output: The code prints information about global attributes, dimensions, and variables in a well-formatted way. Variable Information: For each variable, it prints the dimensions, shape, data type, and attributes.</li> <li>Optional Data Printing: It includes an option to print a few values from the variables. However, this is only done if the variable is relatively small (less than 100 elements) to avoid printing huge arrays to the console.</li> <li>Example of reading a variable: The <code>read_netcdf_variable</code> function demonstrates how to read the data from a specific variable in a NetCDF file. This is how you would actually access the data for analysis.</li> <li>Comments: The code is well-commented to explain what each part does.</li> </ul> <p>How to Use:</p> <ul> <li>Install netCDF4: <code>pip install netCDF4</code></li> <li>Replace <code>your_netcdf_file.nc</code>: Change <code>\"your_netcdf_file.nc\"</code> to the actual path of your NetCDF file.</li> <li>Run the script: Execute the Python script. It will print information about the NetCDF file's contents.</li> <li>Access Variable Data: Use the read_netcdf_variable function to get the data for the variables you need. Remember to replace <code>\"temperature\"</code> with the actual name of the variable you want. You can then work with the data (e.g., plot it, perform calculations, etc.).</li> </ul> <p>Use LLMs to improve or optimise code</p> <p>Prompt</p> <p>From the perspective of a senior developer reviewing code, can you suggest how to improve the following Python function?</p> <p>The two variables <code>precip_jan_2024</code> and <code>precip_jan_2025</code> are 3D arrays of shape (time, X, Y) storing precipitation daily precipitation values. The code is trying to create monthly sums of precipitation for each pixel and then compute the change in precipitation between 2024 and 2025.</p> <pre><code>precip_jan_2024_sum = np.zeros((precip_jan_2024.shape[1], precip_jan_2024.shape[2]))\n\nfor r in range(0, precip_jan_2024.shape[1]):\n    for c in range(0, precip_jan_2024.shape[2]):\n        precip_jan_2024_sum[r, c] = np.sum(precip_jan_2024[:, r, c])\n\nprecip_jan_2025_sum = np.zeros((precip_jan_2025.shape[1], precip_jan_2025.shape[2]))\n\nfor r in range(0, precip_jan_2025.shape[1]):\n    for c in range(0, precip_jan_2025.shape[2]):\n        precip_jan_2025_sum[r, c] = np.sum(precip_jan_2025[:, r, c])\n\ndiff = np.zeros((precip_jan_2025_sum.shape[1], precip_jan_2025_sum.shape[2]))\n\nfor r in range(0, precip_jan_2024_sum.shape[1]):\n    for c in range(0, precip_jan_2024_sum.shape[2]):\n        diff[r, c] = precip_jan_2025_sum[r, c] - precip_jan_2024_sum[r, c]\n</code></pre> Response (Gemini 2.0 Flash) <pre><code>import numpy as np\n\ndef calculate_precipitation_change(precip_jan_2024, precip_jan_2025):\n    \"\"\"Calculates the change in monthly precipitation between two years.\n\n    Args:\n        precip_jan_2024: 3D numpy array (time, X, Y) of daily precipitation \n                        values for January 2024.\n        precip_jan_2025: 3D numpy array (time, X, Y) of daily precipitation \n                        values for January 2025.  Must have the same X and Y\n                        dimensions as precip_jan_2024.\n\n    Returns:\n        A 2D numpy array representing the difference in total monthly \n        precipitation between January 2025 and January 2024 for each \n        grid point (X, Y).  Returns None if the input arrays have incompatible\n        shapes.\n    \"\"\"\n\n    if precip_jan_2024.shape[1:] != precip_jan_2025.shape[1:]:  # Check X, Y dimensions\n        print(\"Error: Precipitation arrays have incompatible X and Y dimensions.\")\n        return None\n\n\n    precip_jan_2024_sum = np.sum(precip_jan_2024, axis=0)  # Sum along the time axis\n    precip_jan_2025_sum = np.sum(precip_jan_2025, axis=0)\n\n    diff = precip_jan_2025_sum - precip_jan_2024_sum\n\n    return diff\n\n\n# Example (replace with your actual data):\nprecip_2024 = np.random.rand(31, 10, 10)  # 31 days, 10x10 grid\nprecip_2025 = np.random.rand(31, 10, 10)\n\nprecipitation_diff = calculate_precipitation_change(precip_2024, precip_2025)\n\nif precipitation_diff is not None:\n    print(precipitation_diff)\n</code></pre> <p>Key Improvements and Explanations:</p> <ul> <li>Vectorization: The original code used nested loops to iterate over the X and Y dimensions. This is very inefficient in Python, especially with NumPy.  The improved code uses NumPy's vectorized operations (np.sum with the axis parameter) to perform the summations and difference calculation.  Vectorization is orders of magnitude faster.</li> <li>Function: Encapsulating the code within a function (calculate_precipitation_change) makes it reusable, more organized, and easier to test.</li> <li>Input Validation: The function now checks if the X and Y dimensions of the input arrays are compatible. This is a crucial step for robust code.  It returns None and prints an informative message if the shapes don't match.</li> <li>Clearer Variable Names:  Slightly more descriptive variable names (e.g., precip_jan_2024_sum instead of just sum2024) improve readability.</li> <li>Docstring: The function includes a docstring explaining its purpose, arguments, and return value. This is essential for maintainability and collaboration.</li> <li>Removed Redundant Initialization: The original code initialized precip_jan_2024_sum, precip_jan_2025_sum and diff with zeros. This is not needed when using np.sum with the axis argument.</li> <li>Direct Calculation: The difference is now calculated directly using NumPy's broadcasting.  This is another example of vectorization and is much more efficient than looping.</li> </ul> <p>Why Vectorization is Crucial:</p> <p>NumPy's vectorized operations are implemented in C and are highly optimized.  When you use loops in Python to perform numerical calculations on arrays, you're essentially iterating through the array elements in Python, which is slow.  Vectorization allows NumPy to perform the operations on the entire array (or a specified axis) at once, using optimized C code under the hood. The performance difference can be enormous, especially for large datasets.  Always try to vectorize your NumPy code whenever possible.</p> <p>Prompt</p> <p>From the perspective of a senior developer reviewing code, can you suggest how to optimise the following Python script? </p> <p>Can you explain how these optimisations work and are suitable for this program?</p> <pre><code>import pystac\nimport planetary_computer as pc\nimport rioxarray  # For easier raster handling\nimport xarray as xr\n\n# get Sentinel-2 images from Planetary Computer\ndef get_sentinel2_images_unoptimized(bbox, start_date, end_date, cloud_cover_max=30):\n    \"\"\"Retrieves Sentinel-2 images from Planetary Computer.\"\"\"\n    catalog = pystac.Catalog.from_file_stac_catalog(pc.CATALOG_URL)\n\n    items = catalog.search(\n        bbox=bbox,\n        datetime=[start_date, end_date],\n        collections=[\"sentinel-2-l2a\"],  # Or appropriate collection\n    ).items()\n\n    image_data = []  # List to hold image data (inefficient)\n\n    for item in items:\n        if item.properties[\"eo:cloud_cover\"] &lt;= cloud_cover_max:\n            asset = item.assets[\"image\"] # Get the image asset\n            href = asset.href\n            try:\n                # Open the raster data\n                raster = rioxarray.open_rasterio(href, masked=True).squeeze() # Squeeze to remove extra dimensions\n                image_data.append(raster) # Append to a list - inefficient!\n            except Exception as e:\n                print(f\"Error processing item {item.id}: {e}\")\n\n    if not image_data:\n        return None\n\n    combined_data = xr.concat(image_data, dim=\"time\") # Concatenate the list of xarray DataArrays\n\n    return combined_data\n</code></pre> Response (Gemini 2.0 Flash) <p>This code snippet retrieves Sentinel-2 images from the Planetary Computer using the pystac library for searching the STAC catalog and rioxarray for loading the images as xarray DataArrays. Here's an optimized version with explanations:</p> <pre><code>import pystac\nimport planetary_computer as pc\nimport rioxarray\nimport xarray as xr\n\ndef get_sentinel2_images_optimized(bbox, start_date, end_date, cloud_cover_max=30):\n    \"\"\"Retrieves Sentinel-2 images from Planetary Computer (optimized).\"\"\"\n\n    catalog = pystac.Catalog.from_file_stac_catalog(pc.CATALOG_URL)\n\n    search = catalog.search(\n        bbox=bbox,\n        datetime=[start_date, end_date],\n        collections=[\"sentinel-2-l2a\"],\n        query={\"eo:cloud_cover\": {\"lt\": cloud_cover_max}}  # Filter by cloud cover\n    )\n\n    items = search.get_items()  # Retrieve the items\n\n    # Use a generator expression for efficient filtering and data loading\n    image_data_generator = (\n        rioxarray.open_rasterio(item.assets[\"image\"].href, masked=True).squeeze()\n        for item in items\n    )\n\n    try:\n        # Concatenate directly from the generator (memory-efficient)\n        combined_data = xr.concat(image_data_generator, dim=\"time\")\n        return combined_data\n    except ValueError:  # Handle the case where no images are found\n        print(\"No Sentinel-2 images found matching the criteria.\")\n        return None\n    except Exception as e:  # Catch other potential errors during processing\n        print(f\"An error occurred: {e}\")\n        return None\n</code></pre> <p>Key Improvements and Explanations:</p> <ul> <li>Cloud Cover Filtering in STAC Query: The optimized version filters the images by cloud cover directly in the STAC query using the query parameter. This reduces the number of items retrieved and processed, improving efficiency.</li> <li>Generator Expression: Instead of appending each image to a list and then concatenating, the optimized version uses a generator expression within the xr.concat function. This avoids loading all the images into memory at once, which is crucial when dealing with large datasets.</li> <li>Direct Concatenation: Concatenating directly from the generator is more memory-efficient than creating an intermediate list of xarray DataArrays.</li> <li>Error Handling: The optimized version includes a try...except block to handle potential errors during file opening or concatenation. It also specifically handles the ValueError that can occur if xr.concat is called with an empty iterable (i.e., no images are found).</li> </ul> <p>Why these changes are important:</p> <ul> <li>Memory Efficiency: The generator expression and direct concatenation significantly reduce memory usage, especially when dealing with many or large images.</li> <li>Performance: The optimized version is likely to be faster due to reduced data retrieval and more efficient concatenation.</li> <li>Robustness: Error handling makes the code more reliable and prevents crashes due to unexpected issues.</li> <li>Best Practices: Using generator expressions, direct concatenation, and proper error handling are considered best practices in Python, making your code more efficient, reliable, and maintainable.</li> </ul> <p>This optimized version demonstrates how to efficiently retrieve and process Sentinel-2 images from the Planetary Computer while adhering to best practices for memory management and error handling.</p> <p>Use LLMs to tidy up, style and document code</p> <p>Prompt</p> <p>Can you add documentation to the following Python code? It is a function that computes the maximum NDVI value in a time-series with the data stored in a 3D array. </p> <pre><code>import numpy as np\n\ndef estimate_max_ndvi_3d(ndvi_array):\n    max_ndvi_values = np.nanmax(ndvi_array, axis=0)\n    return max_ndvi_values\n</code></pre> Response (ChatGPT) <pre><code>import numpy as np\n\ndef estimate_max_ndvi_3d(ndvi_array):\n    \"\"\"\n    Computes the maximum NDVI (Normalized Difference Vegetation Index) value over time.\n\n    This function takes a 3D NumPy array representing NDVI values recorded over time \n    and computes the maximum NDVI value at each spatial location while ignoring NaN values.\n\n    Parameters\n    ----------\n    ndvi_array : numpy.ndarray\n        A 3D array of NDVI values with shape (time, height, width), where:\n        - `time` represents different time points in the series.\n        - `height` and `width` correspond to the spatial dimensions.\n\n    Returns\n    -------\n    numpy.ndarray\n        A 2D array of maximum NDVI values with shape (height, width), containing \n        the highest NDVI value observed at each spatial location over time.\n\n    Notes\n    -----\n    - The function uses `numpy.nanmax` to ignore NaN values when computing the maximum.\n    - If all values for a given spatial location are NaN, the result will be NaN at that location.\n\n    Example\n    -------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ndvi_data = np.array([\n    ...     [[0.2, 0.4], [0.1, np.nan]],\n    ...     [[0.5, 0.3], [0.6, 0.7]],\n    ...     [[0.3, 0.6], [np.nan, 0.8]]\n    ... ])\n    &gt;&gt;&gt; estimate_max_ndvi_3d(ndvi_data)\n    array([[0.5, 0.6],\n        [0.6, 0.8]])\n    \"\"\"\n    max_ndvi_values = np.nanmax(ndvi_array, axis=0)  # Compute max along time axis, ignoring NaNs\n    return max_ndvi_values\n</code></pre> <p>Improvements</p> <ul> <li>Added a detailed docstring that follows the NumPy documentation style.</li> <li>Explained input and output formats for better usability.</li> <li>Included an example to demonstrate how the function works.</li> </ul> <p>This makes the function easier to understand, maintain, and use in larger projects. \ud83d\ude80</p> <p>Prompt</p> <p>From the perspective of a senior developer reviewing code, can you suggest how to improve the clarity of the following Python code? It is a Python function that generates management zones from a multispectral UAV image and a soil map.</p> <pre><code>import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef generate_management_zones(uav_image, soil_map, num_zones):\n    uav_reshaped = uav_image.reshape(-1, uav_image.shape[2])\n    soil_reshaped = soil_map.flatten().reshape(-1, 1)\n    combined_data = np.hstack((uav_reshaped, soil_reshaped))\n    kmeans = KMeans(n_clusters=num_zones, random_state=42)\n    kmeans.fit(combined_data)\n    zones = kmeans.labels_.reshape(uav_image.shape[:2])\n    return zones\n</code></pre> Response (ChatGPT) <pre><code>import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef generate_management_zones(multispectral_image, soil_data, num_clusters):\n    \"\"\"\n    Generate management zones for a field using k-means clustering on multispectral UAV imagery and soil data.\n\n    Parameters:\n    - multispectral_image: numpy.ndarray\n        A 3D array representing the multispectral UAV image with dimensions (height, width, num_bands).\n    - soil_data: numpy.ndarray\n        A 2D array representing the soil map with dimensions (height, width).\n    - num_clusters: int\n        The desired number of management zones (clusters).\n\n    Returns:\n    - management_zones: numpy.ndarray\n        A 2D array with the same spatial dimensions as the input data, where each element indicates the assigned management zone.\n    \"\"\"\n    # Reshape the multispectral image to a 2D array where each row is a pixel and each column is a spectral band\n    pixels = multispectral_image.reshape(-1, multispectral_image.shape[2])\n\n    # Flatten the soil data to create a 1D array of soil values\n    soil_values = soil_data.flatten().reshape(-1, 1)\n\n    # Combine the spectral and soil data into a single feature array\n    features = np.hstack((pixels, soil_values))\n\n    # Initialize the k-means clustering algorithm with the specified number of clusters\n    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n\n    # Fit the k-means algorithm to the combined feature data\n    kmeans.fit(features)\n\n    # Retrieve the cluster labels for each pixel and reshape them to match the original image dimensions\n    management_zones = kmeans.labels_.reshape(multispectral_image.shape[:2])\n\n    return management_zones\n</code></pre> <p>Explanation:</p> <p>Reshaping the Multispectral Image:</p> <ul> <li>The multispectral UAV image is a 3D array with dimensions corresponding to height, width, and the number of spectral bands.</li> <li>We reshape this array into a 2D array (<code>pixels</code>) where each row represents a pixel, and each column corresponds to a spectral band. This transformation is necessary for clustering, as k-means operates on 2D data matrices.</li> </ul> <p>Flattening the Soil Data:</p> <ul> <li>The soil map is a 2D array with spatial dimensions matching the multispectral image.</li> <li>We flatten this array into a 1D array (<code>soil_values</code>) to align each soil measurement with the corresponding pixel in the multispectral image.</li> </ul> <p>Combining Features:</p> <ul> <li>We horizontally stack (<code>np.hstack</code>) the spectral data (<code>pixels</code>) and the soil data (<code>soil_values</code>) to form a single feature matrix (<code>features</code>). Each row in this matrix represents a pixel with its associated spectral and soil attributes.</li> </ul> <p>Applying K-Means Clustering:</p> <ul> <li>We initialize the k-means algorithm with the desired number of clusters (<code>num_clusters</code>) and a fixed random state for reproducibility.</li> <li>The <code>fit</code> method is used to perform clustering on the combined feature matrix.</li> </ul> <p>Reshaping Cluster Labels:</p> <ul> <li>The <code>labels_</code> attribute of the fitted k-means model contains the cluster assignments for each pixel.</li> <li>We reshape these labels back into a 2D array (<code>management_zones</code>) that matches the spatial dimensions of the original multispectral image, where each element indicates the assigned management zone.</li> </ul> <p>This function effectively integrates multispectral imagery and soil data to delineate distinct management zones within a field, facilitating targeted agricultural practices.</p> <p>Use LLMs to help you retrieve data from, or upload data to, web APIs and online data providers</p> <p>Prompt</p> <p>I am a geospatial consultant working for an agricultural technology company that specialises in precision agriculture. I need to upload drone imagery of a farmer's paddock to John Deere's Operations Centre Map Layers. The drone imagery is a GeoTIFF file. </p> <p>Can you write Python code that uploads this image to the correct Map Layers endpoint? </p> <p>Can you provide a step-by-step explanation of the process and also review the code for security issues?</p> Response (Gemini 2.0 Flash) <pre><code>import requests\nimport json\n\n# Replace with your actual values\nclient_id = \"YOUR_CLIENT_ID\"\nclient_secret = \"YOUR_CLIENT_SECRET\"\nrefresh_token = \"YOUR_REFRESH_TOKEN\"\napi_url = \"https://sandboxapi.deere.com/platform\"  # Use sandbox for testing\ntiff_filepath = \"path/to/your/drone_imagery.tiff\"\n\ndef get_access_token(client_id, client_secret, refresh_token):\n    \"\"\"Retrieves an access token using the provided refresh token.\"\"\"\n    url = f\"{api_url}/oauth2/token\"\n    data = {\n        \"grant_type\": \"refresh_token\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        \"refresh_token\": refresh_token,\n    }\n    response = requests.post(url, data=data)\n    response.raise_for_status()  # Raise an exception for bad status codes\n    return response.json()[\"access_token\"]\n\ndef upload_to_map_layers(access_token, tiff_filepath):\n    \"\"\"Uploads a GeoTIFF file to the Map Layers endpoint.\"\"\"\n    url = f\"{api_url}/mapLayers\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/vnd.deere.axiom.v3+json\",\n    }\n    # The 'files' parameter needs to be a list of tuples\n    files = [(\"file\", (\"drone_imagery.tiff\", open(tiff_filepath, \"rb\"), \"image/tiff\"))]\n    response = requests.post(url, headers=headers, files=files)\n    response.raise_for_status()\n    return response.json()\n\n# --- Main execution ---\naccess_token = get_access_token(client_id, client_secret, refresh_token)\nresponse_data = upload_to_map_layers(access_token, tiff_filepath)\n\nprint(json.dumps(response_data, indent=2))  # Print the response\n</code></pre> <p>Step-by-step explanation</p> <p>Import necessary libraries:</p> <p><code>requests</code>: for making HTTP requests to the API. <code>json</code>: for handling JSON data.</p> <p>Set up credentials and API endpoint:</p> <p>Replace the placeholder values for <code>client_id</code>, <code>client_secret</code>, <code>refresh_token</code>, and <code>tiff_filepath</code> with your actual values.</p> <p>Use the sandbox API URL (https://sandboxapi.deere.com/platform) for testing. Once your code is working, switch to the production URL.</p> <p><code>get_access_token</code> function:</p> <p>Sends a POST request to the OAuth2 token endpoint to obtain an access token using the refresh token. Includes error handling to raise an exception if the request fails.</p> <p><code>upload_to_map_layers</code> function:</p> <ul> <li>Sends a POST request to the Map Layers endpoint to upload the GeoTIFF file.</li> <li>Sets the necessary headers, including the Authorization header with the access token.</li> <li>Uses the files parameter to include the GeoTIFF file in the request.</li> <li>Includes error handling to raise an exception if the request fails.</li> </ul> <p>Main execution:</p> <ul> <li>Calls <code>get_access_token</code> to obtain the access token.</li> <li>Calls <code>upload_to_map_layers</code> to upload the GeoTIFF file.</li> <li>Prints the JSON response from the API.</li> </ul> <p>Security review</p> <p>API credentials:</p> <ul> <li>Never hardcode API credentials directly in your code. This is a major security risk. Instead, use environment variables, configuration files, or a secrets management service to store and retrieve your credentials.</li> <li>Error handling: The code includes basic error handling to raise exceptions for failed requests. Consider adding more specific error handling based on the possible HTTP status codes and error responses from the API.</li> <li>Data validation: The code currently doesn't validate the input data (e.g., tiff_filepath). Add validation to ensure the provided file path is valid and the file exists.</li> <li>HTTPS: The code uses HTTPS to communicate with the API, which is essential for secure communication.</li> <li>Sandboxing: Using the sandbox API for testing is a good security practice, as it prevents accidental modifications to production data.</li> </ul> <p>Additional considerations:</p> <ul> <li>Rate limiting: Be aware of the API's rate limits to avoid exceeding them and getting your requests throttled.</li> <li>Documentation: Refer to the John Deere Operations Center API documentation for details on the Map Layers endpoint and other available functionalities.</li> <li>Dependencies: Ensure you have the required libraries installed (requests). You can install them using <code>pip install requests</code>.</li> </ul> <p>Remember to replace the placeholder values with your actual credentials and file path before running the code. Always test your code in the sandbox environment before deploying it to production.</p> <p>Prompt</p> <p>I need to download hourly evaporation data from NASA POWER's API. Can you write a function that will take in date range and location, request the data from NASA POWER's API, and process the response into a pandas DataFrame?</p> <p>Practice iterative prompting</p> <p>The initial response provided by ChatGPT is close to a working solution, but has some errors (e.g. the parameter is <code>EVLAND</code> not <code>EVAP</code>). This illustrates where LLMs can make mistakes and so it is important to verify their output (e.g. run the code). There are also some issues with formatting the date to create the pandas DataFrame. Here, you could do your own research of the NASA POWER API docs to identify the correct parameter and iteratively prompt the LLM to fix the date formatting issue. </p> Response (ChatGPT) <p>To download hourly evaporation data from NASA's Prediction Of Worldwide Energy Resources (POWER) API, you can create a Python function that accepts a date range and location, requests the data, and processes the response into a pandas DataFrame. Here's how you can achieve this:</p> <p>Step 1: Install Required Libraries</p> <p>Ensure you have the necessary libraries installed:</p> <pre><code>pip install requests pandas\n</code></pre> <p>Step 2: Define the Function</p> <p>Below is the Python function to fetch and process the data:  </p> <pre><code>import requests\nimport pandas as pd\n\ndef fetch_hourly_evaporation_data(start_date, end_date, latitude, longitude):\n    \"\"\"\n    Fetch hourly evaporation data from NASA POWER API for a given date range and location.\n\n    Parameters:\n    - start_date (str): Start date in 'YYYYMMDD' format.\n    - end_date (str): End date in 'YYYYMMDD' format.\n    - latitude (float): Latitude of the location.\n    - longitude (float): Longitude of the location.\n\n    Returns:\n    - pd.DataFrame: DataFrame containing the hourly evaporation data.\n    \"\"\"\n    # Define the base URL for the NASA POWER API\n    base_url = \"https://power.larc.nasa.gov/api/temporal/hourly/point\"\n\n    # Define the parameters for the API request\n    params = {\n        \"start\": start_date,\n        \"end\": end_date,\n        \"latitude\": latitude,\n        \"longitude\": longitude,\n        \"parameters\": \"EVAP\",\n        \"community\": \"AG\",\n        \"format\": \"JSON\",\n        \"time-standard\": \"LST\"  # Local Solar Time\n    }\n\n    # Make the API request\n    response = requests.get(base_url, params=params)\n    response.raise_for_status()  # Raise an error for bad status codes\n\n    # Parse the JSON response\n    data = response.json()\n\n    # Extract the hourly evaporation data\n    records = data.get('properties', {}).get('parameter', {}).get('EVAP', {})\n\n    # Convert the records into a DataFrame\n    df = pd.DataFrame.from_dict(records, orient='index', columns=['Evaporation'])\n    df.index = pd.to_datetime(df.index)\n    df.sort_index(inplace=True)\n\n    return df\n</code></pre> <p>Step 3: Usage Example</p> <p>Here's how you can use the function:</p> <pre><code># Define your parameters\nstart_date = \"20250101\"  # YYYYMMDD\nend_date = \"20250107\"    # YYYYMMDD\nlatitude = -31.9505      # Example: Perth, Australia\nlongitude = 115.8605     # Example: Perth, Australia\n\n# Fetch the data\nevaporation_data = fetch_hourly_evaporation_data(start_date, end_date, latitude, longitude)\n\n# Display the DataFrame\nprint(evaporation_data.head())\n</code></pre> <p>Explanation:</p> <p>API Endpoint and Parameters:</p> <ul> <li>The function constructs a request to the NASA POWER API's hourly data endpoint for a specific geographic point.</li> <li>Parameters include the date range (start and end), geographic coordinates (latitude and longitude), the desired parameter (EVAP for evaporation), the user community (AG for agriculture), the response format (JSON), and the time standard (LST for Local Solar Time).</li> </ul> <p>Making the Request:</p> <ul> <li>The function uses the requests library to send a GET request to the API with the specified parameters.</li> <li>It checks for successful response status and raises an error if the request was unsuccessful.</li> </ul> <p>Processing the Response:</p> <ul> <li>The JSON response is parsed to extract the hourly evaporation data located under properties.parameter.EVAP.</li> <li>This data is converted into a pandas DataFrame, with the timestamps as the index and a column named 'Evaporation'.</li> <li>The index is converted to datetime objects, and the DataFrame is sorted by the index to ensure chronological order.</li> </ul> <p>Note:</p> <ul> <li>Ensure that the date range and location provided are within the valid range supported by the NASA POWER API.</li> <li>The API provides data starting from January 1, 2001, to near real-time.</li> <li>For more information on the API and its parameters, refer to the NASA POWER API Documentation.</li> </ul> <p>By following these steps, you can successfully retrieve and process hourly evaporation data for a specified date range and location using the NASA POWER API.</p> <p>Use LLMs to help with data visualisation, exploration and communication </p> <p>Prompt</p> <p>You are a senior geospatial analyst. You are generating figures for a public government report. The purpose of the report is to highlight within-field yield variability and the opportunities for precision agriculture to increase crop production. </p> <p>Can you generate a publication quality figure of an NDVI map for a field with a legend and save it as a PNG file? </p> <p>Can you explain the design and style choices when presenting information on the map. </p> <p>Verifying and improving LLM responses</p> <p>The response generated by Gemini 2.0 Flash below is a good example of why it's important to carefully check the LLM response. Overall, it's provided you with Python code that's a good start and its considered lots of elements of a good quality figure (e.g. scale bar and adding coordinates to the map axes). However, there are places where the model's reasoning is erroneous, such as the suggestion to use a red-yellow-green colour palette because it is colour blind friendly (red and green are indistinguishable for certain types of colour blindness). This LLM response is a good reminder for you to consider the accessibility of your figures and choose appropriate colour palettes, and, also, a reminder to check the LLM's reasoning for errors. </p> <p>Also, this is a first attempt. Think about how you could improve the figure (e.g. adding a basemap for reference). This is something you could go back to the LLM with and prompt it to update its response to include basemaps. It is good to get into the habit of treating the LLM as a coding assistant that you bounce ideas and tasks off. </p> Response (Gemini 2.0 Flash) <p>I can't directly generate a PNG file and display it here. I'm a text-based AI and don't have the capability to create images. However, I can provide you with the Python code using Matplotlib and a conceptual explanation of how to create such a figure, along with the design considerations for a public government report.  You can then run this code in your own Python environment.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.cm as cm\n\n# Sample NDVI data (replace with your actual data)\nnp.random.seed(42)  # for reproducibility\nrows, cols = 50, 50\nndvi_data = np.random.uniform(0.2, 0.9, size=(rows, cols))  # Realistic NDVI range\n\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(8, 6))  # Adjust figure size as needed\n\n# Plot the NDVI data as an image\nim = ax.imshow(ndvi_data, cmap=cm.get_cmap('RdYlGn'),  # Use a suitable colormap\n            vmin=0.2, vmax=0.9,  # Set colorbar limits for consistency\n            extent=[0, cols*10, 0, rows*10], # Example: 10m x 10m grid cells\n            origin='lower')  # Ensure correct orientation\n\n# Add a colorbar with appropriate labels\ncbar = fig.colorbar(im, ax=ax, shrink=0.7)  # Shrink colorbar if needed\ncbar.set_label('Normalized Difference Vegetation Index (NDVI)', fontsize=10)\ncbar.ax.tick_params(labelsize=8)\n\n# Add a title and axis labels\nax.set_title('Within-Field NDVI Variability', fontsize=12)\nax.set_xlabel('Easting (m)', fontsize=10) # Example units\nax.set_ylabel('Northing (m)', fontsize=10) # Example units\nax.tick_params(labelsize=8)\n\n# Add a scale bar (example)\nfrom mpl_toolkits.axes_grid1.anchored_artists import AnchoredScaleBar\nscalebar = AnchoredScaleBar(ax.transData, 100, \"100 m\", loc=3, pad=0.5,\n                        frameon=False, fontsize=8) # Adjust length and position\nax.add_artist(scalebar)\n\n# Improve plot aesthetics\nax.set_aspect('equal') # Important for geospatial data\nfig.tight_layout()  # Adjust spacing\n\n# Save the figure as a PNG\nplt.savefig('ndvi_map.png', dpi=300) # High DPI for publication\n\nplt.show() # If you want to display the plot in your IDE\n</code></pre> <p>Design and Style Choices Explanation:</p> <ul> <li>Colormap:  RdYlGn (Red-Yellow-Green) is a common and intuitive colormap for NDVI, where red indicates lower vegetation health, yellow intermediate, and green higher.  It's also generally colorblind-friendly.  Other options like viridis could be considered.</li> <li>Colorbar Limits: Setting vmin and vmax ensures consistency if you're comparing multiple fields or time periods.  It also prevents the colorbar from being skewed by outliers.  The range 0.2-0.9 is a typical range for NDVI, but you should adjust it based on your data.</li> <li>Figure Size and DPI:  The figsize and dpi settings control the final image resolution.  300 dpi is generally suitable for print publications.</li> <li>Axis Labels and Units: Clear and informative labels are crucial.  Include units (e.g., meters, feet) where appropriate.  If your data is in a specific coordinate system (e.g., UTM), mention it in the figure caption or report text.</li> <li>Title: A concise and descriptive title is essential.</li> <li>Scale Bar: A scale bar provides context and helps the reader understand the spatial dimensions of the field.</li> <li>Aspect Ratio: Setting <code>ax.set_aspect('equal')</code> is extremely important for geospatial data. It ensures that the distances on the x and y axes are represented correctly, preventing distortion.</li> <li>Colorbar Label: The colorbar needs a clear label explaining what the colors represent.</li> <li>Font Sizes: Use consistent and readable font sizes for all text elements.</li> <li>Layout: <code>fig.tight_layout()</code> helps prevent labels and titles from overlapping.</li> <li>Data:  The code uses np.random.uniform to generate sample NDVI data.  Replace this with your actual NDVI data.  Make sure your data is in a format that Matplotlib can handle (e.g., a 2D NumPy array).</li> <li>Extent: The extent argument in imshow is crucial for correctly positioning your data on the map.  It should correspond to the real-world coordinates of your field.  If you don't provide it, the image will be displayed in pixel coordinates.</li> </ul> <p>For a government report:</p> <ul> <li>Clarity is paramount: Avoid jargon and explain any technical terms.</li> <li>Accuracy is essential: Double-check your data and ensure that the map accurately represents the field conditions.</li> <li>Accessibility: Consider colorblind readers when choosing colormaps.</li> <li>Context: Provide sufficient background information in the report text to explain the significance of the NDVI map and its implications for precision agriculture. Include information about the field, the date of the imagery, and any other relevant details.</li> <li>Caption: The figure should have a clear and concise caption that summarizes the key findings.</li> </ul> <p>Remember to install the necessary libraries: pip install matplotlib numpy.  Replace the sample data with your own and adjust the parameters as needed. This detailed explanation should allow you to create a publication-quality NDVI map for your report.</p> <p>You can consider using AI to help with more advanced tasks such as creating interactive data visualisations, animations, web pages or dashboards.</p> <p>Prompt</p> <p>I am building a prototype dashboard that will be used to monitor on-farm water resources and crop water requirements. Can you generate an interactive visualisation of a time-series of evaporation data? The evaporation data is stored in a CSV file.</p> <p>How could you improve this prompt?</p> <p>This is not a particularly challenging coding task, so it is likely that the model's first response will be pretty close-to-the-mark. However, here are some suggestions for how to improve this prompt:</p> <ul> <li>Provide more specifics about the figure you want generated (e.g. colour, themes, styling)</li> <li>Provide more specifics about the figure's interactive features (e.g. what data do you want in hover pop ups?)</li> <li>Be more explicit about the LLMs role (e.g. generate the code using web development best-practice)</li> <li>Encourage the model to think about the task (e.g. you could ask the model to make suggestions on useful interactive features)</li> <li>Ask the LLM to generate a dummy dataset for you to test the code with, this can speed up your development iterations</li> <li>Provide more contextual information (e.g. if this is just a prototype, emphasise that you want a simple codebase that's easy to edit after user testing)</li> </ul> Response (ChatGPT) <p>Below is an example of how you might create an interactive time-series visualization of evaporation data using Plotly and pandas. This example assumes your CSV file has at least two columns (e.g., \"Date\" and \"Evaporation\"):</p> <p><pre><code>import pandas as pd\nimport plotly.express as px\n\n# Load the evaporation data from CSV\ndf = pd.read_csv('evaporation_data.csv')\n\n# Ensure the 'Date' column is interpreted as datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create an interactive line plot for evaporation over time\nfig = px.line(df, x='Date', y='Evaporation', title='Evaporation Time-Series')\n\n# Add a range slider for interactive zooming and panning\nfig.update_layout(\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n                dict(count=7, label=\"1w\", step=\"day\", stepmode=\"backward\"),\n                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(visible=True),\n        type=\"date\"\n    )\n)\n\n# Display the interactive plot\nfig.show()\n</code></pre> Step-by-Step Explanation</p> <p>Import Libraries:</p> <ul> <li>pandas is used to load and process the CSV data.</li> <li>plotly.express is used to quickly create interactive plots.</li> </ul> <p>Load the Data:</p> <ul> <li>The CSV file is read into a pandas DataFrame using pd.read_csv(). Make sure your file path is correct.</li> </ul> <p>Data Preprocessing:</p> <ul> <li>The \"Date\" column is converted to a datetime object with pd.to_datetime() to ensure proper time-series handling.</li> </ul> <p>Create the Interactive Plot:</p> <ul> <li>A line plot is created using px.line(), specifying the x-axis as \"Date\" and the y-axis as \"Evaporation\".</li> <li>A title is added to make the chart self-explanatory.</li> </ul> <p>Enhance Interactivity:</p> <ul> <li>A range selector and range slider are added using fig.update_layout(). This allows users to quickly zoom in on specific periods or pan across the timeline.</li> </ul> <p>Display the Plot:</p> <ul> <li>Finally, fig.show() renders the interactive visualization in a web browser or an interactive environment (like Jupyter Notebook).</li> </ul> <p>This interactive visualization can be embedded in your prototype dashboard, providing users with an intuitive tool to monitor changes in evaporation over time.</p> <p>Prompt</p> <p>Using best practice web development techniques and open-source packages, create a web dashboard that displays weather data for a location that a user clicks on a web map? When a user clicks on a location add a marker to the map and bring up a popup window that displays current temperature, precipitation, windspeed and humidity. </p> <p>Can you provide detailed instructions for how to deploy and test this dashboard?</p> <p>Use thinking and reasoning models for complex tasks</p> <p>This is a complex tasks comrpising a series of steps and discrete sub-tasks (e.g. building the frontend web map user interface, code to handle user interactions, building a backend server to make fetch weather data on the fly, detailed instructions for deployment). There are different ways to complete these tasks (e.g. choices between different software packages, design and styling choices and the choices between technical deployment solutions); the model needs to consider these different options and make a decision on the solution when generating a response. The nature of this task (comprising multiple problems and consideration of multiple options) makes it well suited to \"thinking\" or \"reasoning\" models. </p> <p>Compare these two responses, one using regular ChatGPT and the other using Gemini 2.0 Flash Thinking with reasoning turned on. ChatGPT without thinking has generated a good first start and a template that you could build on. However, contrast this to the response generated by Gemini 2.0 Flash Thinking which has provided a lot of extra and useful detail:</p> <ul> <li>detailed instructions on installing software and packages </li> <li>detailed instructions on how to deploy the dashboard to the cloud/ servers</li> <li>recognised the need for development and production environments</li> <li>justification for suggested software packages and deployment environments</li> <li>suggestions for styling and enhancement</li> <li>suggestions for testing the user interface and backend servers</li> </ul> Respone (ChatGPT) <p>Here\u2019s how to build a web dashboard that displays weather data when a user clicks on a map. The dashboard will use React (Next.js) for the frontend and FastAPI for the backend to fetch weather data from an open-source API like OpenWeatherMap. We'll also use Leaflet.js for the interactive map and Tailwind CSS for styling.</p> <p>Project Overview</p> <p>When a user clicks on a location:</p> <p>\u2705 A marker is added to the map. \u2705 A popup appears with current temperature, precipitation, windspeed, and humidity. \u2705 Weather data is fetched dynamically from an API.</p> <p>Tech Stack</p> <p>Frontend: React (Next.js), Leaflet.js, Tailwind CSS Backend: FastAPI (Python) API: OpenWeatherMap API Deployment: Vercel (Frontend) + Railway/Render (Backend)</p> <p>Code Implementation</p> <p>Step 1: Set Up the Backend (FastAPI)</p> <p>First, install FastAPI and Uvicorn:</p> <pre><code>pip install fastapi uvicorn requests\n</code></pre> <p>Create a file backend/main.py:</p> <pre><code>from fastapi import FastAPI\nimport requests\nfrom fastapi.middleware.cors import CORSMiddleware\n\napp = FastAPI()\n\n# Allow frontend to access the backend\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Replace with your OpenWeatherMap API Key\nAPI_KEY = \"YOUR_OPENWEATHERMAP_API_KEY\"\n\n@app.get(\"/weather/\")\ndef get_weather(lat: float, lon: float):\n    \"\"\"Fetch current weather data for given lat/lon.\"\"\"\n    url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&amp;lon={lon}&amp;units=metric&amp;appid={API_KEY}\"\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        return {\"error\": \"Unable to fetch weather data\"}\n\n    data = response.json()\n\n    return {\n        \"temperature\": data[\"main\"][\"temp\"],\n        \"humidity\": data[\"main\"][\"humidity\"],\n        \"windspeed\": data[\"wind\"][\"speed\"],\n        \"precipitation\": data[\"weather\"][0][\"description\"],\n    }\n</code></pre> <p>Run the backend server with: <code>uvicorn main:app --reload</code></p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>The API is now running at http://127.0.0.1:8000/weather/.</p> <p>Step 2: Set Up the Frontend (Next.js + Leaflet)</p> <p>Create a Next.js project:</p> <pre><code>npx create-next-app frontend\ncd frontend\nnpm install leaflet react-leaflet axios\nCreate a file frontend/components/Map.js:\n\nimport { useState } from \"react\";\nimport { MapContainer, TileLayer, Marker, useMapEvents, Popup } from \"react-leaflet\";\nimport axios from \"axios\";\n\nconst MapComponent = () =&gt; {\nconst [position, setPosition] = useState(null);\nconst [weather, setWeather] = useState(null);\n\nconst fetchWeather = async (lat, lon) =&gt; {\n    try {\n    const response = await axios.get(`http://127.0.0.1:8000/weather/?lat=${lat}&amp;lon=${lon}`);\n    setWeather(response.data);\n    } catch (error) {\n    console.error(\"Failed to fetch weather data\", error);\n    }\n};\n\nfunction LocationMarker() {\n    useMapEvents({\n    click(e) {\n        const { lat, lng } = e.latlng;\n        setPosition([lat, lng]);\n        fetchWeather(lat, lng);\n    },\n    });\n\n    return position ? (\n    &lt;Marker position={position}&gt;\n        &lt;Popup&gt;\n        {weather ? (\n            &lt;&gt;\n            &lt;b&gt;Weather Info&lt;/b&gt;\n            &lt;br /&gt; \ud83c\udf21 Temp: {weather.temperature}\u00b0C\n            &lt;br /&gt; \ud83d\udca7 Humidity: {weather.humidity}%\n            &lt;br /&gt; \ud83d\udca8 Wind: {weather.windspeed} m/s\n            &lt;br /&gt; \ud83c\udf27 Precip: {weather.precipitation}\n            &lt;/&gt;\n        ) : (\n            \"Fetching weather...\"\n        )}\n        &lt;/Popup&gt;\n    &lt;/Marker&gt;\n    ) : null;\n}\n\nreturn (\n    &lt;MapContainer center={[51.505, -0.09]} zoom={5} style={{ height: \"500px\", width: \"100%\" }}&gt;\n    &lt;TileLayer url=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\" /&gt;\n    &lt;LocationMarker /&gt;\n    &lt;/MapContainer&gt;\n);\n};\n\nexport default MapComponent;\n</code></pre> <p>Step 3: Integrate the Map in Next.js</p> <p>Modify frontend/pages/index.js:</p> <pre><code>import dynamic from \"next/dynamic\";\n\nconst MapComponent = dynamic(() =&gt; import(\"../components/Map\"), { ssr: false });\n\nexport default function Home() {\nreturn (\n    &lt;div className=\"flex flex-col items-center justify-center h-screen\"&gt;\n    &lt;h1 className=\"text-2xl font-bold mb-4\"&gt;On-Farm Weather Dashboard&lt;/h1&gt;\n    &lt;MapComponent /&gt;\n    &lt;/div&gt;\n);\n}\n</code></pre> <ol> <li>Running &amp; Testing the Dashboard</li> </ol> <p>Start the Backend</p> <pre><code>cd backend\nuvicorn main:app --reload\n</code></pre> <p>Start the Frontend</p> <pre><code>cd frontend\nnpm run dev\n</code></pre> <p>Open http://localhost:3000, click anywhere on the map, and see the weather data popup!</p> <ol> <li>Deploying the Dashboard</li> </ol> <p>Deploy Backend (FastAPI)</p> <ul> <li> <p>Use Railway, Render, or Fly.io for FastAPI deployment.</p> </li> <li> <p>Example using Railway:</p> </li> </ul> <pre><code>railway init\nrailway up\n</code></pre> <ul> <li>Update frontend/components/Map.js to use the live API URL.</li> </ul> <p>Deploy Frontend (Next.js)</p> <ul> <li>Deploy to Vercel:</li> </ul> <pre><code>vercel\n</code></pre> <ul> <li> <p>Update API endpoint in the frontend with the deployed FastAPI URL.</p> </li> <li> <p>Enhancements &amp; Future Improvements</p> </li> </ul> <p>\u2705 Add weather forecasts for the next few days \u2705 Store user locations for analysis \u2705 Integrate satellite imagery for farm monitoring</p> <p>This setup ensures best practices in modern web development, leveraging open-source packages for interactivity and real-time data updates. \ud83d\ude80</p> Response (Google 2.0 Flash Thinking) <p>Okay, let's create a detailed guide to build your interactive weather dashboard. We'll use best practices by structuring our project, using modern tools, and focusing on maintainability and scalability.</p> <p>Project Goal:  A web dashboard with a map. Users click on the map, a marker appears, and a popup displays weather data for that location.</p> <p>Technology Stack (Open Source &amp; Best Practice):</p> <p>Frontend:</p> <ul> <li>React: A popular JavaScript library for building user interfaces. It's component-based, efficient, and well-supported. (Best Practice for modern web UIs)</li> <li>Leaflet: A lightweight, open-source JavaScript library for interactive maps. It's easy to use and has great performance. (Best Practice for map integration)</li> <li>React Leaflet: React components for Leaflet, making integration seamless within React.</li> <li>Axios: A promise-based HTTP client for making API requests from the frontend. (Best Practice for API interaction)</li> <li>CSS Modules or Styled Components: For styling components in a modular and maintainable way. (Best Practice for CSS management)</li> </ul> <p>Backend (Simple API - for this example):</p> <ul> <li>Node.js with Express: A minimal and flexible Node.js web application framework. Easy to set up a simple API. (Open Source and good for backend flexibility)</li> <li>OpenWeatherMap API: A widely used weather API (free tier available for development) to fetch weather data. (Open Source data source - requires API key) Deployment:</li> </ul> <p>Netlify or Vercel (Frontend): Easy and free static site hosting for React frontends. (Best Practice for frontend deployment) PythonAnywhere, Heroku, or Render (Backend): Platforms for deploying simple backend applications (free or low-cost tiers for initial deployment). (Best Practice for backend deployment) Step-by-Step Instructions:</p> <p>Phase 1: Project Setup and Backend API (Node.js with Express)</p> <p>Set up your Development Environment:</p> <ul> <li>Install Node.js and npm (or yarn): Make sure you have Node.js and npm (Node Package Manager) installed on your machine. Download from https://nodejs.org/.</li> <li>Choose a Code Editor: VS Code, Sublime Text, Atom, etc. (VS Code is highly recommended for React development).</li> <li>Create Backend Folder and Initialize Node.js Project:</li> </ul> <p>Open your terminal and create a folder for your backend (e.g., weather-dashboard-backend):</p> <pre><code>mkdir weather-dashboard-backend\ncd weather-dashboard-backend\nnpm init -y  # Initializes a Node.js project with default settings\n</code></pre> <p>Install Backend Dependencies:</p> <pre><code>npm install express cors dotenv node-fetch\n</code></pre> <ul> <li>express: For creating the API server.</li> <li>cors: For enabling Cross-Origin Resource Sharing (so your frontend can access the backend).</li> <li>dotenv: To load environment variables (like your API key) from a <code>.env</code> file.</li> <li>node-fetch: To make HTTP requests to the OpenWeatherMap API (built-in fetch in newer Node.js versions is also an option)</li> </ul> <p>Create .env file:</p> <ul> <li>In your <code>weather-dashboard-backend</code> folder, create a file named <code>.env</code>.</li> <li>Sign up for an account at https://openweathermap.org/api and get an API key.</li> <li>Add your API key to the .env file:</li> </ul> <pre><code>OPENWEATHERMAP_API_KEY=YOUR_API_KEY_HERE\n</code></pre> <p>Create server.js (or index.js) for your Express API:</p> <pre><code>// server.js (or index.js)\nrequire('dotenv').config(); // Load environment variables from .env\nconst express = require('express');\nconst cors = require('cors');\nconst fetch = require('node-fetch'); // or use built-in fetch\n\nconst app = express();\nconst port = process.env.PORT || 5000; // Use PORT env variable if available, otherwise default to 5000\n\napp.use(cors()); // Enable CORS for all origins (for development - refine for production)\n\napp.get('/weather', async (req, res) =&gt; {\n    const { lat, lon } = req.query; // Get latitude and longitude from query parameters\n\n    if (!lat || !lon) {\n        return res.status(400).json({ error: 'Latitude and longitude are required.' });\n    }\n\n    const apiKey = process.env.OPENWEATHERMAP_API_KEY;\n    const apiUrl = `https://api.openweathermap.org/data/2.5/weather?lat=&lt;span class=\"math-inline\"&gt;\\{lat\\}&amp;lon\\=&lt;/span&gt;{lon}&amp;appid=${apiKey}&amp;units=metric`; // Use metric units\n\n    try {\n        const response = await fetch(apiUrl);\n        if (!response.ok) {\n            throw new Error(`HTTP error! status: ${response.status}`);\n        }\n        const weatherData = await response.json();\n        res.json(weatherData);\n    } catch (error) {\n        console.error('Error fetching weather data:', error);\n        res.status(500).json({ error: 'Failed to fetch weather data.' });\n    }\n});\n\napp.listen(port, () =&gt; {\n    console.log(`Server listening on port ${port}`);\n});\n</code></pre> <p>Run the Backend API:</p> <pre><code>node server.js  # or node index.js if you named it index.js\n</code></pre> <p>Your backend API should be running on http://localhost:5000.</p> <p>Test the Backend API:</p> <ul> <li>Open a browser or use a tool like Postman/curl.</li> <li>Access the endpoint with latitude and longitude parameters, e.g.,: http://localhost:5000/weather?lat=51.5074&amp;lon=0.1278 (London coordinates)</li> <li>You should see a JSON response with weather data from OpenWeatherMap.</li> </ul> <p>Phase 2: Frontend Setup (React) and Map Integration</p> <p>Create Frontend Folder and React App:</p> <p>In the same parent directory as your <code>weather-dashboard-backend</code> folder, create a frontend folder (e.g., <code>weather-dashboard-frontend</code>):</p> <pre><code>npx create-react-app weather-dashboard-frontend\ncd weather-dashboard-frontend\n</code></pre> <p>This uses create-react-app (CRA), a best practice for quickly setting up a React project.</p> <p>Install Frontend Dependencies:</p> <pre><code>npm install leaflet react-leaflet axios\n</code></pre> <ul> <li>leaflet: Leaflet library.</li> <li>react-leaflet: React components for Leaflet.</li> <li>axios: For making API requests to your backend.</li> </ul> <p>Clean up src folder:</p> <ul> <li>Delete unnecessary files from src like <code>App.css</code>, <code>logo.svg</code>, <code>reportWebVitals.js</code>, <code>setupTests.js</code>.</li> <li>Modify <code>index.js</code> to just render App component.</li> <li>Modify <code>App.js</code> (Basic Map Component):</li> </ul> <p><pre><code>// src/App.js\nimport React, { useState } from 'react';\nimport { MapContainer, TileLayer, Marker, Popup } from 'react-leaflet';\nimport 'leaflet/dist/leaflet.css'; // Import Leaflet CSS\nimport axios from 'axios';\nimport L from 'leaflet'; // Import Leaflet for icon fix\n\n// Fix for Leaflet marker icon issue (if needed - sometimes CRA and Leaflet conflict)\ndelete L.Icon.Default.prototype._getIconUrl;\nL.Icon.Default.mergeOptions({\n    iconRetinaUrl: require('leaflet/dist/images/marker-icon-2x.png'),\n    iconUrl: require('leaflet/dist/images/marker-icon.png'),\n    shadowUrl: require('leaflet/dist/images/marker-shadow.png')\n});\n\n\nfunction App() {\n    const [markerPosition, setMarkerPosition] = useState(null);\n    const [weatherData, setWeatherData] = useState(null);\n\n    const handleMapClick = async (event) =&gt; {\n        const { lat, lng } = event.latlng;\n        setMarkerPosition({ lat, lng });\n\n        try {\n            const response = await axios.get('http://localhost:5000/weather', { // Adjust backend URL if needed\n                params: { lat, lon: lng }\n            });\n            setWeatherData(response.data);\n        } catch (error) {\n            console.error('Error fetching weather data:', error);\n            setWeatherData({ error: 'Failed to fetch weather data' }); // Set an error state\n        }\n    };\n\n    return (\n        &lt;div style={{ height: '100vh', width: '100%' }}&gt;\n            &lt;MapContainer center={[51.505, -0.09]} zoom={13} style={{ height: '100%', width: '100%' }} onClick={handleMapClick}&gt;\n                &lt;TileLayer\n                    url=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\"\n                    attribution='&amp;copy; &lt;a href=\"https://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; contributors'\n                /&gt;\n                {markerPosition &amp;&amp; (\n                    &lt;Marker position={[markerPosition.lat, markerPosition.lng]}&gt;\n                        &lt;Popup&gt;\n                            {weatherData ? (\n                                weatherData.error ? (\n                                    &lt;p&gt;Error: {weatherData.error}&lt;/p&gt;\n                                ) : (\n                                    &lt;div&gt;\n                                        &lt;p&gt;Temperature: {weatherData.main?.temp}\u00b0C&lt;/p&gt;\n                                        &lt;p&gt;Description: {weatherData.weather?.[0]?.description}&lt;/p&gt;\n                                        &lt;p&gt;Wind Speed: {weatherData.wind?.speed} m/s&lt;/p&gt;\n                                        &lt;p&gt;Humidity: {weatherData.main?.humidity}%&lt;/p&gt;\n                                        {/* Add more weather data as needed */}\n                                    &lt;/div&gt;\n                                )\n                            ) : (\n                                &lt;p&gt;Loading weather data...&lt;/p&gt;\n                            )}\n                        &lt;/Popup&gt;\n                    &lt;/Marker&gt;\n                )}\n            &lt;/MapContainer&gt;\n        &lt;/div&gt;\n    );\n}\n\nexport default App;\n</code></pre> * Your frontend app should be running on http://localhost:3000.</p> <p>Test the Dashboard:</p> <ul> <li>Open http://localhost:3000 in your browser. \u00a0 </li> <li>Click on the map.</li> <li>A marker should appear, and after a moment, a popup should display weather data for that location.</li> <li>If you see \"Loading weather data...\" and it doesn't resolve, check your backend API is running correctly, and your API key is valid. \u00a0 </li> <li>Inspect the browser's developer console (F12) for any errors (Network, Console tabs).</li> </ul> <p>Phase 3: Styling and Enhancements (Optional but Recommended Best Practices)</p> <p>Styling: \u00a0 </p> <ul> <li>CSS Modules: In <code>App.js</code>, you can create <code>App.module.css</code> file for component-specific styling and import it: <code>import styles from './App.module.css';</code> and use class names like <code>className={styles.mapContainer}</code>. \u00a0 </li> <li>Styled Components: Install <code>npm install styled-components</code> and use them to create styled components within your React code.</li> <li>Tailwind CSS: Install <code>npm install -D tailwindcss postcss autoprefixer</code> and configure Tailwind CSS for rapid utility-first styling.</li> </ul> <p>Enhancements (Best Practices):</p> <ul> <li>Error Handling: Improve error messages in popups.</li> <li>Loading States: Implement better loading indicators (spinners).</li> <li>UI/UX Improvements: Design a better popup, maybe use icons for weather conditions, make the map responsive.</li> <li>Caching: Implement client-side or server-side caching to reduce API calls for frequently clicked locations.</li> <li>Input Field for Location Search: Add a search bar using a geocoding service (like OpenStreetMap Nominatim or Google Maps Geocoding API) to allow users to search for locations by name and then click on the map.</li> <li>More Weather Data: Display more relevant weather information from the OpenWeatherMap API response (sunrise/sunset, feels like temperature, pressure, etc.).</li> <li>Weather Icons: Use weather condition codes from OpenWeatherMap to display relevant weather icons in the popup.</li> <li>Responsive Design: Ensure the dashboard works well on different screen sizes (mobile, tablet, desktop).</li> </ul> <p>Phase 4: Deployment and Testing</p> <p>Deployment - Frontend (React - using Netlify as an example):</p> <p>Build your React Frontend:</p> <pre><code>cd weather-dashboard-frontend\nnpm run build\n</code></pre> <p>This creates an optimized production build of your React app in the build folder.</p> <p>Deploy to Netlify:</p> <ul> <li>Sign up for a free Netlify account at https://www.netlify.com/.</li> <li>Install Netlify CLI globally (optional but recommended for more control): npm install netlify-cli -g.</li> <li>From your weather-dashboard-frontend directory:</li> </ul> <pre><code>netlify deploy --prod\n</code></pre> <p>(If you don't have Netlify CLI, you can also drag and drop the <code>build</code> folder to the Netlify web interface.) \u00a0 </p> <ul> <li>Netlify will give you a live URL for your frontend.</li> </ul> <p>Deployment - Backend (Node.js/Express - using PythonAnywhere as an example):</p> <p>Prepare for Deployment:</p> <ul> <li>Procfile: Create a <code>Procfile</code> in your <code>weather-dashboard-backend</code> folder (no file extension):</li> </ul> <p><pre><code>web: node server.js\n</code></pre> * package.json (engines): Add an engines section to your package.json to specify the Node.js version:</p> <pre><code>\"engines\": {\n    \"node\": \"18.x\"  // Or a specific Node.js version you used\n},\n</code></pre> <p>Deploy to PythonAnywhere (Free Tier - suitable for testing, limitations apply):</p> <ul> <li>Sign up for a free PythonAnywhere account at https://www.pythonanywhere.com/.</li> <li>Create a Web App: Go to the \"Web\" tab and click \"Add a new web app.\" Choose \"Manual configuration\" and Node.js.</li> <li>Upload your Backend Code: Use the \"Files\" tab to upload the contents of your weather-dashboard-backend folder (including <code>package.json</code>, <code>server.js</code>, <code>Procfile</code>, <code>.env</code>). Important: On PythonAnywhere free tier, you may need to manually set environment variables within your web app configuration instead of using a .env file if file access restrictions are in place. </li> <li>Install Dependencies: In a PythonAnywhere console (bash), navigate to your backend directory and run npm install.</li> <li>Configure Web App Settings: In the \"Web\" tab, ensure your \"Working directory\" points to your backend directory and that the \"WSGI file\" (if prompted) is correctly configured (it usually defaults to a basic setup). Make sure to point the \"webapp URL\" to your <code>server.js</code> or <code>index.js</code> as specified in your <code>Procfile</code>.</li> </ul> <p>Deployment - Alternatives:</p> <ul> <li>Frontend: Vercel (similar to Netlify, also excellent for React), AWS S3, Google Firebase Hosting, GitHub Pages. \u00a0 </li> <li>Backend: Heroku (more robust, often easier for Node.js), Render (another good PaaS), AWS EC2/Elastic Beanstalk, Google Cloud App Engine, DigitalOcean.</li> </ul> <p>Testing - Detailed Instructions:</p> <p>Local Development Testing:</p> <ul> <li>Frontend: Test at http://localhost:3000 while running npm start in your frontend.</li> <li>Backend: Test at http://localhost:5000 (and specific API endpoints) while running <code>node server.js</code> in your backend.</li> <li>Use browser developer tools (F12 -&gt; Network tab, Console tab) to inspect API requests, responses, and any errors.</li> <li>Test clicking on different map locations.</li> </ul> <p>Deployed Frontend Testing (on Netlify/Vercel):</p> <ul> <li>Access your live Netlify/Vercel URL.</li> <li>Repeat map clicking tests.</li> <li>Check if the frontend is correctly fetching data from your deployed backend (the URL in axios.get in your frontend needs to point to your deployed backend URL, not localhost:5000 anymore - see \"Phase 5: Configuration for Deployment\" below).</li> </ul> <p>Deployed Backend Testing (on PythonAnywhere/Heroku/Render):</p> <ul> <li>Access your deployed backend URL directly (e.g., from PythonAnywhere's web app URL).</li> <li>Test the <code>/weather</code> endpoint with lat/lon parameters (e.g., <code>your-pythonanywhere-url.pythonanywhere.com/weather?lat=40.7128&amp;lon=-74.0060</code>).</li> <li>Ensure it returns valid weather data.</li> </ul> <p>Cross-Browser Testing:</p> <p>Test your deployed dashboard on different browsers (Chrome, Firefox, Safari, Edge) and different devices (desktop, mobile, tablet) to ensure compatibility and responsiveness. Performance Testing (Basic):</p> <ul> <li>Use browser developer tools (Network tab -&gt; check \"Disable cache\" and reload) to measure API request times.</li> <li>If you have caching implemented, test with and without cache to see performance improvements.</li> </ul> <p>Error Handling Testing:</p> <ul> <li>Simulate API errors:</li> <li>Intentionally use an invalid API key in your backend <code>.env</code>.</li> <li>Temporarily take down your backend server.</li> <li>Test what happens in the frontend when the API call fails \u2013 error messages in popups, graceful handling.</li> </ul> <p>User Acceptance Testing (UAT):</p> <p>Ask other people to use your dashboard and give feedback on usability, functionality, and any issues they encounter.</p> <p>Phase 5: Configuration for Deployment (Important)</p> <p>Backend URL in Frontend:  When you deploy, your frontend will need to connect to your deployed backend URL, not http://localhost:5000.</p> <p>Environment Variables in Frontend: Best practice is to use environment variables in your React frontend as well. Create <code>.env.production</code> and <code>.env.development</code> files in your frontend root.</p> <p>In <code>.env.development</code>:</p> <pre><code>REACT_APP_BACKEND_URL=http://localhost:5000\n</code></pre> <p>In <code>.env.production</code>:</p> <pre><code>REACT_APP_BACKEND_URL=YOUR_DEPLOYED_BACKEND_URL_HERE  // Replace with your actual deployed backend URL\n</code></pre> <ul> <li>Access the URL in your React code like this: <code>process.env.REACT_APP_BACKEND_URL</code>.</li> <li>Netlify/Vercel Environment Variables: When deploying to Netlify or Vercel, you can also set environment variables in their platform settings, which is a more secure way for production.</li> <li>CORS for Production: For a production setup, you should restrict CORS on your backend to only allow requests from your deployed frontend URL instead of <code>cors()</code>. You can configure cors middleware to allow specific origins.</li> </ul> <p>Important Considerations:</p> <ul> <li>API Rate Limits: Be mindful of the OpenWeatherMap API free tier rate limits. For more usage, you might need to upgrade to a paid plan or implement caching.</li> <li>Security: For a production application, you'd need to think more about security:</li> <li>Securely manage API keys (environment variables, don't expose them in frontend code).</li> <li>HTTPS for all communication.</li> <li>Input validation and sanitization.</li> <li>Consider authentication and authorization if you need to control access.</li> <li>Scalability: For a highly scalable application, you might choose different backend technologies (like serverless functions or more robust cloud infrastructure) and database solutions.</li> </ul> <p>This detailed guide should give you a solid foundation for building and deploying your interactive weather dashboard. Remember to test thoroughly at each stage and iterate based on feedback! Good luck!</p> <p>AI code editors</p> <p>There are also AI enabled code editors which let you access AI support in the text editor environment where you write code. These tools have a mix of free and paid options:</p> <ul> <li>copilot in Visual Studio Code</li> <li>Cursor AI</li> <li>Windsurf</li> </ul>"},{"location":"ai/example-uses/#ai-for-literature-reviews-studying-thinking-and-writing","title":"AI for literature reviews, studying, thinking and writing","text":"<p>Before using AI tools to work with literature, it\u2019s important to be aware of the content's terms of service and the copyright, licensing and privacy implications. You should research the copyright and licensing of any material you upload to and use with AI literature tools. </p> <p>General guidelines for uploading data to AI tools</p> <ul> <li>Check how the AI service provider uses your data (i.e. content you upload and your prompts). If possible, select options and use tools that are not trained on and do not view your data. </li> <li>Do not share content that you generate with AI tools. Use these tools privately to enhance your studying, thinking and understanding. Rather than using AI tools as text summarisers / generators, use them to be more active in how you engage with literature and as aids to help you practice critical and analytical reading and thinking. </li> </ul> <p>I want to practice critical / analytical reading and thinking </p> <p>Look at how the AI tools identify limitations and gaps with existing studies as a template for your own reading AND critique the AI responses too - do you agree with the limitations and gaps the AI has identified?</p> <ul> <li>Use  STORM Genie's CO-STORM feature to generate a round table discussion of a topic. This will generate a discussion of a topic presenting different views and perspectives.</li> </ul> <p>Review STORM and CO-STORM's brainSTORMING process</p> <p>When generating Wikipedia-like articles or round table discussions STORM undertakes a \"brainstorming\" process where agents from different perspectives ask and answer questions on a topic, update their knowledge and generate new ideas. Looking at this trace is a good way for you to practice approaching a topic from different perspectives and also identify effective questions that you can reuse elsewhere when researching topics and undertaking literature reviews. </p> <p></p> <p>The STORM interface and viewing the brainSTORMING process.</p> <ul> <li>Upload a paper to NotebookLM and prompt it to generate a list of limitations and recommendations for further work. Or, ask STORM Genie to create to review the literature and create a list of limitations with the current state of the literature for a topic area or task.</li> </ul> <p>Reviewing lists of knowledge gaps or recommendations for further research</p> <p>Actively engage with the lists of limitations and recommendations by prioritising them for further study by considering criteria of impact (what benefits will accrue if the limitation is addressed) and feasibility (what is holding back progress in this area? data availability? knowledge? financial resources?).</p> <p>Also, consider if agree with the limitations and gaps the AI has identified? Are there gaps that you think the AI has overlooked? Why might that be the case?</p> <ul> <li>Ask Google Deep Research (costs) or Perplexity (Deep Research)  to provide a review of knowledge gaps or methodological limitations for a topic area. When reading a paper, consider how it addresses these gaps and identify what its contribution to the literature is. Also, reflect on how relevant the gaps and limitations generated by the AI are. </li> </ul> <p>I want to create a background overview of a subject area before reading papers in detail</p> <p>Generating a literature review prior to reading can help you situate the paper within the broader literature, give you tips to help develop your critical reading and provide you with a background understanding of relevant concepts and theory, which will make your reading more efficient and effective.</p> <p>Ask STORM, Perplexity (Deep Research), or  Google Deep Research (costs) to prepare a subject review (follow up, check and add to the sources used by the AI tool):</p> <p>Prompt</p> <p>Can you write a literature review on land cover classification methods using remote sensing images?</p> <p>Prompt</p> <p>Can you write a literature review on harvest weed seed control?</p> <p>Prompt</p> <p>Can you write a review on Western Australian drylands salinity challenges and management practices?</p> <p>Prompt</p> <p>Can you write a review of methods to segment field boundaries from satellite images? Can you focus on approaches most suited to smallholder cropping systems?</p> <p>Prompt</p> <p>Can you write a review on methods to classify crop type from remote sensing images? </p> <p>I want to create an overview of organisation\u2019s resources / policies</p> <p>Ask Google Deep Research (costs), Perplexity (Deep Research or Pro Search) or ChatGPT (with Search and Reason turned on) to complete a web search and generate a report (follow up, check and add to the sources used by the AI tool):</p> <p>Prompt</p> <p>Can you write a review of the Western Australian Government\u2019s Department of Primary Industry and Regional Development\u2019s recommendations for constructing farm dams?</p> <p>I have 10 minutes and want a quick overview of a paper</p> <ul> <li>Upload the paper to NotebookLM and ask it to create a short podcast conversation about the paper</li> <li>Upload the paper to NotebookLM and ask it to generate a study guide / briefing document</li> <li>Upload the paper to NotebookLM and ask it to generate a blog-style review of the paper</li> </ul> <p>I want to test my understanding of a paper / topic</p> <ul> <li>Upload the paper to NotebookLM and prompt it to generate a study quiz for you</li> <li>Upload the paper to NotebookLM and ask it to generate text for a set of Anki cards with answers</li> </ul> <p>I\u2019m struggling to understand an equation / figure / section of a paper</p> <ul> <li>Screen share with Google AI Studio and verbally ask for an explanation of the content that you\u2019re stuck with. Keep prompting until you are satisfied with your understanding.</li> </ul>"},{"location":"ai/hugging-face-and-colab-setup/","title":"Running your own LLMs: Hugging Face and Google Colab setup","text":"<p>There are advantages to using LLMs and AI tools locally (i.e. downloading the model to the machine you are working on and using it within a Python environment). One of the main advantages is that it lets you use LLMs with private or sensitive data as you are keeping the data on your machine or within an environment you control. </p> <p>The challenge with using AI tools that are not hosted (i.e. not available at chatgpt.com) is that they require some setup and often using dedicated hardware (a GPU). </p> <p>One of the easiest ways to get up and running with LLMs, and AI tools in general, is via Hugging Face. Hugging Face is a company that hosts trained models online and provides easy-to-use software tools for working with LLMs. To access models from Hugging Face you need to create an account and generate an access token that lets you authenticate your Python environment (to download models). </p> <p>Many LLMs will only run on computers with GPUs and smaller LLMs will be more performant on GPUs. Not all computers have GPUs capable of running LLMs; however, it's possible to access computers in the cloud with GPUs. Google Colab provides free access (with some usage limits) to a cloud computer with a Python environment and a GPU.</p> <p>This guide demonstrates how to setup Hugging Face and Google Colab to work with GPUs. </p> <p>Work through this notebook in Google Colab to setup your environment to download models from Hugging Face and run them using Hugging Face's Python tools:</p> <p> </p>"},{"location":"ai/hugging-face-and-colab-setup/#setup-hugging-face-token","title":"Setup Hugging Face token","text":"<p>Create a Hugging Face account:</p> <p></p> <p>Once, you have created a Hugging Face account you can gain access to models. Many LLMs require you accept terms and conditions. For example, Google's Gemma 2 2b it model is performant, recently trained and is performant on Google Colab instances (2b represents 2 billion parameters and it represents instruction tuned). Agree to the model's terms and conditions and usage policy here.</p> <p>Create an Access Token:</p> <p></p> <p>Click on Create new token</p> <p></p> <p>Set the token permissions</p> <p>You can initially set token permissions to Read, which has read access to all your resources. This is a good option for getting started.</p> <p></p> <p>However, as you start developing resources such as models and datasets and using Hugging Face in different environments, it's a good idea to create access tokens with fine-grained permissions with just enough permissions to complete tasks associated with the token.</p> <p></p> <p>If you have selected fine-grained permissions, you will need to add repositories (models) that you want that token to grant permission to.</p> <p></p> <p>Click Create token to generate the access token</p> <p></p> <p>Copy the access token. This is your only opportunity to do this - keep a record of the token (in a secure location). If you lose your token, it's easy to generate a new one.</p> <p></p> <p>In Google Colab, click on the key icon in the left-hand sidebar.</p> <p></p> <p>Add your Hugging Face access token with the name <code>HF_TOKEN</code> and make sure the Notebook access it checked. Restart your Google Colab session to load the token into your environment.</p> <p></p>"},{"location":"ai/hugging-face-and-colab-setup/#setup-google-colab-runtime","title":"Setup Google Colab runtime","text":"<p>Use Google Colab with a T4 GPU runtime type.</p> <p>Before running any code, set the runtime type to T4 GPU.</p> <p></p> <p></p> <p></p>"},{"location":"ai/overview-and-tools/","title":"Using AI for spatial and agricultural data analysis","text":""},{"location":"ai/overview-and-tools/#overview","title":"Overview","text":"<p>This is a short guide to using AI tools and AI generated content for spatial and agricultural data analysis tasks. It provides an overview of different AI tools that can be used for GIS, programming, literature review and general learning and topic understanding tasks. </p> <p>This guide covers:</p> <ul> <li>Overview of AI tools: an overview of AI tools for working with text, coding, images, video, audio, literature and search. </li> <li>Example uses of AI tools: a selection of examples using AI tools for GIS, programming, data visualisation, topic understanding, literature search and reviews and more. </li> <li>Prompt engineering: the quality of prompt you provide to AI tools determines the quality of generated response. This guide provides an overview of best practice and techniques for generating high quality prompts. </li> <li>Responsible AI: notes and resources on responsible use of AI tools. </li> <li>Verification and quality checking: tips for checking the quality, accuracy and factuality of AI generated content. </li> </ul> <p>This is a fast moving field, so these notes just scratch the surface and they\u2019re almost certain to be dated when you read this. </p> <p>There is a skew in the examples towards AI tools provided by Google, the only reason for this is that they\u2019re the tools I\u2019m more familiar using. However, the concepts and techniques discussed here should generalise for use with other AI platforms. </p> <p>These notes focus on freely available AI tools with the intention that they\u2019re broadly accessible; however, in places suggestions for using paid tools are provided. </p> <p>UWA AI resources</p> <p>At UWA, the first port of call for using AI tools should be the UWA Library AI resources.</p> <p>UWA prohibits the use of DeepSeek models.</p>"},{"location":"ai/overview-and-tools/#ai-tools","title":"AI tools","text":""},{"location":"ai/overview-and-tools/#llms-text-generation-and-chat-interfaces","title":"LLMs: text generation and chat interfaces","text":"<p>Large language models (LLMs) are the engine of many generative AI tools. LLMs are trained for next word prediction on massive text datasets and can be fine-tuned for a range of other tasks (e.g. code generation or question and answering). Examples of LLMs include Google\u2019s Gemini models, OpenAI\u2019s family of models and Meta\u2019s Llama models. </p> <p>These models can be accessed via chat interfaces in their apps or websites:</p> <ul> <li>Gemini</li> <li>ChatGPT</li> <li>Llama </li> <li>Copilot</li> <li>Claude</li> </ul> <p>These models can be used for a range of text generation and writing tasks:</p> <ul> <li>Question-answer</li> <li>Instruction generation</li> <li>Text summarisation</li> <li>Writing feedback</li> <li>Code generation</li> </ul>"},{"location":"ai/overview-and-tools/#search","title":"Search","text":"<p>LLMs are increasingly integrated with search. When you prompt the LLM an online search will be conducted and used to generate your response. This can improve the quality of the response (via grounding and reducing the risk of hallucinations), make online content more accessible (e.g. by summarising lengthy content and dense web pages or synthesising content from multiple resources) and help you verify the factuality of the response (by following links).</p> <p>The following have freely available search integrated with LLMs:</p> <ul> <li>Gemini (Flash 2.0)</li> <li>Perplexity </li> <li>ChatGPT </li> </ul> <p>The prompt and response below illustrate the benefits of integrating LLMs with search. The response has synthesised information from several web pages documenting John Deere's Operations Centre, identified relevant URLs (API endpoints) to request data from, provided links for you to verify the responses or for further research and provided a Python code example. Reflect on how long it would take you to do this by manually searching for relevant information in the John Deere Operations Centre documentation and then developing the associated Python code. </p> <p>Prompt</p> <p>How can I download as-applied herbicide application and seeding data from John Deere's Operations Centre? What API endpoints do I need to use? Can you provide a Python code example to download seeding data?</p> Response (Perplexity) <p>To download as-applied herbicide application and seeding data from John Deere's Operations Centre, you should use the Field Operations API. This API provides access to data collected during field work, including application and seeding operations[1].</p> <p>API Endpoints</p> <ol> <li> <p>List Field Operations: GET https://sandboxapi.deere.com/platform/organizations/{orgId}/fields/{fieldId}/fieldOperations</p> </li> <li> <p>View a Field Operation: GET https://sandboxapi.deere.com/platform/fieldOperations/{operationId}</p> </li> </ol> <p>These endpoints allow you to retrieve metadata, totals, map images, and point-level data for field operations[1].</p> <p>Python Code Example for Downloading Seeding Data</p> <p>Here's a Python code example to retrieve seeding data using the Field Operations API:</p> <pre><code>import requests\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\n\n# OAuth2 setup\nclient_id = 'your_client_id'\nclient_secret = 'your_client_secret'\ntoken_url = 'https://authentication.deere.com/oauth2/token'\n\nclient = BackendApplicationClient(client_id=client_id)\noauth = OAuth2Session(client=client)\ntoken = oauth.fetch_token(token_url=token_url, client_id=client_id, client_secret=client_secret)\n\n# API request parameters\norg_id = 'your_organization_id'\nfield_id = 'your_field_id'\nurl = f'https://sandboxapi.deere.com/platform/organizations/{org_id}/fields/{field_id}/fieldOperations'\n\nheaders = {\n    'Accept': 'application/vnd.deere.axiom.v3+json',\n    'x-deere-signature': ''  # Leave empty for the first request\n}\n\nparams = {\n    'fieldOperationType': 'SEEDING',\n    'embed': 'measurementTypes'\n}\n\n# Make the API request\nresponse = oauth.get(url, headers=headers, params=params)\n\nif response.status_code == 200:\n    seeding_data = response.json()\n    print(\"Seeding Data:\", seeding_data)\n\n    # Update x-deere-signature for subsequent requests\n    new_signature = response.headers.get('x-deere-signature')\n    if new_signature:\n        headers['x-deere-signature'] = new_signature\nelse:\n    print(\"Error:\", response.status_code, response.text)\n</code></pre> <p>To use this code: 1. Replace <code>'your_client_id'</code> and <code>'your_client_secret'</code> with your John Deere API credentials. 2. Replace <code>'your_organization_id'</code> and <code>'your_field_id'</code> with the appropriate IDs. 3. Install the required Python packages: <code>requests</code>, <code>oauthlib</code>, and <code>requests_oauthlib</code>.</p> <p>This code will authenticate with the John Deere API, retrieve seeding operations for the specified field, and print the data. You can modify the <code>fieldOperationType</code> parameter to 'APPLICATION' to retrieve herbicide application data instead[1].</p> <p>Remember to handle pagination, error checking, and implement proper data processing for production use.</p> <p>Citations: [1] https://developer.deere.com/dev-docs/field-operations [2] https://withleaf.io/providers/johndeere [3] https://developer.deere.com/whats-new [4] https://learn.microsoft.com/en-us/rest/api/data-manager-for-agri/dataplane/farm-operations/create-data-ingestion-job?view=rest-data-manager-for-agri-dataplane-2023-11-01-preview [5] https://developer.deere.com [6] https://docs.withleaf.io/docs/operations_endpoints [7] https://developer.deere.com/dev-docs/fields [8] https://github.com/RealmFive/my_john_deere_api</p> <p>Answer from Perplexity: pplx.ai/share</p> <p>LLM respones are probabilistic - always check them</p> <p>The first rule of working with LLMs is to check their responses. LLMs are a tool to help you be more efficient, an assistant to help you problem solve and an aid to bounce ideas off and clarify your thinking. They are not good at doing your job for you. They are not guaranteed to generate correct and useful answers. They are good at helping you become better at doing your job. </p> <p>LLMs generate their responses probabilistically, you may get different responses to the same prompt. These responses will vary in quality, relevance and sometimes with the contextual information you've already provided in your chat session. </p> <p>Look at the three responses below generated by Gemini to the same prompt. From a cursory take, response 2 is very generic, has not provided specific details on how to use the John Deere Operations Centre APIs and has not provided a Python example. Responses 1 and 3 look like better first starts for solving the problem providing more detail on API endpoints and a Python example. However, there are still inconsistencies such as suggesting using different API URLs.</p> <ul> <li>You should always verify and check the responses provided by LLMs</li> <li>Providing contextual information and more specific instructions can help LLMs generate more accurate and relevant responses</li> <li>For search tasks, consider including relevant URLs in the prompt (e.g. the API docs)</li> <li>For coding tasks, iterate with the LLM to generate a test example (e.g. a dummy dataset)</li> </ul> Response 1 (Gemini 2.0 Flash) <p>Obtaining as-applied herbicide application and seeding data from John Deere's Operations Center programmatically involves utilizing the John Deere Developer Platform and its APIs. Here's a breakdown of the process and key considerations:</p> <p>Key Concepts</p> <p>John Deere Developer Platform:</p> <ul> <li>This platform provides APIs that allow developers to access and integrate John Deere's data and services. https://developer.deere.com\u00a0 </li> <li>You'll need to create a developer account and obtain API credentials. https://developer.deere.com/precision/get-started </li> </ul> <p>Operations Center APIs:</p> <ul> <li>Specifically, the \"Field Operations API\" is crucial for retrieving as-applied data. https://developer.deere.com/dev-docs/field-operations\u00a0 </li> <li>This API allows you to access data related to various field operations, including seeding and application. https://developer.deere.com/dev-docs/field-operations\u00a0 </li> </ul> <p>Data Formats:</p> <ul> <li>Data can be retrieved in various formats, including shapefiles, which are commonly used for geospatial data. https://help.us.cropwise.com/hc/en-us/articles/27420397322643-Exporting-Shapefiles-from-John-Deere-Operations-Center</li> </ul> <p>General Process</p> <p>Developer Account and API Credentials:</p> <ul> <li>Register for a developer account on the John Deere Developer Platform.</li> <li>Create an application and obtain the necessary API keys and tokens.</li> </ul> <p>Authentication:</p> <ul> <li>Use your API credentials to authenticate with the John Deere API.</li> </ul> <p>API Requests:</p> <ul> <li>Construct API requests to retrieve the desired data.</li> <li>Use the Field Operations API to access as-applied data.</li> <li> <p>You'll likely need to specify parameters such as:</p> <ul> <li>Organization ID</li> <li>Field ID</li> <li>Date range</li> <li>Operation type (seeding, application)</li> </ul> </li> </ul> <p>Data Processing:</p> <ul> <li>Process the retrieved data as needed.</li> <li>If you receive shapefiles, you'll need to use geospatial libraries to work with them.</li> </ul> <p>API Endpoints:</p> <p>Field Operations API: </p> <ul> <li>This is the primary API for accessing as-applied data.</li> <li>Refer to the John Deere Developer Platform documentation for specific endpoints and parameters.</li> </ul> <p>Important Considerations:</p> <p>API Documentation:</p> <ul> <li>The John Deere Developer Platform documentation is essential for understanding the available APIs and how to use them.</li> </ul> <p>Authentication and Authorization:</p> <ul> <li>Properly handle API authentication and authorization to ensure secure access to data.</li> </ul> <p>Data Formats and Processing:</p> <ul> <li>Be prepared to work with various data formats, including geospatial data.</li> </ul> <p>Data Privacy:</p> <ul> <li>Respect data privacy and comply with all applicable regulations.</li> </ul> <p>Where to find the API documentation:</p> <ul> <li>The best location for the most up to date information is the John Deere developer web page, here is a link to the field operations API.</li> <li>Field Operations API - Develop with Deere https://developer.deere.com/dev-docs/field-operations</li> </ul> <p>Python Code Example (General Outline):</p> <p>Providing a fully functional Python code example is complex due to the authentication process and the nuances of the John Deere API. However, here's a general outline to illustrate the process:</p> <pre><code>import requests\nimport json\n\n# Replace with your API credentials\naccess_token = \"YOUR_ACCESS_TOKEN\"\norganization_id = \"YOUR_ORGANIZATION_ID\"\nfield_id = \"YOUR_FIELD_ID\"\n\n# API endpoint (example)\napi_url = f\"https://api.deere.com/platform/organizations/{organization_id}/fields/{field_id}/operations\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {access_token}\",\n    \"Accept\": \"application/json\",\n}\n\nparams = {\n    \"operationType\": \"seeding\", #or application\n    \"startDate\": \"2023-01-01\",\n    \"endDate\": \"2023-12-31\",\n}\n\ntry:\n    response = requests.get(api_url, headers=headers, params=params)\n    response.raise_for_status()  # Raise an exception for bad status codes\n    data = response.json()\n    print(json.dumps(data, indent=4)) #prints the json data in a readable format.\n\n    # Process the data (e.g., extract seeding data)\n    # ...\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error: {e}\")\n</code></pre> <p>Important Notes:</p> <ul> <li>This code is a basic example and requires adaptation to your specific needs.</li> <li>You'll need to install the requests library: <code>pip install requests</code></li> <li>It is very important to use the John Deere developer documentation, as API's can change.</li> </ul> <p>I hope this information is helpful.</p> <p>Sources and related content:</p> <p>Develop with Deere - John Deere: developer.deere.com</p> <p>Create a John Deere User Account - Develop with Deere: https://developer.deere.com/precision/get-started</p> <p>Field Operations API - Develop with Deere: https://developer.deere.com/dev-docs/field-operations</p> <p>Exporting Shapefiles from John Deere Operations Center: https://help.us.cropwise.com/hc/en-us/articles/27420397322643-Exporting-Shapefiles-from-John-Deere-Operations-Center</p> Response 2 (Gemini 2.0 Flash) <p>Accessing data from John Deere's Operations Center programmatically involves using their APIs (https://developer.deere.com). Here's a breakdown of the process and key considerations:</p> <p>Understanding the John Deere API</p> <p>Developer Platform:</p> <ul> <li>John Deere provides a developer platform (developer.deere.com) with documentation and resources for accessing their APIs. This is the primary source for accurate and up-to-date information.</li> </ul> <p>API Capabilities:</p> <p>The APIs allow you to retrieve various types of agricultural data, including:</p> <ul> <li>Field data (boundaries, soil information) https://www.climatefieldview.ca/partners/john-deere-operations-center/</li> <li>Machine data (location, performance) https://developer.deere.com/dev-docs/machine-locations</li> <li>Application data (seeding, spraying) https://help.agworld.com/en/articles/5207825-john-deere-frequently-asked-questions</li> <li>Map layers https://developer.deere.com/dev-docs/map-layers</li> </ul> <p>Authentication:</p> <ul> <li>Accessing the APIs requires authentication, typically using OAuth 2.0. You'll need to register an application with John Deere to obtain the necessary credentials. https://developer.deere.com/dev-docs/operators</li> </ul> <p>API Documentation:</p> <ul> <li>The John Deere developer website is the best source for finding the exact API endpoints. Specifically look at the \"Map layers API\" documentation.</li> </ul> <p>Key Considerations</p> <p>Data Formats:</p> <ul> <li>Data may be available in various formats, such as JSON or geospatial formats. You'll need to handle these formats in your code. https://developer.deere.com/dev-docs/field-operations</li> </ul> <p>Permissions:</p> <ul> <li>Access to specific data may require appropriate permissions from the John Deere Operations Center account holder. https://www.rdoequipment.com/resources/blogs/john-deere-operations-center-faqs</li> </ul> <p>API Changes:</p> <ul> <li>APIs can change over time, so it's essential to stay updated with the latest documentation. https://withleaf.io/en/blog/breaking-changes-in-apis/</li> </ul> <p>General Process</p> <p>Obtain API Credentials:</p> <ul> <li>Register an application on the John Deere developer platform.</li> <li>Obtain your API key, secret, and other necessary credentials.</li> </ul> <p>Authentication:</p> <ul> <li>Use OAuth 2.0 to authenticate your application and obtain an access token.</li> </ul> <p>API Requests:</p> <ul> <li>Use HTTP requests (GET, POST, etc.) to interact with the John Deere APIs.</li> <li>Specify the appropriate API endpoints and parameters to retrieve the desired data.</li> </ul> <p>Data Processing:</p> <ul> <li>Parse the API responses and process the data as needed.</li> <li>Where to find the API endpoints:</li> </ul> <p>The John Deere developer website is the primary resource. Specifically the map layers API.</p> <p>Important Notes:</p> <ul> <li>Due to the sensitive nature of agricultural data and the potential for API changes, providing a complete and always-accurate code example is difficult.</li> <li>Always refer to the official John Deere developer documentation for the most up-to-date information.</li> <li>When dealing with map layers, and application data, you will be dealing with Geospacial data, so having a good understanding of that type of data is very helpful.</li> </ul> <p>I hope this information is helpful.</p> Response 3 (Gemini 2.0 Flash) <p>It's important to understand that accessing precise, real-time data from John Deere's Operations Center requires adherence to their developer guidelines and API usage. Here's a breakdown of how to approach this, combining general API principles with specific John Deere information:</p> <p>Key API Areas:</p> <p>Field Operations API:</p> <ul> <li>This API is crucial for retrieving as-applied data, including seeding and herbicide application information. https://developer.deere.com/dev-docs/field-operations</li> </ul> <p>It allows access to:</p> <ul> <li>Metadata about field operations. https://developer.deere.com/dev-docs/field-operations</li> <li>Totals for areas worked and amounts applied/planted. https://developer.deere.com/dev-docs/field-operations</li> <li>Map images.</li> <li>Point-level data (often available as shapefile exports). https://developer.deere.com/dev-docs/field-operations</li> </ul> <p>Map Layers API:</p> <ul> <li>This API is important for getting the visual representation of the data, and can be used in conjunction with the field operations API.</li> </ul> <p>Products API:</p> <ul> <li>This API is used to get information about the products used in the field, like the specific type of seed or herbicide. https://developer.deere.com/dev-docs/products</li> </ul> <p>General Workflow</p> <p>Developer Access:</p> <ul> <li>Begin by registering on the John Deere developer platform (developer.deere.com). This provides access to the necessary documentation and API keys. https://developer.deere.com/precision/get-started </li> </ul> <p>Authentication:</p> <ul> <li>Implement OAuth 2.0 authentication to obtain access tokens for your API requests.</li> </ul> <p>API Calls:</p> <ul> <li>Use the Field Operations API to query for specific application or seeding data.</li> </ul> <p>Parameters will be needed to filter by:</p> <ul> <li>Date range.</li> <li>Field boundaries.</li> <li>Operation type (seeding, application).</li> <li>Use the product API to get information about the products that where used.</li> <li>Data Handling:</li> <li>Process the API responses, which may be in JSON or geospatial formats (like shapefiles).</li> </ul> <p>You'll likely need libraries for:</p> <ul> <li>HTTP requests (e.g., requests in Python).</li> <li>JSON parsing.</li> <li>Geospatial data manipulation (e.g., geopandas in Python).</li> </ul> <p>Important Considerations:</p> <p>API Documentation:</p> <ul> <li>The John Deere developer portal is the definitive source for API details. Pay close attention to endpoint URLs, request parameters, and response formats.</li> </ul> <p>Data Formats:</p> <ul> <li>Be prepared to handle geospatial data. Shapefiles are a common format for as-applied information.</li> </ul> <p>Permissions:</p> <ul> <li>Ensure you have the necessary permissions within the John Deere Operations Center to access the data.</li> </ul> <p>Python Example (Conceptual):</p> <pre><code>import requests\nimport json\n\n# Replace with your actual credentials\naccess_token = \"YOUR_ACCESS_TOKEN\"\nfield_id = \"YOUR_FIELD_ID\"\n\n# API endpoint (example - refer to John Deere docs)\napi_url = f\"https://developer.deere.com/maplayers/v2/organizations/fields/{field_id}/operations\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {access_token}\",\n    \"Accept\": \"application/json\",\n}\n\nparams = {\n    \"operationType\": \"seeding\", #or \"application\"\n    \"startDate\": \"2024-01-01\",\n    \"endDate\": \"2024-12-31\",\n}\n\ntry:\n    response = requests.get(api_url, headers=headers, params=params)\n    response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n    data = response.json()\n    print(json.dumps(data, indent=4)) #prints the json response.\n    # Process the 'data' JSON to extract seeding information\n    # ... (code to parse and handle data) ...\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error: {e}\")\n</code></pre> <p>Key Notes about the Python example:</p> <ul> <li>This is a basic example. You'll need to adapt it to the specific John Deere API endpoints and data structures.</li> <li>Error handling is essential.</li> <li>You will need to install the requests library. pip install requests</li> <li>Geopandas is very useful for working with the geospatial data that is often returned. pip install geopandas</li> </ul> <p>I strongly advise you to carefully review the official John Deere developer documentation for the most accurate and current API information.</p> <p>Sources and related content:</p> <p>https://developer.deere.com/dev-docs/field-operations</p> <p>https://developer.deere.com/dev-docs/products</p> <p>https://developer.deere.com/precision/get-started</p>"},{"location":"ai/overview-and-tools/#thinking-and-reasoning-models","title":"Thinking and reasoning models","text":"<p>The latest AI models (at the time of writing in February 2025) are being released with \"thinking\" or \"reasoning\" versions. When generating a response to your prompt, these models use a \"thinking\" process where they break a problem down into a series of discrete steps, explore different options and explain the process used to create the response.</p> <p>The following are models with \"thinking\" or \"reasoning\" capabilities:</p> <ul> <li>Gemini (Flash Thinking 2.0)</li> <li>Perplexity </li> <li>ChatGPT</li> </ul> <p>Note, with Perplexity you can access ChatGPT's o3-mini, their most advanced reasoning model as of February 2025, with the free account.</p> <p>These models are well suited to larger and more complex reasoning problems; \"thinking\" models are useful for generating responses to problems that comprise several steps with multiple possible solutions. For each step, the model needs to reason and consider which solution is best. </p> <p>\"Thinking\" models are good candidates for generating templates and examples of end-to-end geospatial workflows that encompass several sub-tasks such as data processing, exploratory analysis, spatial analysis operations and reporting and data visualisation. A \"thinking\" model can focus on each of these steps in-turn. Look at the below example generating a workflow to interpolate soil samples to create a continuous spatial surface; it's generated detailed notes and points for you to consider for each stage in this digital soil mapping task.</p> <p>Using the responses of \"thinking\" models</p> <p>Gemini Flash Thinking 2.0 and Perplexity report the model's \"thinking\" steps when problem solving and generating your response. Use this as a template for your own computational thinking and problem solving where you break large complex problems down into discrete tasks.</p> <p>\"Thinking\" models generate detailed responses with an explanation and justification for each step. Use the response as an entry point to critically and analytically consider the problem and your solution. Do you agree with the model's justifications and choices? Are there aspects the model has not considered? Has the model considered things you had not thought of? </p> <p>Prompt</p> <p>As a senior geospatial analyst, can you develop a workflow to interpolate a sample of soil properties, collected at various locations within a field, to create a continuous spatial surface?</p> <p>Can you provide an explanation and justification for each step in the workflow and accompanying Python code for each step?</p> Response (Gemini Flash Thinking 2.0) <p>Thinking trace Here's a breakdown of the thinking process to construct the workflow for soil property interpolation:</p> <p>Deconstruct the Request:  Identify the core needs. The user wants:</p> <ul> <li>A workflow for interpolating soil properties.</li> <li>Continuous spatial surface as output.</li> <li>Justification for each step.</li> <li>Python code for each step.</li> <li>Senior Geospatial Analyst perspective (implying best practices and consideration of choices).</li> <li>Core Geospatial Analysis Task:  Recognize this is a spatial interpolation problem.  Recall standard interpolation techniques and their suitability for soil data.  Think about common methods: Inverse Distance Weighting (IDW), Kriging, Spline, Nearest Neighbor.</li> </ul> <p>Workflow Structure - Start with the Big Picture: A workflow is sequential. What are the high-level stages in any geospatial analysis like this?  Generally:</p> <ul> <li>Data Input &amp; Preparation.</li> <li>Method Selection &amp; Application.</li> <li>Output &amp; Evaluation.</li> <li>Refinement/Iteration.</li> <li>Data Input &amp; Preparation - Break Down the First Stage: What's needed to use sample data?</li> </ul> <p>Data Input: Need the sample data itself (soil property values and locations). What format? CSV, Shapefile, GeoJSON are common. Decision: Start with CSV as it's easily manipulated in Python and often a common starting point.</p> <p>Data Exploration (EDA): Crucial. Can't just jump to interpolation. Need to understand the data before applying methods. What to look for? </p> <ul> <li>Basic stats (min, max, mean, std dev).</li> <li>Histograms, boxplots (distribution).</li> <li>Spatial distribution plot (are samples clustered, evenly spaced?).</li> <li>Trend analysis (directional trends in the data? May affect interpolation method).</li> <li>Outlier detection (anomalous values that could skew interpolation). Important note: Outliers in soil data can be real, not just errors, so handle with care.</li> <li>Data Quality Check: Ensure data integrity - missing values, coordinate system. Decision: Address missing values by either removing or potentially imputing (but keep it simple for this workflow; removing for now is reasonable first step). Ensure coordinate system is defined/consistent.</li> <li>Method Selection &amp; Application - The Heart of the Workflow: Now choose the interpolation method.</li> </ul> <p>Consider Options: IDW is simple, Kriging is geostatistically robust (if assumptions met), Spline is good for smooth surfaces, Nearest Neighbor creates Thiessen polygons (different type of \"continuous\" surface, maybe less ideal for smooth soil properties).</p> <ul> <li>Justification for Kriging (as primary): Kriging is often favored for soil data because it considers spatial autocorrelation (soil properties tend to be more similar closer together). It also provides variance estimates (uncertainty). This is a stronger method conceptually than IDW for this application.</li> <li>Justification for IDW (as alternative/comparison): IDW is simpler and faster, good for comparison or when Kriging assumptions are difficult to meet or computationally intensive for large datasets. It's a good \"baseline\".</li> </ul> <p>Parameterization: Each method has parameters. For Kriging: variogram modeling is key. For IDW: power parameter. Decision: Briefly mention variogram modeling as a key step in Kriging but focus on the conceptual workflow first, then add a note about needing more detailed variogram modeling if actually implementing. For IDW, mention power parameter and how to potentially optimize (though no code for optimization in this initial workflow \u2013 keep it focused).</p> <p>Output &amp; Evaluation - How to Know if it's Good?:  Just interpolating isn't enough. Need to assess the result.</p> <ul> <li>Visualization: Essential to see the interpolated surface. Maps are the core output.</li> <li>Validation: Quantitatively assess accuracy. Common methods:</li> <li>Hold-out Validation: Split data into training and validation sets. Interpolate using training, predict at validation locations, compare predicted vs. actual.</li> <li>Cross-validation: Leave-one-out cross-validation (LOOCV) is common for geostatistics. Iteratively remove each point, interpolate using the rest, predict at the removed location, compare to actual. Calculate error metrics (RMSE, MAE, etc.).</li> <li>Uncertainty Assessment (for Kriging): Kriging provides variance/standard deviation surfaces. Map these to understand where predictions are more or less certain.</li> <li>Refinement/Iteration - Geospatial Analysis is Often Iterative:  Analysis is rarely perfect on the first try.</li> </ul> <p>Parameter Tuning: Adjust method parameters (variogram model in Kriging, power in IDW) based on validation results or expert knowledge.</p> <p>Method Comparison: Compare different interpolation methods (if you tried Kriging, maybe also try IDW, or Spline, etc.) and see which performs best.</p> <p>Data Re-evaluation: If results are poor, revisit data exploration. Are there outliers, trends not accounted for, data quality issues missed? Could more data collection be beneficial?</p> <p>Python Code Implementation - Choose Libraries &amp; Keep it Practical:</p> <ul> <li>Core Libraries: pandas (data manipulation), numpy (numerical operations), matplotlib &amp; seaborn (plotting), scipy.interpolate (IDW, potentially RBF), geopandas (spatial data handling if using shapefiles, though CSV initially chosen for simplicity), scikit-learn (validation metrics).</li> <li>Geostatistical Libraries: pykrige is a good choice for Kriging in Python. gstools is another option, more comprehensive but potentially steeper learning curve. Decision: Use pykrige as it's relatively focused on Kriging and user-friendly for this kind of workflow demonstration.</li> <li>Code Structure: Break down the code into functions for each step (data loading, EDA, interpolation, validation, visualization) to make it modular and readable.</li> <li>Comments and Explanations: Crucial! Code should be well-commented, and each code block in the workflow needs a clear explanation of what it's doing and why. Justify library choices and function usage.</li> </ul> <p>Senior Geospatial Analyst Perspective - Add the Expert Touch:  Throughout the workflow and justifications, emphasize:</p> <ul> <li>Method Choice Rationale: Explain why Kriging (and IDW) are chosen, the strengths and weaknesses.</li> <li>Data Quality Importance: Stress the need for thorough data exploration and quality checks.</li> <li>Validation is Essential: Highlight that interpolation is not just about making a pretty map, but about creating a reliable surface, and validation proves (or disproves) reliability.</li> <li>Context Matters: Remind the user that the \"best\" workflow depends on the specific data, soil property, and project goals. This workflow is a solid starting point, not a one-size-fits-all solution.</li> <li>Iteration and Refinement: Emphasize that geospatial analysis is iterative.</li> <li>Review and Refine:  After drafting the workflow, review it logically. Does it flow well? Are justifications clear and strong? Is the Python code practical and well-explained?  Are there any missing steps or important considerations?  For example, initially I might forget to explicitly mention coordinate systems in data prep \u2013 need to add that. Or I might not have initially included uncertainty assessment from Kriging - important to add.  Ensure the language is appropriate for a \"senior geospatial analyst\".</li> </ul> <p>By following these steps \u2013 deconstructing the request, thinking about core geospatial concepts, structuring a workflow, detailing each stage with justifications, providing code examples, and adding expert perspective \u2013 we can build a comprehensive and helpful answer to the user's request.</p> <p>Final generated response</p> <p>Certainly! As a senior geospatial analyst, developing a robust workflow for interpolating soil properties is a fundamental task with significant implications for precision agriculture, environmental modeling, and land management. Here's a detailed workflow, explanation, justification, and accompanying Python code for each step, designed to create a continuous spatial surface of soil properties from point samples.</p> <p>Workflow for Soil Property Interpolation This workflow is structured into sequential steps, starting with data preparation and ending with result validation and refinement.</p> <p>Data Collection and Input:</p> <ul> <li>Explanation: This initial step involves gathering your soil property samples and their corresponding geographic locations. Accurate and representative sampling is crucial for reliable interpolation. The data should include at least:</li> <li>Location Coordinates: Latitude and Longitude (or projected coordinates).</li> <li>Soil Property Value: The measured value of the soil property of interest (e.g., pH, organic matter, nutrient concentration).</li> </ul> <p>Justification: The quality of the input data directly determines the quality of the output surface. Representative sampling ensures that the spatial variability of the soil property is captured adequately. Georeferencing is essential for spatial analysis and interpolation.</p> <p>Python Code (Illustrative - Data Input from CSV):</p> <pre><code>import pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# --- Step 1: Data Input ---\ndef load_soil_data(csv_filepath):\n    \"\"\"\n    Loads soil sample data from a CSV file.\n\n    Args:\n        csv_filepath (str): Path to the CSV file.\n\n    Returns:\n        geopandas.GeoDataFrame: GeoDataFrame with soil data and geometry.\n    \"\"\"\n    try:\n        soil_data = pd.read_csv(csv_filepath)\n        # Assuming CSV has columns 'Latitude', 'Longitude', and 'Soil_Property'\n        # Create geometry column from lat/lon\n        geometry = [Point(xy) for xy in zip(soil_data['Longitude'], soil_data['Latitude'])]\n        geo_df = gpd.GeoDataFrame(soil_data, crs=\"EPSG:4326\", geometry=geometry) # Assuming WGS84, adjust CRS if needed\n        print(\"Data loaded successfully.\")\n        return geo_df\n    except FileNotFoundError:\n        print(f\"Error: File not found at {csv_filepath}\")\n        return None\n    except KeyError as e:\n        print(f\"Error: Missing required columns in CSV: {e}\")\n        return None\n\n# Example usage:\nsoil_data_gdf = load_soil_data('soil_samples.csv') # Replace 'soil_samples.csv' with your file path\n\nif soil_data_gdf is not None:\n    print(soil_data_gdf.head())\n</code></pre> <p>Exploratory Data Analysis (EDA):</p> <ul> <li>Explanation: Before interpolation, it's essential to understand the characteristics of your soil data. EDA involves:</li> <li>Descriptive Statistics: Calculate mean, median, standard deviation, minimum, maximum, skewness, kurtosis to understand the data distribution.</li> <li>Histograms and Boxplots: Visualize the data distribution to identify normality or skewness and potential outliers.</li> <li>Spatial Distribution Mapping: Plot the sample locations and visualize the soil property values at these locations to get a sense of spatial patterns (e.g., clustering, trends). Variography (Optional, but highly recommended for Kriging later): If using Kriging, preliminary variogram analysis can provide insights into spatial autocorrelation.</li> </ul> <p>Justification: EDA helps in:</p> <ul> <li>Data Quality Check: Identifying potential errors, outliers, or unusual data points that might need investigation or correction.</li> <li>Understanding Data Distribution: Informing the choice of interpolation method. Some methods are more sensitive to data distribution than others. For instance, Kriging often assumes normality (or transformable to normality).</li> <li>Spatial Pattern Recognition: Guiding the selection of interpolation parameters and helping to understand the underlying spatial processes.</li> </ul> <p>Python Code:</p> <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# --- Step 2: Exploratory Data Analysis (EDA) ---\ndef perform_eda(soil_gdf, property_column='Soil_Property'):\n    \"\"\"\n    Performs Exploratory Data Analysis on soil data.\n\n    Args:\n        soil_gdf (geopandas.GeoDataFrame): GeoDataFrame with soil data.\n        property_column (str): Name of the soil property column.\n    \"\"\"\n    if soil_gdf is None:\n        print(\"No data loaded for EDA.\")\n        return\n\n    print(\"\\n--- Exploratory Data Analysis ---\")\n    print(f\"\\nDescriptive Statistics for {property_column}:\\n\", soil_gdf[property_column].describe())\n\n    plt.figure(figsize=(14, 6))\n\n    # Histogram\n    plt.subplot(1, 2, 1)\n    sns.histplot(soil_gdf[property_column], kde=True)\n    plt.title(f'Histogram of {property_column}')\n\n    # Boxplot\n    plt.subplot(1, 2, 2)\n    sns.boxplot(y=soil_gdf[property_column])\n    plt.title(f'Boxplot of {property_column}')\n    plt.tight_layout()\n    plt.show()\n\n    # Spatial Distribution Map\n    plt.figure(figsize=(8, 8))\n    geo_plot = soil_gdf.plot(column=property_column, legend=True, markersize=50, cmap='viridis') # Customize cmap as needed\n    geo_plot.set_title(f'Spatial Distribution of {property_column}')\n    geo_plot.set_axis_off() # Optional: Remove axis ticks and labels\n    plt.show()\n\n\nperform_eda(soil_data_gdf)\n</code></pre> <p>Data Preprocessing (Handling Outliers and Trends):</p> <ul> <li>Explanation: Based on EDA, you might need to preprocess your data:</li> <li>Outlier Handling: Identify and decide how to treat outliers. Outliers could be genuine extreme values or errors. Decisions might include:</li> <li>Removal: If deemed errors, outliers can be removed (with caution and justification).</li> <li>Capping/Winsorizing: Limit extreme values to a certain percentile.</li> <li>Transformation: Transform data to reduce the influence of outliers (e.g., log transformation).</li> <li>Trend Analysis and Detrending: If EDA reveals a spatial trend (e.g., property values systematically increase from north to south), you might want to detrend the data. This involves:</li> <li>Trend Surface Analysis: Fit a polynomial surface to model the trend.</li> <li>Detrending: Subtract the trend surface from the original data. Interpolation is then performed on the residuals, and the trend is added back to the interpolated surface later.</li> </ul> <p>Justification:</p> <ul> <li>Outlier Mitigation: Outliers can disproportionately influence some interpolation methods (like IDW) and lead to inaccurate surfaces. Handling them appropriately improves the robustness of the interpolation.</li> <li>Trend Removal: Interpolation methods often assume stationarity (that spatial variability is consistent across the area). Trends violate this assumption. Detrending can improve the performance of methods like Kriging by making the residuals more stationary.</li> </ul> <p>Python Code (Illustrative - Outlier Capping and Simple Detrending):</p> <pre><code># --- Step 3: Data Preprocessing ---\ndef preprocess_data(soil_gdf, property_column='Soil_Property', outlier_threshold_percentile=99, detrend=False):\n    \"\"\"\n    Preprocesses soil data by handling outliers and detrending (optional).\n\n    Args:\n        soil_gdf (geopandas.GeoDataFrame): GeoDataFrame with soil data.\n        property_column (str): Name of the soil property column.\n        outlier_threshold_percentile (int): Percentile for outlier capping (e.g., 99th percentile).\n        detrend (bool): Whether to perform simple detrending (not implemented robustly here, for illustration).\n\n    Returns:\n        geopandas.GeoDataFrame: Preprocessed GeoDataFrame.\n    \"\"\"\n    if soil_gdf is None:\n        print(\"No data to preprocess.\")\n        return None\n\n    print(\"\\n--- Data Preprocessing ---\")\n\n    # Outlier Capping (example: capping values above 99th percentile)\n    threshold_value = np.percentile(soil_gdf[property_column], outlier_threshold_percentile)\n    soil_gdf['Processed_Property'] = soil_gdf[property_column].clip(upper=threshold_value) # Create a new processed column\n    print(f\"Outliers capped above {outlier_threshold_percentile}th percentile (value: {threshold_value:.2f}).\")\n\n    # Simple Detrending (Illustrative - Linear Trend based on Longitude)\n    if detrend: # This is a very basic example, more robust trend analysis is needed in practice\n        X = soil_gdf['Longitude'].values.reshape(-1, 1) # Longitude as predictor\n        y = soil_gdf['Processed_Property'].values\n        from sklearn.linear_model import LinearRegression\n        model = LinearRegression()\n        model.fit(X, y)\n        trend = model.predict(X)\n        soil_gdf['Detrended_Property'] = soil_gdf['Processed_Property'] - trend\n        print(\"Simple linear detrending performed based on Longitude.\")\n        property_column = 'Detrended_Property' # Interpolate detrended property\n    else:\n        soil_gdf['Detrended_Property'] = soil_gdf['Processed_Property'].copy() # Just for consistency if not detrending\n        property_column = 'Detrended_Property' # Interpolate capped property if not detrending\n\n    print(\"Preprocessing completed.\")\n    return soil_gdf, property_column # Return the processed GeoDataFrame and the column to interpolate\n\nsoil_data_gdf_processed, property_column_to_interpolate = preprocess_data(soil_data_gdf, detrend=False) # Adjust detrend=True to enable simple detrending\nif soil_data_gdf_processed is not None:\n    print(soil_data_gdf_processed.head())\n</code></pre> <p>Interpolation Method Selection and Parameterization:</p> <ul> <li>Explanation: Choose an appropriate interpolation method based on data characteristics, desired output, and assumptions. Common methods include:</li> <li>Inverse Distance Weighting (IDW): Simple, deterministic method. Assumes closer points have more influence. Parameters include power parameter (controlling distance decay) and search radius.</li> <li>Kriging: Geostatistical method that considers spatial autocorrelation. More complex but often more accurate for spatially correlated data like soil properties. Requires variogram modeling (to quantify spatial autocorrelation). Types include Ordinary Kriging, Simple Kriging, Universal Kriging, etc.</li> <li>Spline: Deterministic, smooth surface interpolation. Good for gently varying phenomena. Parameters include spline type and tension.</li> <li>Nearest Neighbor (Thiessen Polygons): Creates discrete zones around sample points. Not typically used for creating a continuous surface in the same sense as other methods, but useful in some contexts.</li> </ul> <p>Justification:</p> <ul> <li>Method Choice Matters: Different methods have different assumptions and produce different results. The \"best\" method depends on the specific data and objectives. Kriging often preferred for soil properties: Because it explicitly models spatial autocorrelation, which is a fundamental characteristic of soil properties. It also provides estimates of prediction uncertainty. IDW as a simpler alternative or baseline: IDW is easy to implement and understand, useful as a comparison method or when Kriging assumptions are difficult to meet.</li> <li>Parameter Tuning: Each method requires parameterization. These parameters significantly affect the interpolation outcome. Parameters should be chosen based on data characteristics and cross-validation (see step 6).</li> </ul> <p>Python Code (Illustrative - IDW and Ordinary Kriging using scipy and pykrige):</p> <pre><code>from scipy.interpolate import griddata\nfrom pykrige.ok import OrdinaryKriging\nimport numpy as np\n\n# --- Step 4: Interpolation Method and Parameterization ---\ndef perform_interpolation(soil_gdf, property_column='Detrended_Property', method='kriging', grid_resolution=50):\n    \"\"\"\n    Performs interpolation using selected method (IDW or Kriging).\n\n    Args:\n        soil_gdf (geopandas.GeoDataFrame): GeoDataFrame with soil data.\n        property_column (str): Name of the property column to interpolate.\n        method (str): Interpolation method ('idw' or 'kriging').\n        grid_resolution (int): Number of grid cells per dimension for output raster.\n\n    Returns:\n        tuple: Interpolated raster (numpy array), extent of interpolation, method name.\n    \"\"\"\n    if soil_gdf is None:\n        print(\"No data for interpolation.\")\n        return None, None, method\n\n    print(f\"\\n--- Performing {method.upper()} Interpolation ---\")\n\n    # Define grid for interpolation\n    min_lon, min_lat = soil_gdf.bounds.minx.min(), soil_gdf.bounds.miny.min()\n    max_lon, max_lat = soil_gdf.bounds.maxx.max(), soil_gdf.bounds.maxy.max()\n    lon_grid = np.linspace(min_lon, max_lon, grid_resolution)\n    lat_grid = np.linspace(min_lat, max_lat, grid_resolution)\n    lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n\n    points = soil_gdf[['Longitude', 'Latitude']].values\n    values = soil_gdf[property_column].values\n\n    if method == 'idw':\n        # Inverse Distance Weighting (IDW)\n        interpolated_raster = griddata(points, values, (lon_mesh, lat_mesh), method='linear') # 'linear' is a form of IDW\n        print(\"IDW Interpolation performed.\")\n\n    elif method == 'kriging':\n        # Ordinary Kriging\n        try:\n            OK = OrdinaryKriging(soil_gdf['Longitude'].values, soil_gdf['Latitude'].values, values,\n                                variogram_model='spherical', # Choose variogram model (spherical, exponential, gaussian, etc.)\n                                verbose=False, enable_plotting=False)\n            interpolated_raster, _ = OK.execute('grid', lon_grid, lat_grid) # _ for variance if needed\n            print(\"Ordinary Kriging Interpolation performed.\")\n        except Exception as e:\n            print(f\"Error during Kriging: {e}. Please check Kriging parameters and data. Falling back to None for Kriging result.\")\n            interpolated_raster = None\n\n    else:\n        print(f\"Error: Method '{method}' not recognized. Please choose 'idw' or 'kriging'.\")\n        return None, None, method\n\n    return interpolated_raster, (min_lon, max_lon, min_lat, max_lat), method\n\n\ninterpolated_surface_idw, extent_idw, method_idw = perform_interpolation(soil_data_gdf_processed, method='idw')\ninterpolated_surface_kriging, extent_kriging, method_kriging = perform_interpolation(soil_data_gdf_processed, method='kriging')\n</code></pre> <p>Result Visualization and Mapping:</p> <ul> <li>Explanation: Visualize the interpolated surface to examine the spatial patterns and assess the results visually. This involves:</li> <li>Creating Raster Maps: Display the interpolated surface as a raster image with a color scale representing the soil property values.</li> <li>Overlaying Sample Points: Plot the original sample locations on top of the interpolated raster to see how the interpolated surface relates to the original data.</li> <li>Contour Maps (Optional): Generate contour lines to show gradients and levels of the soil property.</li> </ul> <p>Justification: Visualization is crucial for:</p> <ul> <li>Qualitative Assessment: Visually inspect the smoothness, continuity, and overall spatial patterns of the interpolated surface. Does it make sense based on your understanding of the area and soil processes?</li> <li>Identifying Artifacts: Detect potential interpolation artifacts or areas where the interpolation might be unreliable (e.g., bullseye effects around sample points, unrealistic sharp gradients).</li> <li>Communication: Maps are a powerful way to communicate the results to stakeholders and understand spatial variability.</li> </ul> <p>Python Code:</p> <pre><code># --- Step 5: Result Visualization and Mapping ---\ndef visualize_interpolation_result(raster, extent, method_name, property_name='Soil Property'):\n    \"\"\"\n    Visualizes the interpolated raster surface.\n\n    Args:\n        raster (numpy.ndarray): Interpolated raster grid.\n        extent (tuple): Extent of the raster (min_lon, max_lon, min_lat, max_lat).\n        method_name (str): Name of the interpolation method used.\n        property_name (str): Name of the soil property being visualized.\n    \"\"\"\n    if raster is None:\n        print(f\"No raster data to visualize for {method_name}.\")\n        return\n\n    print(f\"\\n--- Visualizing {method_name.upper()} Interpolation Result ---\")\n\n    min_lon, max_lon, min_lat, max_lat = extent\n\n    plt.figure(figsize=(10, 8))\n    img = plt.imshow(raster, extent=[min_lon, max_lon, min_lat, max_lat], origin='lower', cmap='viridis') # Customize cmap\n    plt.colorbar(img, label=property_name)\n    plt.title(f'Interpolated {property_name} Surface using {method_name.upper()}')\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.scatter(soil_data_gdf['Longitude'], soil_data_gdf['Latitude'], color='red', s=10, label='Sample Points') # Overlay sample points\n    plt.legend()\n    plt.axis('equal') # Ensure aspect ratio is maintained\n    plt.show()\n\n\nvisualize_interpolation_result(interpolated_surface_idw, extent_idw, method_idw, property_name=property_column_to_interpolate)\nvisualize_interpolation_result(interpolated_surface_kriging, extent_kriging, method_kriging, property_name=property_column_to_interpolate)\n</code></pre> <p>Validation and Accuracy Assessment:</p> <ul> <li>Explanation: Quantitatively assess the accuracy of the interpolation. Common validation techniques include:</li> <li>Hold-Out Validation: Split the original dataset into training and validation sets. Interpolate using the training set and predict values at the locations of the validation set. Compare predicted values with actual values from the validation set.</li> <li>Cross-Validation (e.g., Leave-One-Out Cross-Validation - LOOCV): For each sample point, temporarily remove it from the dataset. Interpolate using the remaining points and predict the value at the location of the removed point. Compare the predicted value with the actual value. Repeat for all sample points.</li> </ul> <p>Error Metrics: Calculate error metrics like:</p> <ul> <li>Mean Error (ME): Indicates bias (over- or under-prediction on average). Ideally close to zero.</li> <li>Mean Absolute Error (MAE): Average magnitude of errors.</li> <li>Root Mean Squared Error (RMSE): Penalizes larger errors more heavily than MAE. Sensitive to outliers.</li> <li>Correlation Coefficient (R): Measures the linear relationship between predicted and observed values.</li> </ul> <p>Justification: Validation is critical to:</p> <ul> <li>Quantify Accuracy: Provides numerical measures of how well the interpolation method predicts soil property values at un-sampled locations.</li> <li>Method Comparison: Allows for objective comparison of different interpolation methods or parameter settings to determine which performs best for your data.</li> <li>Uncertainty Assessment: Provides an indication of the reliability of the interpolated surface. Higher accuracy (lower error metrics) implies a more reliable surface.</li> </ul> <p>Python Code (Illustrative - Cross-Validation for IDW and Kriging):</p> <pre><code>from sklearn.model_selection import LeaveOneOut\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\n\n# --- Step 6: Validation and Accuracy Assessment ---\ndef validate_interpolation(soil_gdf, property_column='Detrended_Property', method='kriging', grid_resolution=50):\n    \"\"\"\n    Performs Leave-One-Out Cross-Validation (LOOCV) and calculates error metrics.\n\n    Args:\n        soil_gdf (geopandas.GeoDataFrame): GeoDataFrame with soil data.\n        property_column (str): Name of the property column to interpolate.\n        method (str): Interpolation method ('idw' or 'kriging').\n        grid_resolution (int): Grid resolution (not directly used in LOOCV here but kept for consistency).\n\n    Returns:\n        dict: Dictionary of error metrics (ME, MAE, RMSE) for validation.\n    \"\"\"\n    if soil_gdf is None:\n        print(\"No data for validation.\")\n        return None\n\n    print(f\"\\n--- Performing Cross-Validation for {method.upper()} ---\")\n\n    loo = LeaveOneOut()\n    predicted_values = []\n    actual_values = soil_gdf[property_column].values\n    coords = soil_gdf[['Longitude', 'Latitude']].values\n\n    for train_index, test_index in loo.split(coords):\n        train_coords, test_coord = coords[train_index], coords[test_index]\n        train_values, actual_value = actual_values[train_index], actual_values[test_index][0] # LOOCV returns single test index\n\n        if method == 'idw':\n            interp_function = lambda x, y: griddata(train_coords, train_values, (x, y), method='linear') # IDW function\n            predicted_value = interp_function(test_coord[0][0], test_coord[0][1]) # Predict at test coord\n\n        elif method == 'kriging':\n            try:\n                OK_loo = OrdinaryKriging(train_coords[:, 0], train_coords[:, 1], train_values,\n                                        variogram_model='spherical', verbose=False, enable_plotting=False)\n                predicted_value, _ = OK_loo.execute('points', np.array([test_coord[0][0]]), np.array([test_coord[0][1]])) # Predict at test coord\n                predicted_value = predicted_value[0] # Extract single prediction value\n\n            except Exception as e:\n                print(f\"Error during LOOCV Kriging: {e}. Setting predicted value to NaN.\")\n                predicted_value = np.nan\n\n        else:\n            print(f\"Error: Method '{method}' not recognized for validation.\")\n            return None\n\n        predicted_values.append(predicted_value)\n\n    predicted_values = np.array(predicted_values)\n    # Calculate Error Metrics, handle NaN values if Kriging failed for some LOOCV iterations\n    mask_valid = ~np.isnan(predicted_values) # Mask out NaN values from Kriging errors if any\n    me = np.mean(predicted_values[mask_valid] - actual_values[mask_valid])\n    mae = mean_absolute_error(actual_values[mask_valid], predicted_values[mask_valid])\n    rmse = np.sqrt(mean_squared_error(actual_values[mask_valid], predicted_values[mask_valid]))\n\n\n    error_metrics = {'ME': me, 'MAE': mae, 'RMSE': rmse}\n    print(f\"\\nValidation Metrics for {method.upper()}: {error_metrics}\")\n    return error_metrics\n\n\nvalidation_metrics_idw = validate_interpolation(soil_data_gdf_processed, method='idw')\nvalidation_metrics_kriging = validate_interpolation(soil_data_gdf_processed, method='kriging')\n</code></pre> <p>Refinement and Iteration (Optional but Highly Recommended):</p> <ul> <li>Explanation: Based on the validation results, you might need to refine your workflow. This could involve:</li> <li>Parameter Tuning: Adjusting parameters of the chosen interpolation method (e.g., power parameter in IDW, variogram model and parameters in Kriging). Parameter tuning can be done manually or using optimization techniques (e.g., grid search to minimize RMSE in cross-validation).</li> <li>Method Comparison: If validation shows poor performance with the initial method, try a different interpolation method. Data Re-evaluation: Revisit the EDA and preprocessing steps. Are there still outliers affecting the results? Was detrending effective? Could data transformations (e.g., log transformation) improve results?</li> <li>Further Data Collection: If accuracy is still insufficient, consider whether additional sampling is needed, especially in areas with high spatial variability or sparse data.</li> </ul> <p>Justification: Geospatial analysis is often iterative. Refinement based on validation leads to improved interpolation accuracy and a more reliable final surface. It's important to not just accept the first result, but to critically evaluate and improve upon it.</p> <p>Example Iteration (Illustrative - Parameter Tuning for IDW Power Parameter):</p> <pre><code># --- Step 7 (Example): Refinement - Parameter Tuning for IDW ---\ndef tune_idw_power(soil_gdf, property_column='Detrended_Property', power_values=range(1, 11)):\n    \"\"\"\n    Tunes the power parameter for IDW using LOOCV RMSE.\n\n    Args:\n        soil_gdf (geopandas.GeoDataFrame): GeoDataFrame with soil data.\n        property_column (str): Property column to interpolate.\n        power_values (range or list): Range of power values to test.\n\n    Returns:\n        dict: Dictionary with power values and corresponding RMSE values.\n    \"\"\"\n    print(\"\\n--- Tuning IDW Power Parameter ---\")\n    rmse_values = {}\n    for power in power_values:\n        loo = LeaveOneOut()\n        predicted_values = []\n        actual_values = soil_gdf[property_column].values\n        coords = soil_gdf[['Longitude', 'Latitude']].values\n\n        for train_index, test_index in loo.split(coords):\n            train_coords, test_coord = coords[train_index], coords[test_index]\n            train_values = actual_values[train_index]\n\n            # IDW with varying power parameter\n            interp_function = lambda x, y: griddata(train_coords, train_values, (x, y), method='linear') # Using 'linear' for simplicity here, adapt if needed\n            predicted_value = interp_function(test_coord[0][0], test_coord[0][1])\n            predicted_values.append(predicted_value)\n\n        predicted_values = np.array(predicted_values)\n        rmse = np.sqrt(mean_squared_error(actual_values, predicted_values)) # Calculate RMSE\n        rmse_values[power] = rmse\n        print(f\"Power: {power}, RMSE: {rmse:.4f}\")\n\n    best_power = min(rmse_values, key=rmse_values.get) # Power with minimum RMSE\n    print(f\"\\nOptimal IDW Power Parameter based on LOOCV RMSE: {best_power}\")\n    return rmse_values, best_power\n\n# Example usage:\nif soil_data_gdf_processed is not None:\n    idw_power_rmse, best_idw_power = tune_idw_power(soil_data_gdf_processed)\n    print(\"\\nRMSE values for different power parameters:\", idw_power_rmse)\n    print(\"Best Power Parameter:\", best_idw_power)\n\n    # Re-interpolate with the best power parameter (if applicable, not directly implemented with scipy griddata 'linear')\n    # For a proper IDW implementation where power is adjustable, you might use a library like `gstat` in R through rpy2, or implement IDW from scratch for full parameter control.\n    # Or use libraries like `verde` in Python which has more flexible IDW implementations.\n</code></pre> <p>Final Output:</p> <p>The workflow results in:</p> <ul> <li>A continuous raster surface: Representing the interpolated soil property values across the field.</li> <li>Validation metrics: Quantifying the accuracy and reliability of the interpolation.</li> <li>Visualization maps: For visual interpretation and communication of results.</li> </ul> <p>Important Considerations for Real-World Applications:</p> <ul> <li>Variogram Modeling for Kriging: The provided Kriging code uses a default 'spherical' variogram model. In practice, proper variogram analysis (empirical variogram calculation and model fitting) is crucial for optimal Kriging results. Libraries like pykrige and gstools offer robust variogram modeling capabilities.</li> <li>Computational Cost: Kriging can be computationally intensive, especially for large datasets. Consider efficiency and optimize code as needed. </li> <li>Software and Libraries: While Python is used here, other GIS software (like ArcGIS, QGIS) and statistical software (like R) also provide powerful tools for spatial interpolation. The choice depends on your specific needs, expertise, and available resources.</li> <li>Data Quality: Emphasize the importance of high-quality input data. Errors in sampling, measurement, or georeferencing will propagate through the interpolation process and affect the accuracy of the final surface.</li> <li>Domain Expertise: Soil science knowledge is essential to interpret the results, validate them against field observations, and understand the limitations of the interpolation.</li> </ul> <p>This detailed workflow and accompanying code provide a solid foundation for interpolating soil properties. Remember to adapt and refine the workflow based on the specific characteristics of your data and the goals of your analysis.</p> <p>\"Thinking\" models are also useful when there is a need to consider different options, for comparative analyses and for helping you weigh up the pros and cons of different methods and approaches to solving problems. Look at the example below that's considered the different methods for readiometric calibration of UAV multispectral images for a crop monitoring task. You can also see the reasoning capabilities of \"thinking\" models in the example above where the model considers the strengths and weaknesses of different spatial interpolation methods.</p> <p>Prompt</p> <p>I am capturing UAV images over a paddock with the intention of estimating crop biophysical parameters such as leaf area index and chlorophyll content. I need to make sure the UAV images are radiometrically calibrated. </p> <p>Can you review different radiometric calibration methods and assess their suitability for this task. The primary goal of this task is to obtain the highest quality multispectral images for subsequent crop biophysical parameter estimation.</p> Response (ChatGPT Reason) <p>Radiometric calibration is a crucial step in converting raw digital numbers (DN) from UAV multispectral images into actual surface reflectance values. This conversion is essential for accurately estimating crop biophysical parameters like leaf area index (LAI) and chlorophyll content. Below is a review of several radiometric calibration methods and an assessment of their suitability for high-quality multispectral imaging in precision agriculture:</p> <p>Empirical Line Calibration (ELC)</p> <p>Overview:</p> <ul> <li>Method: Establishes a linear relationship between the measured DN values in the image and known reflectance values obtained from calibration targets.</li> <li>Process: Involves capturing images of ground-based calibration targets (often with known reflectance values measured using a spectroradiometer) during the UAV flight. A regression model is then computed to convert DN values to reflectance.</li> </ul> <p>Pros:</p> <ul> <li>Direct and Robust: Provides a straightforward linear model that is robust when multiple targets across the reflectance range are used.</li> <li>Accuracy: Can yield high accuracy if the calibration targets are well characterized and properly measured.</li> </ul> <p>Cons:</p> <ul> <li>Field Dependency: Requires deploying and correctly positioning calibration panels in the field, which may add complexity and require extra fieldwork.</li> <li>Sensitivity: Errors in the field measurements of target reflectance or panel degradation can propagate into the calibrated image.</li> </ul> <p>Suitability:</p> <ul> <li>Highly suitable for crop biophysical parameter estimation when the necessary calibration targets are available and measured accurately. Its direct correlation with ground truth makes it a preferred method in many precision agriculture applications.</li> </ul> <p>Reflectance Panel Calibration</p> <p>Overview:</p> <ul> <li>Method: Utilizes standardized reflectance panels (e.g., Spectralon) placed within the imaging area. These panels serve as in-scene references.</li> <li>Process: UAV images include shots of the reflectance panels, and the known reflectance values of the panels are used to calibrate the image data.</li> </ul> <p>Pros:</p> <ul> <li>In-Flight Calibration: Captures the same illumination conditions as the target field, making the calibration more representative of the flight conditions.</li> <li>Consistency: When maintained properly, reflectance panels provide reliable, consistent reference data.</li> </ul> <p>Cons:</p> <ul> <li>Logistics: Requires physical access to the field to place and retrieve the panels.</li> <li>Environmental Sensitivity: Panels can be affected by dirt, damage, or improper placement, which may compromise calibration accuracy.</li> </ul> <p>Suitability:</p> <p>Very suitable for UAV-based agricultural imaging, especially when precise calibration is needed. This method is widely used in research and commercial applications where accurate reflectance is critical.</p> <p>Sensor-Based (Factory or Onboard) Calibration</p> <p>Overview:</p> <ul> <li>Method: Relies on the camera's internal calibration data and sensor metadata, sometimes supplemented by onboard downwelling light sensors.</li> <li>Process: The sensor\u2019s inherent calibration parameters (provided by the manufacturer) are used to adjust raw imagery. Some UAV systems integrate incident light sensors to dynamically adjust for changing light conditions during flight.</li> </ul> <p>Pros:</p> <ul> <li>Convenience: Eliminates the need for additional field calibration targets, reducing operational complexity.</li> <li>Automation: Often integrated into the UAV\u2019s processing pipeline, which can streamline data processing.</li> </ul> <p>Cons:</p> <ul> <li>Limited Adaptability: May not fully account for real-time changes in illumination conditions or atmospheric variability.</li> <li>Reliance on Manufacturer Data: Accuracy depends on the quality and stability of the sensor calibration provided by the manufacturer, which may degrade over time.</li> </ul> <p>Suitability:</p> <ul> <li>Suitable for routine monitoring where small errors in reflectance are acceptable, but for high-precision estimation of crop biophysical parameters, this method might be less ideal unless validated with periodic ground measurements.</li> </ul> <p>Vicarious Calibration</p> <p>Overview:</p> <ul> <li>Method: Uses naturally occurring targets or well-characterized reference sites (such as deserts or uniform surfaces) to adjust the imagery post-flight.</li> <li>Process: The reflectance properties of a known, stable surface are used to calibrate the UAV imagery indirectly.</li> </ul> <p>Pros:</p> <ul> <li>No Additional Equipment: Can be useful when deploying calibration panels is impractical. *Wide Applicability: Allows calibration even in remote or difficult-to-access areas.</li> </ul> <p>Cons:</p> <ul> <li>Complexity: Requires careful selection of reference sites and sophisticated atmospheric correction models.</li> <li>Uncertainty: Often less direct and can introduce additional uncertainties compared to on-site calibration methods.</li> </ul> <p>Suitability:</p> <ul> <li>Generally less preferred for high-precision crop biophysical parameter estimation unless no other calibration method is feasible. It may serve as a backup when field calibration targets are unavailable.</li> </ul> <p>Integrated Calibration Approaches</p> <p>Overview:</p> <ul> <li>Method: Combines multiple calibration techniques\u2014for example, using reflectance panels with onboard sensor data and atmospheric corrections.</li> <li>Process: Involves the integration of ground-based calibration targets, sensor metadata, and even atmospheric models to achieve the highest accuracy.</li> </ul> <p>Pros:</p> <ul> <li>High Precision: By accounting for multiple sources of error, this approach can provide the most accurate reflectance data.</li> <li>Robustness: Compensates for variations in lighting, sensor drift, and atmospheric conditions.</li> </ul> <p>Cons:</p> <ul> <li>Complexity and Cost: Requires additional equipment, more complex data processing, and potentially higher operational costs.</li> <li>Expertise: Demands a higher level of technical expertise to implement and validate.</li> </ul> <p>Suitability:</p> <ul> <li>Most suitable when the highest quality multispectral images are required for precise estimation of crop biophysical parameters. This method is ideal for research and high-end commercial applications where data quality directly impacts decision-making.</li> </ul> <p>Conclusion</p> <p>For estimating crop biophysical parameters such as LAI and chlorophyll content, achieving high radiometric accuracy is critical. Reflectance Panel Calibration combined with Empirical Line Calibration is generally recommended due to its direct correspondence with field measurements and its robustness in accounting for in-flight illumination conditions. If available, integrating sensor-based calibration (with downwelling light sensors) further enhances accuracy by dynamically adjusting for changing light conditions. While vicarious calibration can be an alternative when field calibration is impractical, it typically does not provide the same level of precision needed for high-quality biophysical parameter estimation.</p> <p>Choosing the right method often depends on operational constraints, available equipment, and the required accuracy. For precision agriculture applications where the goal is to obtain the highest quality multispectral images, a combination of reflectance panel calibration and empirical line calibration (with sensor integration if possible) is the most robust approach.</p> <p>This assessment should help guide the selection of a calibration method that best fits your UAV imaging workflow for high-precision crop biophysical parameter estimation.</p>"},{"location":"ai/overview-and-tools/#multi-modal-ai-images-videos-and-audio","title":"Multi-modal AI: images, videos and audio","text":"<p>Many generative AI tools (e.g. Google\u2019s Gemini or OpenAI\u2019s ChatGPT) are multi-modal. These tools can work with a range of data formats beyond text such as images, video or sound. </p> <p>Get feedback on maps and graphs</p> <p>You can upload an image to a multi-modal model and ask it to summarise the image or provide feedback on how to improve it. Use this feature to get ideas to improve the quality of your figures or maps. </p> <p>This figure of a flood extent on Vanua Levu, Fiji, following Tropical Cyclone Yasa was uploaded to Gemini to get suggestions for how to convert it to a publication quality map and instructions for doing this in QGIS. </p> <p></p> <p>Prompt</p> <p>This is an image of flooding after Tropical Cyclone Yasa struck Vanua Levu in Fiji. The blue flood layer is computed from Sentinel-1 SAR images. I'd like to present this information as a publication quality map. Can you make suggestions for how to enhance this image so it is of publication quality?</p> <p>The flood layer is stored as a GeoTIFF file. Can you provide detailed instructions for how to use QGIS to visualise it as a publication quality figure.</p> Response (Gemini 2.0 Flash) <p>Absolutely! Let's transform this informative image into a publication-quality map using QGIS.</p> <p>Enhancement Suggestions for Publication Quality</p> <ul> <li>Clear and Informative Title: Instead of just \"Flooding after Tropical Cyclone Yasa,\" use a more descriptive title like \"Extent of Flooding in [Specific Region] Vanua Levu, Fiji, Following Tropical Cyclone Yasa.\" Include the date of the cyclone for context.</li> <li>Legend: A legend is crucial. It should clearly explain the meaning of the blue layer (e.g., \"Flooded Areas\"). Consider adding other relevant features to the legend, like settlements, rivers, etc., if they are included.</li> <li>Scale Bar and North Arrow: Essential for spatial context.</li> <li>Coordinate Grid/Graticule: Adds geographic reference.</li> <li>Data Source Information: Include details about the Sentinel-1 SAR imagery (date acquired, sensor type, etc.) and the source of the flood layer.</li> <li>Inset Map (Optional): Show the location of Vanua Levu within Fiji or the wider Pacific region.</li> <li>Consistent Styling: Use consistent fonts, colors, and line weights throughout the map.</li> <li>High Resolution Output: Export the map at a high DPI (dots per inch) for print publications.</li> <li>Minimalist Design: Avoid clutter. Focus on conveying the essential information clearly.</li> </ul> <p>Detailed QGIS Instructions</p> <p>Import Data:</p> <ul> <li>Open QGIS: Launch QGIS Desktop.</li> <li>Add Raster Layer: Go to Layer &gt; Add Layer &gt; Add Raster Layer. Browse to your GeoTIFF file and open it.</li> <li>Add Basemap (Optional but Recommended): Use the QuickMapServices plugin (Plugins &gt; Manage and Install Plugins) to add a base map like Google Satellite or OpenStreetMap. This provides geographic context.</li> </ul> <p>Style the Flood Layer:</p> <ul> <li>Double-click the Flood Layer: This opens the Layer Properties.</li> <li>Symbology Tab:<ul> <li>Render Type: Ensure it's set to \"Singleband pseudocolor\" or \"Paletted/Unique values\" depending on your data.</li> <li>Color Ramp: Choose a suitable color ramp for the blue flood layer. A simple blue gradient is often effective.</li> <li>Opacity: Adjust the opacity if needed to see the underlying basemap.</li> <li>Transparency Tab: If necessary, set a No Data value to make areas without flood data transparent.</li> </ul> </li> </ul> <p>Create the Map Layout:</p> <ul> <li>New Print Layout: Go to Project &gt; New Print Layout. Give it a name.</li> <li>Add Map: Click \"Add Item\" &gt; \"Add Map.\" Draw a rectangle where you want the map to appear.</li> <li>Adjust Map Extent: Use the \"Move item content\" tool (under \"Item Properties\" when the map is selected) to pan and zoom the map to the desired area.</li> <li>Add Scale Bar: Click \"Add Item\" &gt; \"Add Scale Bar.\" Place it on the map. Adjust the units and style in the \"Item Properties.\"</li> <li>Add North Arrow: Click \"Add Item\" &gt; \"Add North Arrow.\" Choose a style and place it on the map.</li> <li>Add Grid/Graticule:<ul> <li>Select the map in the layout.</li> <li>In \"Item Properties,\" go to \"Grids.\"</li> <li>Click the \"+\" button to add a grid.</li> <li>Adjust the grid interval, style, and frame options.</li> </ul> </li> <li>Add Legend: Click \"Add Item\" &gt; \"Add Legend.\" Place it on the map.</li> <li>Uncheck \"Auto update\" to manually edit the legend items.</li> <li>Remove unnecessary items and rename the flood layer item to \"Flooded Areas\" or a more descriptive name.</li> <li>Add Title: Click \"Add Item\" &gt; \"Add Label.\" Type your title and adjust the font, size, and style in \"Item Properties.\"</li> <li>Add Data Source Information: Add another label for the data source details.</li> </ul> <p>Add Inset Map (Optional):</p> <ul> <li>Add Another Map Item: Add a smaller map item to the layout.</li> <li>Zoom to Fiji or the Pacific Region: Change the extent of this map to show the wider location.</li> <li>Add a Rectangle to Indicate the Main Map Area: Use \"Add Item\" &gt; \"Add Shape\" &gt; \"Add Rectangle\" to draw a rectangle on the inset map showing the area covered by the main map.</li> </ul> <p>Style and Refine:</p> <ul> <li>Font and Color Consistency: Ensure all text elements have consistent fonts and colors.</li> <li>Line Weights: Adjust line weights for clarity.</li> <li>Layout Alignment: Use the alignment tools in the layout to ensure elements are neatly arranged.</li> </ul> <p>Export the Map:</p> <ul> <li>Export as Image or PDF: Go to Layout &gt; Export as Image or Layout &gt; Export as PDF.</li> <li>Set DPI: For print publications, set the DPI (resolution) to 300 or higher.</li> </ul> <p>Additional Tips:</p> <ul> <li>Use Plugins: Explore QGIS plugins for additional cartographic tools and features.</li> <li>Reference Maps: Look at published maps from reputable sources for inspiration and best practices.</li> <li>Iterate: Don't be afraid to experiment with different styles and layouts until you achieve the desired result.</li> </ul> <p>By following these steps, you can create a clear, informative, and publication-quality map that effectively communicates the extent of flooding in Vanua Levu after Tropical Cyclone Yasa. Remember to tailor the map to the specific requirements of your publication. Good luck!</p> <p>Google\u2019s AI studio (which is free to use at the time of writing) lets you interact with Gemini models via text, image, audio and video formats. You can stream video with Gemini via screen sharing or web cams and have a verbal conversation with the model about what it can see. </p> <p>Screen share and chat with AI</p> <ul> <li>Screen share a paper while you are reading it and ask for an explanation of phrases, figures or equations</li> <li>Screen share your desktop GIS software and ask for instructions on how to complete tasks</li> <li>Use the web cam to practice giving oral presentations and get tips for improving your delivery from the AI </li> </ul> <p>Screen share of QGIS with Google AI Studio to get instructions on a workflow to turn raster pixels into a polygon layer.</p>"},{"location":"ai/overview-and-tools/#ai-literature-and-research-tools","title":"AI literature and research tools","text":"<p>There are numerous AI tools for literature and search tasks that take advantage of LLM\u2019s long context windows, file upload features or are dedicated AI research and literature apps. </p>"},{"location":"ai/overview-and-tools/#long-context-windows-and-file-uploads","title":"Long context windows and file uploads","text":"<p>The most recent LLMs have long context windows (the amount of information you can provide to the model in your prompt). At the time of writing, Gemini 1.5 pro has a context window of 1 million tokens and GPT-4o has a context window of 128,000 tokens. A token is a representation of inputs provided to the LLM, 1 million tokens roughly corresponds to 8 novels or the transcripts of 200 podcasts (see here).</p> <p>Alongside adding contextual information to a LLM\u2019s chat interface (e.g. writing or copy and pasting prompts), you can upload text and image files. The LLM will process the information in these files and combine it with your written prompt when generating a response. </p> <p>Using long context windows</p> <p>You can use the long context windows or file uploads to provide the LLM with extra information to use when generating your responses. Providing more contextual information:</p> <ul> <li>can improve the accuracy, factuality and relevance of responses </li> <li>gives you a tool to summarise large amounts of text</li> <li>allows you to interactively engage with reference material through iterative question and answer prompting </li> </ul> <p>Copyright and licensing</p> <p>Be careful to consult the copyright or licensing of content you upload to AI tools and be extra careful with sharing AI generated content. </p>"},{"location":"ai/overview-and-tools/#research-and-literature-ai-apps","title":"Research and literature AI apps","text":"<p>Apps built on top of generative AI models can be used to:</p> <ul> <li>search for content and generate literature reviews</li> <li>generate wikipedia-like articles for arbitrary topics</li> <li>create podcasts discussing a paper (and you can join in the conversation)</li> <li>screen share and have a conversation with the AI as you read a paper asking it to explain concepts, equations or figures</li> <li>act as a note taking assistant</li> <li>generate study guides, quizzes and practice questions</li> </ul> <p>Effective use of AI literature and research tools</p> <p>The key to AI enabled literature and research tools is using them to augment your existing reading, make your studying more efficient and effective, and enable active engagement with the literature. </p> <p>For example, generating a literature review on a topic before reading papers can:</p> <ul> <li>give you an overview of the relevant background theory</li> <li>help refresh your memory of key concepts</li> <li>give you a structure to organise your thoughts and supplement with your own ideas as you read </li> </ul> <p>Screen sharing and talking with AI as you read a paper allows you to:</p> <ul> <li>actively engage with the content</li> <li>break up the monotony of silent reading and mindless highlighting </li> <li>create and refine a mental model of the paper\u2019s concepts and findings with the AI's feedback and support </li> <li>speed up your reading by asking the AI questions to clarify or explain aspects of a paper (e.g. equations or a chunk of text) </li> </ul> <ul> <li>STORM: Generate Wikipedia-like articles or round table discussions on a topic </li> <li>NotebookLM: An AI enabled notebook for papers and documents with tools for question-answering, study guide generation and creating podcasts </li> <li>Perplexity: An AI chat tool with options for online search and deep research and literature review generation tool  </li> <li>Deep Research (Google Gemini - costs): A search and literature review generation tool </li> <li>ChatGPT with reason and search turned on (OpenAI): A search and literature review generation tool</li> <li>Deep Research (OpenAI - costs): A search and literature review generation tool</li> <li>Google AI Studio: Screen share and chat with a LLM as you read a paper </li> <li>Research Rabbit: An AI academic literature search tool that finds connections between papers </li> <li>Semantic Scholar: An AI tool for academic literature search, paper summarising and question and answering </li> <li>Elicit: An AI tool for comparing papers and conducting systematic reviews</li> </ul> <p>Diverse perspectives and active literature engagement with STORM and CO-STORM</p> <p>STORM focuses on exploring topics from diverse perspectives, capturing different opinions on a topic and lets you discover new ideas and trails-of-thought that you would not have encountered following your own search and question-answering. STORM app authors describe this as helping users discover unknown-unknowns.</p> <p>You provide STORM with a prompt (e.g. a topic you'd like to know more about) and it will generate a Wikipedia-like article for you using the following process, focusing on the research and pre-writing stage:</p> <ul> <li>AI-driven search to find relevant references and generate an article outline.  Two strategies are used to enhance this pre-writing research phase:<ul> <li>Diverse perspective-guided question answering: STORM uses an initial search to identify different perspectives on a topic and then uses these perspectives to guide question-answer information retrieval from articles on a topic. </li> <li>Simulated-conversations to iteratively build knowledge and understanding: STORM simulates a multi-turn conversation between a Wikipedia article writer and LLMs acting as topic experts with access to internet sources for grounding. Through this conversation the model updates its topic knowledge and refines follow-up questions. </li> </ul> </li> <li>Using the retrieved sources and outline, the LLM writes a Wikipedia-like article. </li> </ul> <p>While STORM focuses on generating Wikipedia-like articles, CO-STORM simulates a round-table conversation that you can observe and occasionally interject in. This lets you engage with and shape the knowledge discovery and information synthesising process. CO-STORM simulates a conversation between two LLM agents: experts who ask and answer questions from different perspectives and moderators who are non-experts and steer the conversation. The user can join the conversation at any time too. </p> <ul> <li>The act of asking questions allows you to actively engage with a topic, update your understanding of key concepts and develop new ideas.</li> <li>CO-STORM can generate a mind-map of the discussion, comprising relationships between concepts, to give you a structure for thinking critically about a topic to guide further reading. </li> <li>CO-STORM can convert the mind-map of concepts, and associated information, generated through discussion into a report.  </li> </ul> <p></p> <p>The CO-STORM interface and interacting with a discussion.</p>"},{"location":"ai/overview-and-tools/#useful-resources","title":"Useful resources:","text":"<ul> <li>Macquarie University's Using AI-powered tools for literature reviews</li> <li>The Research tab in UWA Library\u2019s Gen AI Tools section</li> </ul>"},{"location":"ai/prompt-engineering/","title":"Prompt engineering","text":"<p>The quality of an LLM\u2019s response depends on the quality of the input. Prompt engineering is the process of constructing prompts for LLMs to return best possible answers. A prompt is the input you provide to LLM, often it is a question or an instruction. </p> <p>There are a range of techniques that you can use to improve the quality of AI generated responses. You can use more than one of these techniques in a single prompt. </p> <p>Beyond improving the quality of generated AI responses for a specific task, using prompt engineering techniques can improve your learning in general. For example, you can use role-based prompting and ask the LLM to suggest improvements to your Python script from the view of a senior developer or your GIS workflow from the view of senior geospatial analysis. By doing this, you will pick up pointers to improve a range of skills from programming, data analysis to clarity of writing. With chain-of-thought prompting you can see the steps the model goes through when solving a problem. Seeing this in action can help you refine your problem solving skills, see examples of how to deconstruct complex problems into discrete steps and pick up tips that you can transfer to different tasks. </p>"},{"location":"ai/prompt-engineering/#tldr-prompt-template","title":"TL;DR - Prompt template","text":"<p>Effective prompts:</p> <ul> <li>provide explicit instructions and define the desired output format </li> <li>give the LLM a role and relevant task contextual information </li> <li>encourage the LLM to think by providing a step-by-step account of its reasoning</li> <li>provide an example solution (if you have one) </li> <li>iterate with the LLM to refine the solution </li> </ul> <p>An example prompt template: <pre><code>From the perspective of [INSERT ROLE HERE], generate [PROVIDE INSTRUCTIONS HERE]. This response is intended for [TARGET AUDIENCE / USER].\n\nGenerate the response using the following format [DEFINE OUTPUT FORMAT / STRUCTURE] using this example: [PROVIDE EXAMPLE, IF AVAILABLE].\n\nProvide a step-by-step explanation of each step with a justification\n</code></pre></p>"},{"location":"ai/prompt-engineering/#explicit-instructions-and-specificity","title":"Explicit instructions and specificity","text":"<p>The place to start with crafting effective prompts is providing clear and specific instructions to the LLM. Be very clear on what kind of information you want generated. If you provide a general instruction or ask a vague question, you will likely get a general response. </p> <p>There are various ways you can improve the specificity of a prompt such as providing contextual information and defining the output format. </p> <p>When providing contextual informaton, you can you can specify if are you using the response to: better understand a topic? seek suggestions for improving scientific writing? improve the clarity of text for a policy audience? generate interactive visualisations for public viewing online? generate cartographic quality maps for a report? or improve the robustness of your code or GIS workflows? Provide this information in your prompt and the response will be tailored accordingly. </p> <p>Things to consider when defining the format of the response are: what language do you want code examples to be provided in (e.g. R, Python, JavaScript)? do you want generated text with references? what structure do you want the generated text to have (you can provide headings and sub-headings for the response in your prompt)?</p> <p>Consider this prompt. It outlines the instruction for the LLM and provides a little ancillary information indicating that analysis will be performed in QGIS. However, it is left quite open as to the format and level of detail the LLM should generate in response.</p> <p>Prompt</p> <p>Can you outline a methodology to compute the area of vegetation on bare earth roaded catchments, that drain into farm dams, from UAV images using QGIS?</p> Response (Gemini 2.0 Flash) <p>Here's a methodology to compute the area of vegetation on bare earth roaded catchments draining into farm dams using UAV images and QGIS:</p> <p>Data Acquisition and Preprocessing:</p> <ul> <li>UAV Image Acquisition: Capture high-resolution UAV images of the catchment area. Ensure sufficient overlap (at least 70%) for accurate orthomosaic creation. Consider capturing images at different times of the day for varying light conditions if necessary. If possible, acquire multispectral imagery to aid in vegetation classification.</li> <li>Orthomosaic and DEM Creation: Use photogrammetry software (e.g., Agisoft Metashape, Pix4D) to process the UAV images and generate an orthomosaic (georeferenced image) and a Digital * Elevation Model (DEM). Accurate georeferencing is crucial. Use Ground Control Points (GCPs) if possible for increased accuracy.</li> <li>Boundary Delineation:<ul> <li>Catchment Boundary: Delineate the catchment boundary that contributes to the farm dam(s). Use the DEM to identify flow paths and ridgelines. This can be done manually in QGIS or using hydrological tools.</li> <li>Farm Dam Boundary: Digitize the boundary of the farm dam(s) on the orthomosaic.</li> </ul> </li> <li>Road Network: Digitize the road network within the catchment. This will be used to identify areas of bare earth and potentially filter out vegetation growing on the road itself (if you only want vegetation off the roads).</li> <li>Data Projection: Ensure all your data (orthomosaic, DEM, boundaries) are in the same projected coordinate system. This is essential for accurate area calculations.</li> </ul> <p>Vegetation Classification:</p> <ul> <li>Image Enhancement (Optional): If needed, adjust the brightness, contrast, or perform other image enhancements to improve the visual distinction between vegetation and bare earth.</li> <li>Vegetation Index Calculation (If using multispectral imagery): Calculate vegetation indices like NDVI (Normalized Difference Vegetation Index) or other relevant indices. These indices enhance the spectral signature of vegetation, making classification easier and more accurate. In QGIS, use the Raster Calculator for this.</li> </ul> <p>Supervised or Unsupervised Classification:</p> <ul> <li>Supervised Classification: Create training areas (polygons) representing different land cover classes (e.g., vegetation, bare earth, water, roads). Use a classification algorithm (e.g., Random Forest, Support Vector Machine) in QGIS (e.g., Semi-Automatic Classification Plugin (SCP), Orfeo Toolbox (OTB)) to classify the orthomosaic based on these training areas. This is generally more accurate.</li> <li>Unsupervised Classification: Use a clustering algorithm (e.g., K-means) to automatically group pixels into clusters based on their spectral characteristics. Then, manually assign these clusters to land cover classes. This is faster but less accurate.</li> <li>Accuracy Assessment: Assess the accuracy of your classification by comparing the classified image to ground truth data (if available) or by visual inspection. You can create a confusion matrix to quantify the accuracy. Refine your training areas or classification parameters if necessary.</li> </ul> <p>Area Calculation:</p> <ul> <li>Vectorization of Classified Raster: Convert the classified raster image (containing vegetation and bare earth classes) into a vector layer (polygons). QGIS's \"Polygonize\" tool can be used for this.</li> <li>Area Calculation: Use the field calculator in the attribute table of the vectorized layer to calculate the area of each polygon. Filter the polygons to select only those classified as \"vegetation.\" Sum the areas of these polygons to get the total vegetated area within the catchment.</li> <li>Area within Catchment Boundary: Use the \"Clip\" tool in QGIS to clip the vectorized vegetation layer to the catchment boundary. This ensures you're only calculating the vegetated area within the defined catchment.</li> <li>Area excluding roads (Optional): Use the \"Difference\" tool to subtract the road network polygons from the vegetation polygons. This will give you the vegetated area off the roads.</li> </ul> <p>Reporting and Visualization:</p> <ul> <li>Summarize Results: Calculate the percentage of vegetated area within the catchment.</li> <li>Create Maps: Generate maps showing the catchment boundary, farm dam, road network, and the classified vegetation. Use appropriate symbology and labels for clear communication.</li> </ul> <p>Software and Plugins:</p> <ul> <li>QGIS: Free and open-source Geographic Information System.</li> <li>Agisoft Metashape or Pix4D: Photogrammetry software for orthomosaic and DEM creation.</li> <li>Semi-Automatic Classification Plugin (SCP) or Orfeo Toolbox (OTB) for QGIS: For image classification.</li> </ul> <p>Key Considerations:</p> <ul> <li>Accuracy of UAV Data: The accuracy of your area calculation depends heavily on the quality of your UAV data (orthomosaic, DEM). GCPs are highly recommended.</li> <li>Classification Accuracy: Careful selection of training areas (for supervised classification) or cluster interpretation (for unsupervised classification) is crucial for accurate results.</li> <li>Resolution of UAV Imagery: Higher resolution imagery will generally provide more accurate results, especially for smaller vegetation patches.</li> <li>Shadows: Shadows can affect vegetation classification. Consider capturing images when shadows are minimal.</li> <li>Seasonal Variations: Vegetation cover can change seasonally. Be mindful of the timing of your UAV image acquisition.</li> </ul> <p>This methodology provides a comprehensive framework.  You may need to adapt it depending on the specifics of your study area and the available data. Remember to document all steps of your process for reproducibility.</p> <p>We can improve this prompt in a couple of ways. For example, we could provide more information on what roaded catchments are indicating that they're human constructions and may not be easily delineated via DEM-based GIS catchment mapping tools. We could also provide more details on how we'd like to map vegetation (e.g. using the multispectral images to create a NDVI layer and setting threshold for vegetation cover). We can also provide more detail about the output format, such explicitly asking for detailed instructions for using QGIS. </p> <p>Prompt</p> <p>Can you outline a methodology to compute the area of vegetation on bare earth roaded catchments, that drain into farm dams, from UAV images using QGIS? </p> <p>To compute the area of vegetation you will need to compute the NDVI from the UAV image and use a pixel NDVI threshold of greater than 0.2 to define vegetation. Provide detailed instructions and explanations for each step in the workflow including listing which QGIS operations and tools to use and where to find these tools in the QGIS user interface.</p> Response (Gemini 2.0 Flash) <p>Here's a detailed methodology to compute the area of vegetation using NDVI and a threshold in QGIS, specifically for bare earth roaded catchments draining into farm dams:</p> <p>Data Acquisition and Preprocessing (Same as before, but with emphasis on multispectral):</p> <ul> <li>UAV Image Acquisition: Crucially, you must acquire multispectral imagery (at least Red, Green, and Near-Infrared bands). This is essential for calculating NDVI.</li> <li>Orthomosaic and DEM Creation: Use photogrammetry software (e.g., Agisoft Metashape, Pix4D) to generate an orthomosaic and DEM. Ensure accurate georeferencing using GCPs if possible.</li> <li>Boundary Delineation: Delineate the catchment boundary, farm dam(s), and road network as polygons in QGIS, using the DEM and orthomosaic as a guide.</li> <li>Data Projection: Ensure all data are in the same projected coordinate system.</li> </ul> <p>NDVI Calculation in QGIS:</p> <ul> <li>Open Raster Calculator: Go to Raster -&gt; Raster calculator.</li> <li>Enter NDVI Formula: In the Raster calculator expression box, enter the following formula, adjusting band names if necessary (e.g., if your Red band is not called \"red\"): <code>( \"NIR\" - \"Red\" ) / ( \"NIR\" + \"Red\" )</code></li> <li>Replace \"NIR\" and \"Red\" with the actual band names from your orthomosaic. You can see the band names by clicking the \"...\" button next to the expression box and expanding your raster layer.</li> <li>Output Layer: Specify a name and location for the output NDVI raster file.</li> <li>Click OK: QGIS will calculate the NDVI for each pixel. NDVI values typically range from -1 to +1, with higher values indicating more vegetation.</li> </ul> <p>Vegetation Classification using NDVI Threshold:</p> <ul> <li>Reclassify Raster: Go to Raster -&gt; Raster calculator.</li> <li>Create Reclassification Expression: In the Raster calculator, use a conditional expression to classify pixels based on the NDVI threshold. For a threshold of 0.2: CASE WHEN \"NDVI@1\" &gt; 0.2 THEN 1 ELSE 0 END</li> <li>Replace \"NDVI@1\" with the name of your NDVI raster layer. 1 represents vegetation, and 0 represents non-vegetation (bare earth, water, etc.).</li> <li>Output Layer: Specify a name and location for the reclassified raster (e.g., \"vegetation_mask\").</li> <li>Click OK: QGIS will create a new raster where pixels with NDVI &gt; 0.2 are assigned the value 1 (vegetation), and others are assigned 0.</li> </ul> <p>Refining the Vegetation Mask (Optional but Recommended):</p> <ul> <li>Remove Small Patches (Sieve): Small, isolated pixels classified as vegetation might be noise. Go to Raster -&gt; Analysis -&gt; Sieve. Input your reclassified raster. Set the \"Pixel threshold\" to a value that removes small patches (e.g., 4 for removing patches smaller than 4 pixels).</li> <li>Fill Holes: Small holes within larger vegetation patches can be filled. Go to Raster -&gt; Analysis -&gt; Fill nodata.</li> </ul> <p>Area Calculation:</p> <ul> <li>Polygonize Raster: Convert the classified raster to a vector layer (polygons). Go to Raster -&gt; Conversion -&gt; Polygonize (raster to vector). Input your reclassified raster (or the sieved/filled raster if you performed those steps).</li> <li>Calculate Area: Open the attribute table of the newly created polygon layer (right-click the layer -&gt; Open Attribute Table).</li> <li>Add Area Field: If an area field doesn't already exist, create one. Click the \"Toggle Editing\" button (pencil icon), then click the \"Open Field Calculator\" button (abacus icon).</li> <li>New field name: area</li> <li>New field type: Decimal number (real)</li> <li>Expression: $area (This will calculate the area in the layer's units)</li> <li>Click OK.</li> <li>Filter Vegetation Polygons: In the attribute table, use the \"Select by expression\" tool (epsilon icon) to select polygons where the value of the reclassified field (usually named something like \"DN\") is equal to 1 (vegetation). The expression would be: \"DN\" = 1</li> <li>Calculate Sum of Area: With the vegetation polygons selected, you can see the sum of the area of the selected features in the attribute table (usually at the bottom).</li> </ul> <p>Clipping to Catchment Boundary:</p> <ul> <li>Clip Vector Layer: Go to Vector -&gt; Geoprocessing Tools -&gt; Clip. Input your vegetation polygon layer, and the catchment boundary polygon as the \"Clip layer.\" This will give you the vegetated area within the catchment. Repeat the area calculation from step 5 on this clipped layer.</li> <li>Clip Vector Layer to exclude roads (Optional): Go to Vector -&gt; Geoprocessing Tools -&gt; Difference. Input your clipped vegetation polygon layer, and the road network as the \"Overlay layer.\" This will give you the vegetated area off the roads. Repeat the area calculation from step 5 on this difference layer.</li> </ul> <p>Reporting and Visualization:</p> <ul> <li>Summarize Results: Calculate the percentage of vegetated area.</li> <li>Create Maps: Generate maps showing the catchment, dam, roads, and the classified vegetation.</li> </ul> <p>Key Considerations:</p> <ul> <li>NDVI Threshold: The 0.2 threshold is a starting point. You might need to adjust it based on your specific area and vegetation types. Experiment with different thresholds and validate your results.</li> <li>Shadows and Water: Shadows can affect NDVI. Water bodies often have low NDVI values, so they might be classified as non-vegetation. You might need to use additional techniques (e.g., masking) to deal with these issues.</li> <li>Accuracy: The accuracy of your area calculation depends on the quality of your UAV data, the chosen NDVI threshold, and the accuracy of your boundary delineations.</li> </ul> <p>This detailed workflow should help you calculate the vegetated area within your catchments. Remember to document your steps and justify your choices (e.g., the NDVI threshold you used).</p>"},{"location":"ai/prompt-engineering/#chain-of-thought-prompting","title":"Chain-of-thought prompting","text":"<p>Chain-of-thought prompting instructs the LLM to break solutions to questions or instructions down into a series of intermediate problems that are solved individually. This technique is particularly effective for large, complex or multi-step tasks and reasoning problems. Chain-of-thought prompting encourages the model to focus attention on individual parts of the problem, not skip reasoning steps and often returns more accurate responses. </p> <p>You can implement chain-of-thought prompting via providing explicit instructions of the steps you want the model to work through (example 1) or finish your prompt with an instruction for the model to implement step-by-step reasoning (example 2). </p> <p>Example 1: </p> <p>Prompt - example 1</p> <p>You are a remote sensing and machine learning specialist and consultant. Your task is to develop a land cover classification product for a regional local government organisation in Western Australia. </p> <p>You need to develop a land cover classification workflow that will generate a 10 m spatial resolution land cover map that includes the following classes: urban, water, grassland, scrubland, agriculture, mangrove, mudflats, open trees, forest. Please can you generate instructions and explanatory notes for the following steps:</p> <ol> <li>Generate training data that captures the geographic diversity of the land cover classes in the study area.</li> <li>Generate test data from a probability sample to support robust map accuracy assessment. </li> <li>Download Sentinel-2 images from Planetary Computer.</li> <li>Combine data from Sentinel-2 images with the labelled training and test data and transform the data to a format suitable for machine learning model development. </li> <li>Train a machine learning model to classify Sentinel-2 images into land cover maps. </li> <li>Evaluate the model using the test data. </li> <li>Generate a web map viewer for the classified land cover map land cover map. </li> </ol> <p>Include Python code for each step where relevant. </p> Response (ChatGPT) <p>Developing a 10-meter spatial resolution land cover map for a regional local government organization in Western Australia involves several key steps. Below is a detailed workflow, including Python code snippets where applicable.</p> <p>Generate Training Data</p> <p>Objective: Collect representative samples for each land cover class to train the classification model.</p> <ul> <li>Field Surveys: Conduct on-site visits to collect GPS coordinates and photographs of distinct land cover types.</li> <li>High-Resolution Imagery: Utilize platforms like Google Earth or commercial satellite imagery to visually identify and digitize polygons corresponding to each class.</li> <li>Existing Datasets: Incorporate data from local environmental agencies or previous studies that have classified land cover in the region.</li> <li>Note: Ensure that the training data captures the geographic diversity and variability of each class within the study area.</li> </ul> <p>Generate Test Data</p> <p>Objective: Create a statistically robust dataset to evaluate the accuracy of the land cover map.</p> <ul> <li>Probability Sampling: Implement a stratified random sampling approach to select test points across different land cover classes.</li> <li>Sample Size Determination: Calculate the number of samples needed per class to achieve a desired confidence level and margin of error.</li> <li>Data Collection: For each selected point, record the true land cover class through field verification or high-resolution imagery interpretation.</li> <li>Note: The test data should be independent of the training data to provide an unbiased assessment of model performance.</li> </ul> <p>Download Sentinel-2 Images from Planetary Computer</p> <p>Objective: Acquire Sentinel-2 imagery covering the study area for the relevant time period.</p> <ul> <li>Utilize the sentinelsat Python package to search and download Sentinel-2 data from the Copernicus Open Access Hub.</li> </ul> <pre><code>from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt\nfrom datetime import date\n\n# User credentials for Copernicus Open Access Hub\napi = SentinelAPI('username', 'password', 'https://scihub.copernicus.eu/dhus')\n\n# Define the area of interest (AOI) by loading a geojson file\nfootprint = geojson_to_wkt(read_geojson('aoi.geojson'))\n\n# Search for Sentinel-2 products over the AOI within the specified date range\nproducts = api.query(footprint,\n                    date=('20220101', '20220131'),\n                    platformname='Sentinel-2',\n                    cloudcoverpercentage=(0, 30))\n\n# Download all results from the search\napi.download_all(products)\n</code></pre> <p>Note: Replace 'username' and 'password' with your actual credentials. Adjust the date range and cloud cover percentage as needed.</p> <p>Prepare Data for Machine Learning</p> <p>Objective: Process and organize the imagery and labeled data for model training and evaluation.</p> <p>Preprocessing Sentinel-2 Imagery:</p> <ul> <li>Atmospheric Correction: Apply correction algorithms (e.g., Sen2Cor) to convert Top-Of-Atmosphere (TOA) reflectance to Bottom-Of-Atmosphere (BOA) reflectance.</li> <li>Resampling: Standardize all bands to a 10-meter resolution.</li> <li>Clipping: Trim the imagery to the study area's boundaries.</li> </ul> <p>Feature Extraction:</p> <ul> <li>Spectral Indices: Compute indices such as NDVI (Normalized Difference Vegetation Index) to enhance class separability.</li> </ul> <pre><code>import rasterio\nfrom rasterio.plot import show\nimport numpy as np\n\n# Open the red and NIR bands\nwith rasterio.open('B04.tif') as red:\n    red_band = red.read(1).astype('float32')\nwith rasterio.open('B08.tif') as nir:\n    nir_band = nir.read(1).astype('float32')\n\n# Calculate NDVI\nndvi = (nir_band - red_band) / (nir_band + red_band)\n\n# Save NDVI to a new GeoTIFF\nndvi_meta = red.meta\nndvi_meta.update(dtype=rasterio.float32, count=1)\nwith rasterio.open('ndvi.tif', 'w', **ndvi_meta) as dst:\n    dst.write(ndvi, 1)\n</code></pre> <p>Data Integration:</p> <ul> <li>Label Assignment: Overlay training and test data points on the processed imagery to extract corresponding pixel values and assign class labels.</li> <li>Dataset Creation: Organize the extracted features and labels into structured arrays or dataframes suitable for input into machine learning algorithms.</li> </ul> <p>Train a Machine Learning Model</p> <p>Objective: Develop a model capable of classifying land cover types based on the input features.</p> <ul> <li>Model Selection: Choose an appropriate algorithm, such as Random Forest, Support Vector Machine, or a Convolutional Neural Network, depending on the complexity and size of the dataset.</li> </ul> <p>Training the Model:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Assume X contains feature vectors and y contains corresponding labels\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize and train the Random Forest classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Validate the model\ny_pred = rf_model.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f'Validation Accuracy: {accuracy:.2f}')\n</code></pre> <p>Note: Hyperparameter tuning and cross-validation are essential to optimize model performance.</p> <p>Evaluate the Model</p> <p>Objective: Assess the accuracy and reliability of the land cover classification.</p> <ul> <li>Confusion Matrix: Generate a confusion matrix to visualize the performance across different classes.</li> </ul> <pre><code>from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute confusion matrix\ncm = confusion_matrix(y_val, y)\n</code></pre> <p>Prompt - example 2</p> <p>You are a remote sensing and machine learning specialist and consultant. Your task is to develop a land cover classification product for a regional local government organisation in Western Australia. </p> <p>You need to develop a land cover classification workflow that will generate a 10 m spatial resolution land cover map that includes the following classes: urban, water, grassland, scrubland, agriculture, mangrove, mudflats, open trees, forest. The task requires that you need to generate training data that captures the geographic diversity of the land cover classes in the study area and test data from a probability sample. You need to create a data processing pipeline to download Sentinel-2 images from Microsoft\u2019s Planetary Computer and combine it with the labelled training and test data. You need to train a machine learning model to classify Sentinel-2 images into land cover maps and evaluate this model. The final task is to visualise the land cover map on a slippy web map. </p> <p>Can you outline the solution in a step-by-step manner. For each step, please explain why this approach is suitable for this task. Also, include Python code for each step where relevant.</p> Response (Perplexity) <p>Certainly! I'll outline a step-by-step solution for developing a land cover classification product using Sentinel-2 imagery and machine learning techniques. This approach is tailored for a regional local government organization in Western Australia, aiming to create a 10 m spatial resolution land cover map with specific classes.</p> <p>Step 1: Data Collection and Preparation</p> <p>Sentinel-2 Image Acquisition</p> <p>We'll use Microsoft's Planetary Computer to access Sentinel-2 imagery. This platform provides easy access to a vast archive of Earth observation data, including Sentinel-2 images^1.</p> <pre><code>import planetary_computer\nimport pystac_client\nimport rasterio\nimport numpy as np\n\n# Initialize the Planetary Computer STAC API client\ncatalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n# Search for Sentinel-2 imagery\nsearch = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=[115.0, -35.0, 129.0, -13.0],  # Approximate bounding box for Western Australia\n    datetime=\"2024-01-01/2025-02-23\",\n    query={\"eo:cloud_cover\": {\"lt\": 20}}\n)\n\n# Download and stack relevant bands\nitems = list(search.get_items())\nfor item in items:\n    signed_item = planetary_computer.sign(item)\n    bands = [\"B02\", \"B03\", \"B04\", \"B08\", \"B11\", \"B12\"]  # Blue, Green, Red, NIR, SWIR1, SWIR2\n    arrays = []\n    for band in bands:\n        with rasterio.open(signed_item.assets[band].href) as src:\n            arrays.append(src.read(1))\n    stacked_image = np.stack(arrays)\n</code></pre> <p>This approach is suitable because it allows us to efficiently access and process large volumes of satellite imagery without the need for local storage^1.</p> <p>Training Data Generation</p> <p>To capture the geographic diversity of land cover classes, we'll use a stratified random sampling approach:</p> <pre><code>import geopandas as gpd\nfrom sklearn.model_selection import train_test_split\n\n# Load a shapefile of the study area\nstudy_area = gpd.read_file(\"study_area.shp\")\n\n# Generate stratified random points\npoints = study_area.sample(n=1000, stratify=\"land_cover_class\")\n\n# Split into training and validation sets\ntrain_points, val_points = train_test_split(points, test_size=0.2, stratify=points[\"land_cover_class\"])\n\n# Save points for field verification\ntrain_points.to_file(\"training_points.shp\")\nval_points.to_file(\"validation_points.shp\")\n</code></pre> <p>This method ensures that we have representative samples for each land cover class across the study area^9.</p> <p>Step 2: Image Processing and Feature Extraction</p> <p>We'll create spectral indices and texture features to enhance the classification:</p> <pre><code>def calculate_indices(image):\n    blue, green, red, nir, swir1, swir2 = image\n\n    ndvi = (nir - red) / (nir + red)\n    ndwi = (green - nir) / (green + nir)\n    savi = 1.5 * (nir - red) / (nir + red + 0.5)\n\n    return np.vstack((image, ndvi, ndwi, savi))\n\nprocessed_images = [calculate_indices(img) for img in stacked_images]\n</code></pre> <p>These spectral indices are particularly useful for distinguishing between vegetation types, water bodies, and built-up areas^4.</p> <p>Step 3: Machine Learning Model Training</p> <p>We'll use a Random Forest classifier, which has shown good performance in land cover classification tasks^4:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Prepare training data\nX = extract_features(train_points, processed_images)\ny = train_points[\"land_cover_class\"]\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Perform grid search\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(rf, param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X, y)\n\n# Train final model\nbest_rf = grid_search.best_estimator_\nbest_rf.fit(X, y)\n</code></pre> <p>Random Forest is suitable for this task due to its ability to handle high-dimensional data, resistance to overfitting, and good performance on imbalanced datasets^4.</p> <p>Step 4: Model Evaluation</p> <p>We'll use the validation set to assess the model's performance:</p> <pre><code>from sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report\n\n# Prepare validation data\nX_val = extract_features(val_points, processed_images)\ny_val = val_points[\"land_cover_class\"]\n\n# Make predictions\ny_pred = best_rf.predict(X_val)\n\n# Calculate metrics\naccuracy = accuracy_score(y_val, y_pred)\nkappa = cohen_kappa_score(y_val, y_pred)\nreport = classification_report(y_val, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Kappa: {kappa}\")\nprint(report)\n</code></pre> <p>These metrics provide a comprehensive assessment of the model's performance, with the Kappa coefficient being particularly useful for multi-class problems^9.</p> <p>Step 5: Land Cover Map Generation</p> <p>Apply the trained model to the entire study area:</p> <pre><code>def predict_landcover(image, model):\n    features = calculate_indices(image)\n    prediction = model.predict(features.reshape(-1, features.shape[^0]))\n    return prediction.reshape(image.shape[^1], image.shape[^2])\n\nlandcover_map = predict_landcover(stacked_image, best_rf)\n</code></pre> <p>Step 6: Visualization on a Slippy Web Map</p> <p>We'll use Folium to create an interactive web map:</p> <pre><code>import folium\nfrom branca.colormap import LinearColormap\n\n# Create a color map\ncolors = ['#FF0000', '#00FF00', '#0000FF', ...]  # Define colors for each class\ncolor_map = LinearColormap(colors=colors, vmin=0, vmax=len(colors))\n\n# Create a map centered on the study area\nm = folium.Map(location=[-27.0, 121.0], zoom_start=6)\n\n# Add the land cover layer\nfolium.raster_layers.ImageOverlay(\n    landcover_map,\n    bounds=[[study_area.total_bounds[^1], study_area.total_bounds[^0]],\n            [study_area.total_bounds[^3], study_area.total_bounds[^2]]],\n    colormap=color_map,\n    opacity=0.7\n).add_to(m)\n\n# Add a legend\ncolor_map.add_to(m)\n\n# Save the map\nm.save(\"landcover_map.html\")\n</code></pre> <p>This interactive map allows users to explore the land cover classification results easily^5.</p> <p>This comprehensive workflow combines state-of-the-art satellite imagery, robust machine learning techniques, and interactive visualization to create an accurate and user-friendly land cover classification product for the Western Australian local government organization. The use of Sentinel-2 imagery provides high spatial resolution (10 m) suitable for detailed land cover mapping, while the Random Forest classifier offers good performance and interpretability. The final web map visualization ensures that the results are accessible and easy to understand for decision-makers and stakeholders.</p>"},{"location":"ai/prompt-engineering/#role-based-prompting","title":"Role-based prompting","text":"<p>Role-based prompting assigns the LLM a \u201crole\u201d for to the task or interaction and it can improve the accuracy of responses and their relevance for target audiences or tasks. For example, if you want the LLM to generate detailed content for a technical or scientific topic, you could assign it the role of a domain expert such as a \u201cremote sensing specialist\u201d or \u201csenior geospatial analyst\u201d. Or, if you are looking for clear and accessible explanation of a concept you could assign the LLM the role of a \u201cteacher\u201d. </p> <p>Prompt</p> <p>You are a senior geospatial data scientist. Can you make suggestions for improving this workflow for crop type classification using remote sensing data?</p> <ol> <li>Generate crop type labels using Google Street View</li> <li>Generate predictor variables using single-date Landsat data</li> <li>Train machine learning model to associate Landsat data with crop type labels</li> <li>Evaluate model using government datasets and farmer records</li> </ol> Response (Perplexity with ChatGPT's o3-mini reasoning model) <p>To improve the workflow for crop type classification using remote sensing data, I suggest the following enhancements based on the current workflow and relevant findings from the search results:</p> <p>Use Multi-Date Satellite Imagery Instead of Single-Date Landsat Data</p> <p>Reason: Single-date imagery may not capture the full phenological cycle of crops, which is crucial for distinguishing between crop types. Studies show that multi-date imagery improves classification accuracy by leveraging spectral and temporal variations across different stages of crop growth.</p> <p>Improvement:</p> <ul> <li>Incorporate multi-date Landsat or Sentinel-2 data to capture key phenological stages (e.g., planting, peak growth, harvest).</li> <li>Generate vegetation indices (e.g., NDVI, EVI) for each date to enhance feature space.</li> </ul> <p>Code Example:</p> <pre><code>from datetime import datetime\nimport planetary_computer\n\n# Define time periods for multi-date imagery\ntime_periods = [\"2024-05-01/2024-06-30\", \"2024-08-01/2024-09-30\", \"2024-11-01/2024-12-31\"]\n\n# Search for multi-date Landsat data\nimages = []\nfor period in time_periods:\n    search = catalog.search(\n        collections=[\"landsat-c2-l2\"],\n        bbox=[115.0, -35.0, 129.0, -13.0],  # Western Australia bounding box\n        datetime=period,\n        query={\"eo:cloud_cover\": {\"lt\": 20}}\n    )\n    images.extend(list(search.get_items()))\n</code></pre> <p>Enhance Crop Type Labels with Automated Deep Learning on Street View Imagery</p> <p>Reason: Generating crop type labels manually from Google Street View (GSV) is time-consuming and prone to errors. Automated deep learning pipelines can efficiently extract crop type information from street-level imagery with minimal manual labeling.</p> <p>Improvement:</p> <ul> <li>Use a Convolutional Neural Network (CNN) or pre-trained models (e.g., GPT-4V) to classify GSV images automatically.</li> <li>Combine weakly labeled GSV data with remote sensing time series to generate robust training datasets.</li> </ul> <p>Code Example:</p> <pre><code>from tensorflow.keras.models import load_model\n\n# Load pre-trained CNN model for crop type classification\nmodel = load_model(\"crop_type_cnn.h5\")\n\n# Predict crop types from GSV images\ndef predict_crop_type(image_path):\n    image = preprocess_image(image_path)  # Preprocess image for CNN input\n    prediction = model.predict(image)\n    return np.argmax(prediction)\n\n# Apply predictions to a dataset of GSV images\ncrop_labels = [predict_crop_type(img_path) for img_path in gsv_image_paths]\n</code></pre> <p>Integrate Phenology-Based Approaches**</p> <p>Reason: Crop-specific phenological patterns (e.g., growth cycles) can help distinguish between similar crops. Incorporating phenology-based classification methods enhances accuracy by leveraging temporal changes in vegetation indices.</p> <p>Improvement:</p> <ul> <li>Use time-series analysis of vegetation indices like NDVI, SAVI, and EVI to identify unique crop phenologies.</li> <li>Combine spectral indices with texture features for improved discrimination.</li> </ul> <p>Code Example:</p> <pre><code>import numpy as np\n\n# Calculate NDVI over multiple dates\ndef calculate_ndvi(nir_band, red_band):\n    return (nir_band - red_band) / (nir_band + red_band)\n\nndvi_series = [calculate_ndvi(images[i][\"B5\"], images[i][\"B4\"]) for i in range(len(images))]\nphenology_features = np.stack(ndvi_series, axis=-1)\n</code></pre> <p>Leverage Sentinel-2 Data for Higher Spatial Resolution</p> <p>Reason: Sentinel-2 provides 10 m spatial resolution compared to Landsat's 30 m, which is more suitable for smallholder farms or heterogeneous landscapes often found in regions like Western Australia.</p> <p>Improvement:</p> <ul> <li>Replace Landsat data with Sentinel-2 imagery to capture finer spatial details.</li> <li>Use Sentinel-2's additional bands (e.g., Red Edge) to improve spectral discrimination between crops.</li> </ul> <p>Evaluate Model with Independent Datasets and Stratified Sampling**</p> <p>Reason: Current evaluation relies on government datasets and farmer records, which may not be spatially representative or fully accurate. Stratified sampling ensures better geographic and class representation.</p> <p>Improvement:</p> <ul> <li>Use stratified random sampling to create validation datasets covering diverse land cover types.</li> <li>Supplement evaluation with independent datasets like the Cropland Data Layer (CDL) or other open-access crop maps.</li> </ul> <p>Code Example:</p> <pre><code>from sklearn.model_selection import train_test_split\n\n# Stratify by crop type class\ntrain_data, test_data = train_test_split(data, test_size=0.2, stratify=data[\"crop_type\"])\n</code></pre> <p>Incorporate Ensemble Learning Models</p> <p>Reason: Ensemble methods such as Random Forests or Gradient Boosting outperform single models by combining predictions from multiple learners.</p> <p>Improvement:</p> <ul> <li>Train ensemble models on multi-date features and phenology-based indices.</li> <li>Evaluate ensemble performance against single classifiers.</li> </ul> <p>Code Example:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\n\n# Train Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_model.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = rf_model.predict(X_test)\n</code></pre> <p>Visualize Results Using Interactive Maps</p> <p>Reason: Visualizing classification results on a web map improves stakeholder accessibility and decision-making.</p> <p>Improvement:</p> <ul> <li>Use tools like Folium or Kepler.gl to create interactive maps showing classified crop types.</li> <li>Include overlays of ground truth points and confidence scores.</li> </ul> <p>Code Example:</p> <pre><code>import folium\n\n# Create an interactive map with classified results\nm = folium.Map(location=[-27.0, 121.0], zoom_start=6)\nfolium.raster_layers.ImageOverlay(\n    image=classified_map,\n    bounds=[[study_area.bounds[^1], study_area.bounds[^0]],\n            [study_area.bounds[^3], study_area.bounds[^2]]],\n    opacity=0.6,\n).add_to(m)\nm.save(\"crop_classification_map.html\")\n</code></pre> <p>Summary of Improvements:</p> <ol> <li>Use multi-date imagery to capture phenological variations.</li> <li>Automate label generation from GSV using deep learning.</li> <li>Incorporate phenology-based approaches for temporal analysis.</li> <li>Replace Landsat with Sentinel-2 for higher spatial resolution.</li> <li>Evaluate using stratified sampling and independent datasets.</li> <li>Train ensemble models for better predictive performance.</li> <li>Visualize results interactively for stakeholders.</li> </ol> <p>These enhancements address limitations in the current workflow by improving data quality, leveraging advanced machine learning techniques, and providing actionable insights through visualization tools.</p> <p>[^22]: https://learn.geo4.dev/Satellite Crop Mapping.html</p> <p>[^57]: https://www.infrastructure.gov.au/sites/default/files/documents/BCR Open government data report - final - 5 Feb2016.pdf</p>"},{"location":"ai/prompt-engineering/#example-based-prompting","title":"Example-based prompting","text":"<p>LLMs can generate responses to questions or instructions without seeing any examples; this is called zero-shot prompting. However, you can provide one (single-shot prompting) or multiple (many-shot prompting) examples that illustrate the style of response you\u2019d like returned or the logic the LLM should use to solve problems. </p> <p>Prompt</p> <p>Can you write Python functions to compute the following vegetation indices: EVI, OSAVI and NDWI? The functions should take in a Sentinel-2 image stored as a Numpy array. Use the following as an example format:</p> <pre><code>import numpy as np\n\ndef calculate_ndvi(red, nir):\n    \"\"\"\n    Calculates the Normalized Difference Vegetation Index (NDVI).\n\n    Args:\n        red (numpy.ndarray): Red band array.\n        nir (numpy.ndarray): Near-infrared band array.\n\n    Returns:\n        numpy.ndarray: NDVI array.\n    \"\"\"\n\n    # Prevent division by zero\n    ndvi = np.where((nir + red) == 0, 0, (nir - red) / (nir + red))\n    return ndvi\n</code></pre>"},{"location":"ai/prompt-engineering/#multi-media-multi-modal-prompting","title":"Multi-media / multi-modal prompting","text":"<p>Prompts to multi-modal models can include more than one type of data. For example, you could upload a file, image or code alongside providing a written prompt. Be clear when prompting the model to use the all the data sources; if you upload an image you can start your prompt by asking the model to describe the image (focusing the model\u2019s attention on the image) before generating the response you\u2019re actually after. Multi-media prompting allows you to provide more context the model; this can improve the relevancy and factuality of generated responses. </p> <p>While it's not about agricultural or geospatial data, this is a great guide to multi-modal prompting. However, this can illustrate ways you can use multi-modal prompting for spatial and agricultural analysis tasks. Here are some examples for you to try out:</p> <ul> <li>Spatial reasoning and logic: upload a map and prompt the AI to detect spatial patterns and interpret them (e.g. upload a map of crop yield within a paddock and prompt the AI to describe the spatial pattern of yield, comment on spatial variability in yield and identify underperforming zones).</li> <li>Image sequences: upload a \"film strip\" of images an prompt the AI to describe the spatio-temporal dynamics present in the images (e.g. upload a sequence of RGB satellite images of a canola field and ask the AI to describe the spatio-temporal pattern of flowering and explain the variability in timing and intensity of flowering within the field; upload regional maps of precipitation and prompt the AI to detect drying trends or drought risk; upload a series of land cover maps and prompt the AI to describe patterns in deforestation or agricultural expansion).</li> <li>Image feedback: upload an image (e.g. a map or figure) to the AI and ask for improvements for communicating key patterns, insights or information or ask for suggestions on how to edit for different audiences (e.g. a scientific graph to a graph for policy brief or infographic).</li> <li>Video streaming and conversations: With Google\u2019s AI studio you can screenshare a paper you're reading, your coding environment or desktop GIS and have a verbal conversation about what's on your screen. For example, you could ask the AI to help explain a figure in a paper or report on your screen or help you navigate around QGIS to find operations. </li> <li>Presentation feedback: Share your web cam with Google\u2019s AI studio and practice a presentation and get feedback on ways to improve delivery. </li> </ul>"},{"location":"ai/prompt-engineering/#iterative-prompting","title":"Iterative prompting","text":"<p>Iterative prompting can be an effective technique to generate more accurate and relevant responses. Often, AI tools don't generate a correct or the most relevant prompt at the first attempt. In successive prompts you can provide feedback to the AI on how it should refine its responses. This has multiple benefits; with successive prompts you are providing more context to the AI tool, which it can use to improve the quality of the response, and you get the opportunity to clarify your thinking or develop new ideas as you reflect on the AI's responses.</p> <p>Let's use the previous prompt example from the chain-of-thought prompting, which asks for a worfklow to generate a land cover map, to demonstrate an iterative prompting workflow. We'll focus on iteratively refining and adding detail to the part of this workflow that involves generating the training data. This is the initial prompt:</p> <p>Prompt</p> <p>You are a remote sensing and machine learning specialist and consultant. Your task is to develop a land cover classification product for a regional local government organisation in Western Australia. </p> <p>You need to develop a land cover classification workflow that will generate a 10 m spatial resolution land cover map that includes the following classes: urban, water, grassland, scrubland, agriculture, mangrove, mudflats, open trees, forest. Please can you generate instructions and explanatory notes for the following steps:</p> <ol> <li>Generate training data that captures the geographic diversity of the land cover classes in the study area.</li> <li>Generate test data from a probability sample to support robust map accuracy assessment. </li> <li>Download Sentinel-2 images from Planetary Computer.</li> <li>Combine data from Sentinel-2 images with the labelled training and test data and transform the data to a format suitable for machine learning model development. </li> <li>Train a machine learning model to classify Sentinel-2 images into land cover maps. </li> <li>Evaluate the model using the test data. </li> <li>Generate a web map viewer for the classified land cover map land cover map. </li> </ol> <p>Include Python code for each step where relevant. </p> Response (Gemini 2.0 Flash) <p>Step-by-Step Workflow:</p> <p>Generate Training Data</p> <p>Concept: Training data is crucial for supervised learning. It should be representative of the land cover classes across the study area.</p> <p>Method:</p> <ul> <li>Visual Interpretation: Utilize high-resolution imagery (e.g., Google Earth, Nearmap) as a base layer.</li> <li>Stratified Random Sampling: Divide the study area into zones based on known land cover patterns. Generate random points within each zone.</li> <li>Manual Digitization: Digitize polygons around representative areas of each land cover class at the sampled points.</li> <li>Geodatabase: Store the digitized polygons in a GeoJSON or shapefile format, with an attribute field indicating the land cover class.</li> </ul> <p>Considerations:</p> <ul> <li>Ensure a balanced representation of each class.</li> <li>Capture the variability within each class (e.g., different types of urban areas, varying densities of forest).</li> <li>Use expert knowledge of the region to identify and delineate complex land cover types.</li> </ul> <p>Python (Example - Geopandas for creating a basic training file):</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import Polygon\n\n# Example polygons (replace with your actual digitized polygons)\ndata = {\n    'geometry': [\n        Polygon([(115, -32), (115.1, -32), (115.1, -32.1), (115, -32.1)]),  # Urban\n        Polygon([(116, -33), (116.1, -33), (116.1, -33.1), (116, -33.1)]),  # Forest\n        #... more polygons\n    ],\n    'class': ['urban', 'forest']  # Corresponding land cover classes\n}\n\ngdf_train = gpd.GeoDataFrame(data, crs=\"EPSG:4326\")  # Set CRS to WGS 84\ngdf_train.to_file(\"training_data.geojson\", driver=\"GeoJSON\")\n</code></pre> <p>It's an OK templare to start with, but the code snippet and instructions are incomplete or too vague to implement and we can add more detail to the explanatory notes. Let's go back to the AI and prompt it to provide more information:</p> <p>Prompt</p> <p>Focusing on step 1 (generating training data) can you provide detailed instructions on how to implement stratified sampling to generate a series of points that are representative of the region? </p> <p>Include information on the datasets required to do this, provide a Python code example, and reflect on any limitations with this approach with respect to training a land cover classifier.</p> Response (Gemini 2.0 Flash) <p>Data Requirements</p> <ul> <li>Study Area Boundary: A polygon shapefile or GeoJSON defining the region of interest.</li> <li>Stratification Layers: Raster or vector datasets that capture the geographic variability within your study area. These could include:</li> <li>Elevation: A digital elevation model (DEM) to capture variations in terrain.</li> <li>Landforms: A landform classification map to differentiate between plains, hills, valleys, etc.</li> <li>Soil Types: A soil map to account for differences in soil properties.</li> <li>Vegetation Indices: Pre-calculated NDVI or other vegetation indices from existing imagery.</li> <li>Existing Land Cover: A coarse-resolution land cover map to broadly stratify the area.</li> </ul> <p>Stratification Process</p> <ul> <li>Combine Layers (Optional): If using multiple stratification layers, you might need to combine them into a single composite layer. This can be done using raster algebra or spatial joins, depending on the data format.</li> <li>Define Strata: Divide the composite layer into distinct classes or zones. The number and type of strata will depend on the complexity of your study area and the available data. For example, you could combine elevation and landform to create strata like \"lowland plains,\" \"upland hills,\" etc.</li> <li>Allocate Sample Points: Determine the desired number of sample points for each stratum. This can be done proportionally to the area of each stratum or based on prior knowledge of land cover distribution.</li> <li>Generate Random Points: Use a random point generator to create points within each stratum. Ensure that the points are well-distributed and avoid clustering.</li> </ul> <p>Limitations and Considerations</p> <ul> <li>Strata Definition: Defining meaningful strata can be challenging and requires careful consideration of the landscape and available data.</li> <li>Data Availability: The quality and availability of stratification layers can influence the effectiveness of the sampling.</li> <li>Sample Size: Determining the appropriate sample size per stratum is crucial and may require statistical analysis or expert knowledge.</li> <li>Rare Classes: Stratified sampling may not adequately capture rare or sparsely distributed land cover classes. Additional targeted sampling may be needed.</li> <li>Temporal Variability: If significant land cover changes occur throughout the year, consider using imagery from multiple seasons for training data collection.</li> <li>Accessibility: Ensure that the generated sample points are accessible for ground-truthing or high-resolution image interpretation if you plan to validate your training data with field observations.</li> </ul> <p>Refinement and Validation</p> <ul> <li>Visual Inspection: After generating the sample points, visually inspect their distribution on a map to ensure they adequately represent the different strata and land cover types.</li> <li>Ground-Truthing or High-Resolution Imagery: If possible, validate the land cover at the sample points using field visits or high-resolution imagery to confirm the accuracy of your training data.</li> </ul> <p>By carefully implementing stratified random sampling, you can create a more representative set of training points for your land cover classification model, leading to improved accuracy and generalization across the study area.</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import Point\nfrom shapely.ops import cascaded_union\nimport random\n\ndef stratified_random_sampling(strata_gdf, num_points_per_stratum):\n    \"\"\"\n    Generates stratified random sample points.\n\n    Args:\n        strata_gdf (GeoDataFrame): GeoDataFrame with strata polygons and a 'stratum' column.\n        num_points_per_stratum (dict): Dictionary mapping stratum names to the desired number of points.\n\n    Returns:\n        GeoDataFrame: GeoDataFrame with the generated sample points.\n    \"\"\"\n\n    sample_points =\n    for stratum in strata_gdf['stratum'].unique():\n        stratum_polygon = cascaded_union(strata_gdf[strata_gdf['stratum'] == stratum]['geometry'])\n        minx, miny, maxx, maxy = stratum_polygon.bounds\n\n        num_points = num_points_per_stratum.get(stratum, 0)\n        for i in range(num_points):\n            while True:\n                x = random.uniform(minx, maxx)\n                y = random.uniform(miny, maxy)\n                point = Point(x, y)\n                if stratum_polygon.contains(point):\n                    sample_points.append({'geometry': point, 'stratum': stratum})\n                    break\n\n    return gpd.GeoDataFrame(sample_points, crs=strata_gdf.crs)\n\n# Example usage\n# Assuming you have a GeoDataFrame 'strata_gdf' with your strata\nnum_points = {'lowland': 50, 'upland': 30, 'water': 10}  # Example\nsample_points_gdf = stratified_random_sampling(strata_gdf, num_points)\n\n# Save the sample points\nsample_points_gdf.to_file(\"sample_points.geojson\", driver='GeoJSON')\n</code></pre> <p>There is more detail on how to create strata. Some notes have been generated that raise important points to consider (e.g. what's the required sample size and how are you going to account for temporal variability in the data you are using to generate the strata).</p> <p>However, the response is still quite vague on the methods used to define the strata. We can go back to AI and prompt it to expand on a method to convert geospatial layers into zones that define the strata.</p> <p>Prompt</p> <p>Can you outline a workflow to convert geospatial layers of vegetation indices, soil type and elevation into groups that represent strata for sampling. Use the k-means algorithm to generate clusters. </p> <p>Generate Python code that fits the k-means algorithm, converts the geospatial layers into a layer of zones defined by the clusters, and vectorise the clusters and store them as a GeoDataFrame.</p> Response (Gemini 2.0 Flash) <p>Workflow for Stratification using K-means Clustering</p> <p>Data Preparation</p> <ul> <li>Load your geospatial layers (vegetation indices, soil type, elevation) into appropriate data structures (e.g., NumPy arrays for raster data, GeoDataFrames for vector data).</li> <li>Ensure all layers have the same spatial extent and resolution. If not, resample or clip them to a common grid.</li> <li>Normalize the data for each layer to a common scale (e.g., between 0 and 1) to prevent features with larger values from dominating the clustering process.</li> </ul> <p>K-means Clustering</p> <ul> <li>Choose the number of clusters (k) you want to create. This will determine the number of strata for your sampling. Consider factors like the complexity of the landscape, the diversity of land cover types, and the desired level of detail in your stratification.</li> <li>Apply the k-means algorithm to the combined data from your geospatial layers. The algorithm will group pixels or polygons with similar characteristics into clusters.</li> </ul> <p>Python code example (using scikit-learn):</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming you have NumPy arrays: ndvi_array, soil_array, elevation_array\n# Stack the arrays into a single feature matrix\nX = np.dstack([ndvi_array, soil_array, elevation_array]).reshape(-1, 3)\n\n# Normalize the data\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-means clustering\nk = 5  # Number of clusters (strata)\nkmeans = KMeans(n_clusters=k, random_state=42)\nclusters = kmeans.fit_predict(X_scaled)\n\n# Reshape the cluster labels back to the original shape\ncluster_array = clusters.reshape(ndvi_array.shape)\n</code></pre> <p>Zone Creation</p> <ul> <li>Create a new raster layer or vector layer where each pixel or polygon is assigned a zone ID based on its cluster membership.</li> <li>Python code example (using rasterio for raster data):</li> </ul> <pre><code>import rasterio\n\n# Assuming you have a rasterio dataset 'src' for your original data\nwith rasterio.open('strata_raster.tif', 'w', **src.profile) as dst:\n    dst.write(cluster_array, 1)\n</code></pre> <p>Vectorization (Optional)</p> <ul> <li>If you prefer working with vector data, convert the raster layer of zones into a polygon shapefile or GeoJSON. This will create distinct polygons representing each stratum.</li> <li>Python code example (using rasterio and geopandas):</li> </ul> <pre><code>import geopandas as gpd\nfrom rasterio.features import shapes\n\n# Read the raster data\nwith rasterio.open('strata_raster.tif') as src:\n    image = src.read(1)\n    results = ({'properties': {'zone': v}, 'geometry': s}\n            for i, (s, v) in enumerate(shapes(image, mask=None, transform=src.transform)))\n\n# Create a GeoDataFrame\nstrata_gdf = gpd.GeoDataFrame.from_features(list(results), crs=src.crs)\n\n# Save the GeoDataFrame\nstrata_gdf.to_file(\"strata_polygons.geojson\", driver='GeoJSON')\n</code></pre> <p>Key Considerations</p> <ul> <li>Optimal k Value: Experiment with different values of k to find the optimal number of clusters that best capture the variability in your data. You can use techniques like the elbow method or silhouette analysis to evaluate cluster quality.</li> <li>Cluster Interpretation: Analyze the characteristics of the clusters (e.g., mean values of vegetation indices, soil types, elevation) to understand the environmental conditions they represent. This will help you interpret the results and refine your stratification if needed.</li> <li>Data Normalization: Ensure that you normalize your data appropriately to avoid bias in the clustering process.</li> <li>Spatial Autocorrelation: Consider the potential impact of spatial autocorrelation in your data. If neighboring pixels or polygons are highly correlated, it might influence the clustering results. You can explore spatial clustering techniques if this is a concern.</li> <li>Computational Resources: K-means clustering can be computationally intensive for large datasets. Consider using optimized libraries or cloud computing resources if necessary.</li> </ul> <p>This workflow provides a structured approach to generate strata for sampling using k-means clustering. Remember to adapt the code and parameters to your specific data and requirements.</p> <p>With only a handful of prompts we have a workflow in Python code that i) converts several geospatial layers into zones that can be used as strata, and ii) generates a sample of points using these zones as strata. There are also notes explaining each step and points raised for you to consider when implementing this workflow. If you scroll back to the top of this section, you will see through that we have very quickly generated more detailed and useful information to implement the training data generation component of this land cover classification workflow.   </p>"},{"location":"ai/resources/","title":"Resources","text":""},{"location":"ai/resources/#llms","title":"LLMs","text":""},{"location":"ai/resources/#ai-tools","title":"AI tools","text":"<ul> <li>STORM paper</li> <li>CO-STORM paper</li> <li>Introduction to NotebookLM (DAIR)</li> </ul>"},{"location":"ai/resources/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>DAIR.AI's prompting guide</li> <li>OpenAI's prompting guide</li> <li>Google Gemini's prompting guide</li> <li>Meta's prompting guide</li> </ul>"},{"location":"ai/responsible-use-of-ai/","title":"Responsible use of AI","text":""},{"location":"ai/responsible-use-of-ai/#ai-at-uwa","title":"AI at UWA","text":"<p>The following resources provide information on using AI at UWA and in your studies. You should consult these resources before using any AI tools.</p> <ul> <li>UWA Library AI resources</li> <li>Using AI Tools at UWA: A Guide for Students</li> </ul>"},{"location":"ai/responsible-use-of-ai/#ai-and-plagiarism","title":"AI and plagiarism","text":"<p>UWA's Academic Integrity Policy states that using AI generated content, without citation, counts as plagiarism.</p>"},{"location":"ai/responsible-use-of-ai/#ai-and-bias","title":"AI and bias","text":"<p>LLMs can generate biased responses; this might reflect biases in the models training data. For example, models may generate different responses given different geographic, cultural, demographic or socio-economic contexts. You can mitigate some biases in LLM responses by effective prompting; however, the risk of biased responses remains. You should carefully review responses for bias and be alert to this issue. </p> <p>Read this paper that highlights some the geographic biases in LLMs.</p>"},{"location":"ai/responsible-use-of-ai/#ai-and-private-and-personal-data","title":"AI and private and personal data","text":"<p>You should not reveal personal or private data to hosted LLMs (e.g. LLMs that you access from chatgpt.com or gemini.google.com). Where possible, you should turn off data sharing with hosted LLM and AI tool providers and select options that prevent your data being used to train models. However, taking that step doesn't necessarily mean you can share personal or private data with hosted LLMs. Some steps you can take to working with LLMs and personal or private data are:</p> <ul> <li>Uunderstanding what counts and personal or private data</li> <li>Use a LLM to generate a fake dataset that mimics the structure of your private data, and solve the problem working with the fake dataset</li> <li>Use a local LLM or a LLM running in a computational environment you control. See this example for setting up and running your own LLM using Hugging Face and Google Colab</li> </ul>"},{"location":"ai/responsible-use-of-ai/#ai-licensing-and-copyright","title":"AI, licensing and copyright","text":"<p>You should not upload content to AI tools unless permitted by the content's licensing or copyright, and you should not share AI generated response that are based on uploaded content that's licensing or copyright prohibits this use. You can find more information out about AI, copyright and licensing using the following resources:</p> <ul> <li>UWA Library AI resources - head to Ethical Considerations</li> <li>Creative Commons licenses and AI</li> <li>Arts Law - AI and copyright</li> </ul>"},{"location":"ai/verifying-and-checking-ai-responses/","title":"Verifying and checking AI outputs","text":"<p>It's important to verify and check the responses generated by AI tools, including large language models (LLMs). LLMs are probabilistic; they can generate different responses to the same prompt which may be more or less correct. LLMs can also hallucinate and confidently give you wrong answers. </p> <p>There are various techniques you can deploy to improve the quality (correctness and relevance) of AI generated responses. Many of these are outlined in the notes on prompt engineering. However, you will still need to check the responses generated by AI before you use them. The following sections outline some strategies and techniques you can use to verify AI generated responses.  </p>"},{"location":"ai/verifying-and-checking-ai-responses/#iteratively-work-with-ai-to-develop-solutions","title":"Iteratively work with AI to develop solutions","text":"<p>You will always need to implement some form of final checking of generated responses (as you should with all workflows), ultimately this will come down to a form of manual verification. However, you should not be passively generating AI content in your studies. You should be iteratively working with the AI as assistant to solve problems, to further your understanding of topics and concepts and to generate new ideas. Thus, through your active engagement with the AI tool as you solve problem you should be familiar with the LLM's response and be refining it to be more accurate and relevant for your task. </p>"},{"location":"ai/verifying-and-checking-ai-responses/#code-tests","title":"Code tests","text":"<p>In programming, tests verify that code behaves as expected and generates correct answers. It is best practice to implement some form of testing strategy for any code you write or use. This principle extends to working with AI generated outputs; you can manually create tests for code generated by AI tools. For example, if your code comprises a function that computes monthly precipitation totals your test could compare the function's result to a known value using a small curated dataset. Read more about testing here.</p> <p>A more efficient, but potentially riskier approach, is to prompt the LLM to generate a comprehensive list of tests for code it has generated. Use prompt engineering techniques to help the model generate good quality and relevant tests (this could be a good task for \"thinking\" models that encourage the LLM to think carefully about generating effective tests for the specific code in question). This technique has the advantage of quickly generating a large number of tests, which can be necessary for complex programs or analyses. The obvious disadvantage is that the tests are generated by an LLM, and so are susceptible to error too.</p>"},{"location":"ai/verifying-and-checking-ai-responses/#search","title":"Search","text":"<p>As discussed in the notes on AI tools, many LLMs are integrated with web search. This can improve response quality by allowing the LLM to combine knowledge from its internal weights (i.e. what it learnt during training) with information it retrieves from the internet while handling your prompt. The information from online sources can help keep the generated responses grounded with factual content, mitigating hallucinations, and aligned with up-to-date information. The LLMs provide links to the sources used; this makes it easy for you to click through and manually review the LLM generated response against these sources (and also undertake your own appraisal of the source quality).</p> <p>Google's Gemini app gives you a Double-check response option (found by clicking the three vertical dots at the bottom of the response). This will run a Google search and highlight your response in green, orange or red depending on whether supporting content was found online. ChatGPT and Perplexity also both integrate search with LLM prompts and providing grounding and links to help you check the response.</p> <p></p>"},{"location":"ai/verifying-and-checking-ai-responses/#grounding","title":"Grounding","text":"<p>Integrating LLM response generation with search is one form of grounding. LLMs are general reasoning, problem solving and text generation tools. While these models will have picked up information during their training, they are not databases, have not been optimised for your specific task and may not have access to information relevant to the task (e.g. private or more recent data). </p> <p>When creating your prompts, consider how you can provide the model with good quality and relevant information to ground its generated response. This could be through document uploads, dataset uploads, incorporating example datasets, sharing videos or uploading images. Check the copyright and licensing of any datasets you use for grounding.</p>"},{"location":"ai/verifying-and-checking-ai-responses/#picking-the-right-model","title":"Picking the \"right\" model","text":"<p>Generally, the most commonly used models, such as the LLMs released by OpenAI, Google or Meta, are highly performant across a range of tasks. However, they're constantly improving and adding new features (e.g. the \"thinking\" capabilities is a relatively new release for these models, as of February 2025). However, it's important to be aware of the different features associated with using different models and using the right model for your task to get the best outcomes. </p> <p>LLMs are great base models, they can be fine-tuned for different downstream tasks such as coding or reasoning or question-answering. You can find a comprehensive list of models on Hugging Face and you can review their model cards to see what tasks they're well suited for. </p> <p>There are a range of benchmark datasets used for evaluating and comparing LLMs across different tasks (e.g. maths, reasoning, conversation, visual understanding, coding). You can check a model's score on benchmarks relevant for the task in question or use leaderboards to compare models. </p>"},{"location":"notebooks/week-1_0_llms/","title":"Week 1 0 llms","text":"In\u00a0[1]: Copied! <pre>import torch\nfrom transformers import pipeline\n</pre> import torch from transformers import pipeline In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline(\n    \"text-generation\",\n    model=\"google/gemma-2-2b-it\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n</pre> pipe = pipeline(     \"text-generation\",     model=\"google/gemma-2-2b-it\",     model_kwargs={\"torch_dtype\": torch.bfloat16},     device_map=\"auto\", ) <p>You can pass in a list of prompts to <code>pipe</code> and specify the number of tokens you want generated (<code>max_new_tokens</code>).</p> <p>Each prompt is a dictionary object with keys for <code>\"role\"</code> and <code>\"content\"</code>. The text values associated with the <code>\"content\"</code> key is the prompt provided to the model.</p> <p><code>pipe</code> returns a list of dictionary objects storing the conversation between the user and the AI assistant.</p> In\u00a0[\u00a0]: Copied! <pre>prompts = [\n    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the distance between two points?\"},\n]\n\noutputs = pipe(prompts, max_new_tokens=512)\nprint(outputs)\n</pre> prompts = [     {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the distance between two points?\"}, ]  outputs = pipe(prompts, max_new_tokens=512) print(outputs) <p>It's a little fiddly to extract the generated response, but it's a good exercise to practice extracting information from lists and dictionary objects:</p> <ol> <li>subset the first element in the list object <code>outputs[0]</code> (remember Python indexes from 0)</li> <li>get the data in the dict under the <code>\"generated_text\"</code> key</li> <li>get the last element in that list (which stores the user-assistant conversation); <code>-1</code> means index the last element</li> <li>subset the text stored against the <code>\"content\"</code> key</li> </ol> <p>Work through this to make sure you understand the different ways of subsetting data from a list (subsetting by index position) and a dict (by key).</p> <p>Calling <code>strip()</code> on the string text data removes trailing and leading whitespace.</p> <p>Finally, <code>print()</code> the response from the AI assistant.</p> In\u00a0[\u00a0]: Copied! <pre>response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(response)\n</pre> response = outputs[0][\"generated_text\"][-1][\"content\"].strip() print(response) In\u00a0[\u00a0]: Copied! <pre>## NOTE! If you run the code above different Python code could be generated as LLMs are probabilistic.\n\nimport math\n\ndef distance(x1, y1, x2, y2):\n  \"\"\"Calculates the distance between two points.\n\n  Args:\n    x1: The x-coordinate of the first point.\n    y1: The y-coordinate of the first point.\n    x2: The x-coordinate of the second point.\n    y2: The y-coordinate of the second point.\n\n  Returns:\n    The distance between the two points.\n  \"\"\"\n  return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n\n# Example usage\nx1 = 1\ny1 = 2\nx2 = 4\ny2 = 6\n\ndistance_between_points = distance(x1, y1, x2, y2)\nprint(f\"The distance between the points ({x1}, {y1}) and ({x2}, {y2}) is: {distance_between_points}\")\n</pre> ## NOTE! If you run the code above different Python code could be generated as LLMs are probabilistic.  import math  def distance(x1, y1, x2, y2):   \"\"\"Calculates the distance between two points.    Args:     x1: The x-coordinate of the first point.     y1: The y-coordinate of the first point.     x2: The x-coordinate of the second point.     y2: The y-coordinate of the second point.    Returns:     The distance between the two points.   \"\"\"   return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)  # Example usage x1 = 1 y1 = 2 x2 = 4 y2 = 6  distance_between_points = distance(x1, y1, x2, y2) print(f\"The distance between the points ({x1}, {y1}) and ({x2}, {y2}) is: {distance_between_points}\") <p>LLMs are helpful as assistants when trying to solve more complex coding or spatial analysis tasks.</p> Example prompts with code snippets (note, you might need to change the number of tokens you want generated to capture the full response). <pre>prompts = [\n    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the distance between two points on a sphere? The points are represented as latitude and longitude pairs.\"},\n]\n\noutputs = pipe(prompts, max_new_tokens=512)\n\nresponse = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(response)\n</pre> <pre>prompts = [\n    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the shortest line between three points?\"},\n]\n\noutputs = pipe(prompts, max_new_tokens=1024)\n\nresponse = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(response)\n</pre> <pre>prompts = [\n    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the length of all the edges of a Polygon?\"},\n]\n\noutputs = pipe(prompts, max_new_tokens=1024)\n\nresponse = outputs[0][\"generated_text\"][-1][\"content\"].strip()\nprint(response)\n</pre>"},{"location":"notebooks/week-1_0_llms/#using-llms-in-google-colab","title":"Using LLMs in Google Colab\u00b6","text":"<p>This notebook demonstrates how you can use LLMs programmatically within a Jupyter notebook environment on Google Colab.</p> <p>While you can easily use LLMs via their apps or website chat interfaces, it's also possible to programmatically pass prompts to these models and capture their responses. You can also download the actual LLM to your environment and pass prompts to it directly (i.e. you are not passing information to a model hosted by Google or OpenAI or Meta) There are some advantages to doing this:</p> <ul> <li>Access LLMs directly within your coding environment</li> <li>Run LLMs on your own hardware or within a personal environment, this is useful when you are working with private data that you cannot share or upload to hosted LLMs</li> <li>Demonstrates how you can build custom apps and workflows on top of LLM technologies</li> </ul> <p>To use LLMs within Jupyter notebooks on Google Colab, there are two initial setup steps:</p> <ol> <li>Create a Hugging Face account and add your account token to the Google Colab Secrets. Then restart your Google Colab session.</li> <li>Ensure that you are using the GPU Google Colab runtime.</li> </ol>"},{"location":"notebooks/week-1_0_llms/#hugging-face","title":"Hugging Face\u00b6","text":"<p>Hugging Face are a machine learning and AI technology company that host trained models and develop software tools for working with AI models.</p>"},{"location":"notebooks/week-1_0_llms/#hugging-face-accounts-and-tokens","title":"Hugging Face accounts and tokens\u00b6","text":"<p>To use many of the models on Hugging Face you need to create an account and store your accounts API key in Google Colab's secrets. This lets you make authenticated calls to Hugging Face.</p>"},{"location":"notebooks/week-1_0_llms/#hugging-face-pipelines","title":"Hugging Face pipelines\u00b6","text":"<p>A pipeline is a Python object that provides a wrapper around the code required to use LLMs and other AI tools. In short, pipelines take away a lot of the work of setting up AI models within your Python environment and the associated data processing. You simply create a <code>pipeline</code> object, pass your prompt in, and <code>print()</code> the response.</p>"},{"location":"notebooks/week-1_0_llms/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-1_0_llms/#setup-hugging-face-token","title":"Setup Hugging Face token\u00b6","text":"<ol> <li>Create a Hugging Face account:</li> </ol> <p>Once, you have created a Hugging Face account you can gain access to models. Many LLMs require you accept terms and conditions. For this exercise we will be working with Google's Gemma 2 2b it model (2b represents 2 billion parameters and it represents instruction tuned). Agree to the model's terms and conditions and usage policy here.</p> <ol> <li>Create an Access Token:</li> </ol> <ol> <li>Click on Create new token</li> </ol> <ol> <li>Set the token permissions</li> </ol> <p>You can initially set token permissions to Read, which has read access to all your resources. This is a good option for getting started.</p> <p>However, as you start developing resources such as models and datasets and using Hugging Face in different environments, it's a good idea to create access tokens with fine-grained permissions with just enough permissions to complete tasks associated with the token.</p> <p>If you have selected fine-grained permissions, you will need to add repositories (models) that you want that token to grant permission to.</p> <ol> <li>Click Create token to generate the access token.</li> </ol> <ol> <li>Copy the access token. This is your only opportunity to do this - keep a record of the token (in a secure location). If you lose your token, it's easy to generate a new one.</li> </ol> <ol> <li>In Google Colab, click on the key icon in the left-hand sidebar.</li> </ol> <ol> <li>Add your Hugging Face access token with the name <code>HF_TOKEN</code> and make sure the Notebook access it checked. Restart your Google Colab session to load the token into your environment.</li> </ol>"},{"location":"notebooks/week-1_0_llms/#setup-runtime","title":"Setup runtime\u00b6","text":"<p>This lab will only work (quickly) using Google Colab with a T4 GPU runtime type.</p> <p>Before running any code, set the runtime type to T4 GPU.</p> <p></p>"},{"location":"notebooks/week-1_0_llms/#install-packages","title":"Install packages\u00b6","text":""},{"location":"notebooks/week-1_0_llms/#using-pipeline-objects","title":"Using <code>pipeline</code> objects\u00b6","text":"<p>To create a <code>pipeline</code> object call the <code>pipeline()</code> instantiator function as pass in the task (<code>\"text-generation\"</code> here), the model (<code>\"google/gemma-2-2b-it\"</code>), and some additional arguments. Set the <code>device_map</code> argument to <code>\"auto\"</code> to run the model on Colab's GPU when it is available.</p> <p>This will download the model to your Colab environment, which will take a moment or two.</p>"},{"location":"notebooks/week-1_0_llms/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Understanding a Python program, can you identify: <ul> <li>What is the purpose of the program?</li> <li>What input data does the program require?</li> <li>What data type is the input data?</li> <li>What user defined functions does the program use?</li> <li>What built in operations does the program use?</li> <li>What data type is the data returned by the function?</li> </ul> </p> <p>We can copy and paste the Python code out of the printed response by the LLM and execute it (the Python code is contained within ``` marks).</p>"},{"location":"notebooks/week-1_0_llms/#activities","title":"Activities\u00b6","text":"Create prompts and generate responses for each of the following tasks. Use the code snippets above as an example of how to use <code>pipeline</code> objects.  <ul> <li>Computing the distance between two points on a sphere.</li> <li>Computing the shortest line that joins three points.</li> <li>Computing the length of all the edges of a polygon. </li> </ul> How can you verify the response from the LLM is correct? <ul> <li>Ask the LLM to generate test examples (check the tests are correct).</li> <li>Look at how the LLM has solved the problem and compare these solutions with what you find from independent research.</li> <li>Generate a suite of cases where you know the correct answer and compare these ground truth values with what is returned when executing the LLM generated solution.</li> </ul>"},{"location":"notebooks/week-1_1/","title":"Week 1 1","text":"In\u00a0[26]: Copied! <pre>import os\nimport torch\nfrom transformers import pipeline\n</pre> import os import torch from transformers import pipeline In\u00a0[\u00a0]: Copied! <pre>pipe = pipeline(\n    \"text-generation\",\n    model=\"google/gemma-2-2b-it\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n</pre> pipe = pipeline(     \"text-generation\",     model=\"google/gemma-2-2b-it\",     model_kwargs={\"torch_dtype\": torch.bfloat16},     device_map=\"auto\", ) In\u00a0[\u00a0]: Copied! <pre>prompt = \"What is canola?\"\n\noutputs = pipe(prompt, max_new_tokens=512)\nresponse = outputs[0][\"generated_text\"].strip()\nprint(response)\n</pre> prompt = \"What is canola?\"  outputs = pipe(prompt, max_new_tokens=512) response = outputs[0][\"generated_text\"].strip() print(response) In\u00a0[\u00a0]: Copied! <pre>prompt = \"Drought is affecting my wheat crop. What strategies can I use to protect crop yields?\"\n\noutputs = pipe(prompt, max_new_tokens=512)\nresponse = outputs[0][\"generated_text\"].strip()\nprint(response)\n</pre> prompt = \"Drought is affecting my wheat crop. What strategies can I use to protect crop yields?\"  outputs = pipe(prompt, max_new_tokens=512) response = outputs[0][\"generated_text\"].strip() print(response) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>prompt = \"What insects and pests affect banana crops?\"\n\noutputs = pipe(prompt, max_new_tokens=512)\nresponse = outputs[0][\"generated_text\"].strip()\nprint(response)\n</pre> In\u00a0[54]: Copied! <pre>prompts = [\n    \"When are wheat crops ready for harvest in Western Australia?\",\n    \"What is harvest weed seed control?\",\n]\n</pre> prompts = [     \"When are wheat crops ready for harvest in Western Australia?\",     \"What is harvest weed seed control?\", ] <p><code>prompts</code> is a list of two elements. Each element is a prompt for our LLM of <code>str</code> type data. Elements in a Python <code>list</code> have an index position starting at 0 for the first element. We can access elements of a list using their index. For example, to access the second element of the list we would execute the following statement:</p> <p><code>prompts[1]</code></p> <p>Note that index position 1 corresponds to the second element. Indexing starts at 0.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"the first element in the list is {prompts[0]}\")\nprint(f\"the second element in the list is {prompts[1]}\")\n</pre> print(f\"the first element in the list is {prompts[0]}\") print(f\"the second element in the list is {prompts[1]}\") <p>We can add items to our list using a <code>list</code> object's <code>append()</code> method. Note, this is a function as you can see the <code>()</code> after append.</p> <p>To use the <code>append()</code> method, we pass in the data we wish to append to the end of the list. Let's add another prompt to our list: <code>\"What herbicides are used in Australia?\"</code></p> In\u00a0[\u00a0]: Copied! <pre>prompts.append(\"What herbicides are used in Western Australia?\")\nprint(prompts)\n</pre> prompts.append(\"What herbicides are used in Western Australia?\") print(prompts) In\u00a0[62]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>prompts.append(\"What grain crops are grown in France?\")\nprompts\n</pre> In\u00a0[\u00a0]: Copied! <pre>for p in prompts:\n    print(p)\n</pre> for p in prompts:     print(p) In\u00a0[\u00a0]: Copied! <pre>for p in prompts:\n    print(\"Question for the farm assistant:\")\n    outputs = pipe(p, max_new_tokens=512)\n    response = outputs[0][\"generated_text\"].strip()\n    print(response)\n    print(\"\")\n    print(\"***************\")\n    print(\"\")\n</pre> for p in prompts:     print(\"Question for the farm assistant:\")     outputs = pipe(p, max_new_tokens=512)     response = outputs[0][\"generated_text\"].strip()     print(response)     print(\"\")     print(\"***************\")     print(\"\")"},{"location":"notebooks/week-1_1/#python-programs","title":"Python programs\u00b6","text":"<p>This is an introductory lab where you will run a Python program to create an AI-powered farm management chat assistant using a language model.</p> <p>The goal of this lab is to provide a brief introduction to the Python programming language; demonstrate some programming concepts such as data types, data structures, and functions; and to illustrate why programming is a useful skill for solving problems and implementing data-driven solutions.</p>"},{"location":"notebooks/week-1_1/#a-python-program","title":"A Python program\u00b6","text":"<ul> <li>A program is a series of statements that are executed to complete a task</li> <li>Here, the statements are written using the syntax of the Python programming language</li> <li>Programs complete tasks by loading, transforming, and visualising data</li> <li>Data in programs can be of different types (numeric, text, boolean)</li> <li>Data can be combined into data structures to represent complex concepts and objects</li> <li>Functions operate on data to complete tasks</li> </ul>"},{"location":"notebooks/week-1_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-1_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-1_1/#setup-hugging-face-token","title":"Setup Hugging Face token\u00b6","text":"<ol> <li>Create a Hugging Face account:</li> </ol> <p>Once, you have created a Hugging Face account you can gain access to models. Many LLMs require you accept terms and conditions. For this exercise we will be working with Google's Gemma 2 2b it model (2b represents 2 billion parameters and it represents instruction tuned). Agree to the model's terms and conditions and usage policy here.</p> <ol> <li>Create an Access Token:</li> </ol> <ol> <li>Click on Create new token</li> </ol> <ol> <li>Set the token permissions</li> </ol> <p>You can initially set token permissions to Read, which has read access to all your resources. This is a good option for getting started.</p> <p>However, as you start developing resources such as models and datasets and using Hugging Face in different environments, it's a good idea to create access tokens with fine-grained permissions with just enough permissions to complete tasks associated with the token.</p> <p>If you have selected fine-grained permissions, you will need to add repositories (models) that you want that token to grant permission to.</p> <ol> <li>Click Create token to generate the access token.</li> </ol> <ol> <li>Copy the access token. This is your only opportunity to do this - keep a record of the token (in a secure location). If you lose your token, it's easy to generate a new one.</li> </ol> <ol> <li>In Google Colab, click on the key icon in the left-hand sidebar.</li> </ol> <ol> <li>Add your Hugging Face access token with the name <code>HF_TOKEN</code> and make sure the Notebook access it checked. Restart your Google Colab session to load the token into your environment.</li> </ol>"},{"location":"notebooks/week-1_1/#setup-runtime","title":"Setup runtime\u00b6","text":"<p>This lab will only work (quickly) using Google Colab with a T4 GPU runtime type.</p> <p>Before running any code, set the runtime type to T4 GPU.</p> <p></p>"},{"location":"notebooks/week-1_1/#import-modules","title":"Import modules\u00b6","text":"<p>A package is a collection of files containing Python code that we can use in our program. Files containing Python code are called modules (a package is a collection of modules). By importing Python packages, we can make our programs shorter and reuse code that has already been written to complete a task.</p> <p>For example, the pandas provides a DataFrame structure for storing tabular data and specific functions for working with tabular data.</p> <p>We need to immport the modules into our Python program where they can be used.</p> <p>Here, we import a package called <code>os</code>, which contains tools for working with the operating system (e.g. creating paths to folders and files).</p> <p>We also import <code>torch</code> and <code>transformers</code>, which are machine learning packages.</p> <p>Importing these packages into our program means we can re-use code that has already been written to work with deep learning models that can receive and generate natural language. We'll use these tools in our program to create an AI farm chat assistant.</p>"},{"location":"notebooks/week-1_1/#using-pipeline-objects","title":"Using <code>pipeline</code> objects\u00b6","text":"<p>To create a <code>pipeline</code> object call the <code>pipeline()</code> instantiator function as pass in the task (<code>\"text-generation\"</code> here), the model (<code>\"google/gemma-2-2b-it\"</code>), and some additional arguments. Set the <code>device_map</code> argument to <code>\"auto\"</code> to run the model on Colab's GPU when it is available.</p> <p>An object in Python programs is a container for related data and functions. Data is information related to a particular concept, represented by the object, and functions do things with data to complete a task.</p> <p>This will download the model to your Colab environment, which will take a moment or two. Here, we are using Google's Gemma 2 2b it model - follow this link to make yourself aware of the terms of use for this model.</p>"},{"location":"notebooks/week-1_1/#functions","title":"Functions\u00b6","text":"<p>Our <code>pipe</code> object can be treated as a function. You can spot functions by <code>()</code> appearing after the function name. Functions can take input data - this is the function's arguments -  and use these inputs to complete it's task.</p> <p>Functions can return data back to the Python program, display data on your screen, or save data to disk.</p> <p>The general pattern for executing a function / method is:</p> <ol> <li>The function is called and any data are passed in as arguments inside <code>()</code></li> <li>The function performs operations on the data passed in</li> <li>The function returns the result of operating on the data</li> </ol> <p>The <code>pipe()</code> function takes in text data as a prompt (here we ask <code>\"What is canola?\"</code>) and a number to the <code>max_new_tokens</code> paramter.</p> <p>It returns the output text that the Gemma model has generated in response to the prompt. This output text is referenced by the variable <code>output</code>.</p> <p>The Gemma model we are using is a reduced size and open large language model. Therefore, its responses might not be as accurate as ones generated from larger models available online (e.g. Gemini or ChatGPT). We would probably not use this version of the Gemma model directly in a agricultural chat assistant; we would fine-tune or train a LLM to be more accurate with agricultural topics. However, it is a good example of how we can use Python to build a program to provide a useful service or solve a task. In this case, we're building a program that would let people use AI to get more information about farm management.</p> <p>It is also your responsibility to only provide agriculturally relevant prompts to the model. You should not create offensive, personal, or controversial prompts. This is also a more general principle when working with big datasets, machine learning models and when developing technology. It is important to do so in a way that causes no harm.</p>"},{"location":"notebooks/week-1_1/#recap-quiz","title":"Recap quiz\u00b6","text":"Is <code>print()</code> a function? <p>Yes, <code>print()</code> is a function. You can tell it's a function because of the parentheses after the function name. The fact that print is a verb also indicates it's a function - something is being done (nouns are often used to indicate data). <code>print()</code> takes in data from our Python program and prints a representation of this data on our display. Above, we pass in the output from generating text with our Gemma model and print it on the display.</p>"},{"location":"notebooks/week-1_1/#data","title":"Data\u00b6","text":"<p>Programs generally complete their tasks by doing things with data. Data can be scientific data which can be analysed and visualised. However, in a computer program data refers to any information the program needs to run. For example, it could be text data such as file paths or file names or URLs for resources on the internet.</p> <p>There are different types of data. Python provides support for the following built-in data types:</p> <ul> <li><code>float</code> - storing floating point numeric values.</li> <li><code>int</code> - storing integer numeric values.</li> <li><code>str</code> - storing text data as strings.</li> <li><code>bool</code> - storing True or False as boolean values.</li> <li><code>None</code> - storing null values.</li> <li><code>bytes</code> - storing raw binary data.</li> </ul> <p>Our AI-powered farm assistant program requires two key pieces of data. Text prompts for the user (i.e. questions or requests for information) and text outputs from the LLM. Here, we'll be storing text data as string (<code>str</code>) type data in our program.</p> <p>In Python, we create <code>str</code> type data by placing text inside quotation marks <code>\"a string\"</code>.</p> <p>Let's create some different text prompts as <code>str</code> type data.</p>"},{"location":"notebooks/week-1_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you create a new prompt of <code>str</code> type data to ask the model about pests affecting bananas?</p>"},{"location":"notebooks/week-1_1/#data-structures","title":"Data structures\u00b6","text":"<p>Python provides a series of built-in data structures that can be used to group together and store related data.</p> <p>You can think of a data structure as a container for related data values in a Python program. For example, a DataFrame is often used to store a tabular datset similar to how you would store data in a spreadsheet in excel.</p> <p>A commonly used data structure in Python is a <code>list</code>. A <code>list</code> stores .... a list of data values.</p> <p>A Python <code>list</code> is created by placing data values inside square brackets <code>[]</code>. If we have many prompts or questions we want to ask out farm assistant, we can store them all in a list.</p>"},{"location":"notebooks/week-1_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you add another item to <code>prompts</code>? The item to add is <code>\"What grain crops are grown in France?\"</code>.</p>"},{"location":"notebooks/week-1_1/#flow-control","title":"Flow control\u00b6","text":"<p>Flow control refers to the order in which the statements that make up a program are executed. A common flow control tool are a for loops. For loops allow us to iterate over items in a sequence; we can loop over items in a list. To loop over the items in <code>prompts</code> and print the current item we would execute:</p>"},{"location":"notebooks/week-1_1/#putting-it-all-together-a-python-program","title":"Putting it all together: a Python program\u00b6","text":"<p>Now we can put together the various concepts we've covered above (data types, data structures, functions, and flow control) to create a simple program that can store multiple prompts or questions from a user, pass these prompts into our LLM, and return answers.</p> <p>This is a basic template of Python program to create an AI-powered farm chat assistant app. If we were expanding this we might build a user interface (e.g. a web form) where users could enter prompts that would be added to the <code>prompts</code>. We should also use a model that is trained and tested for the application domain we're working in. However, this is a small illustration of how to write a Python program to complete a task (in this case building a small prototype application for a farm chat assistant).</p>"},{"location":"notebooks/week-1_2/","title":"Week 1 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-1_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-1_2\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-1_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-1_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-1_2\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-1_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre># Import modules\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> # Import modules import os import pandas as pd import geopandas as gpd import plotly.express as px import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\")\n\n# Get a list of canola yield data\nharvester_data_files = os.listdir(harvester_data_path)\n\n# Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n\ndf = pd.concat(dfs, axis=0)\n\n# Inspect the data frame\ndf.head()\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\")  # Get a list of canola yield data harvester_data_files = os.listdir(harvester_data_path)  # Combine the csv files into one data frame dfs = []  for i in harvester_data_files:     if i.endswith(\".csv\"):         tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))         dfs.append(tmp_df)  df = pd.concat(dfs, axis=0)  # Inspect the data frame df.head() In\u00a0[\u00a0]: Copied! <pre># Transform the yield data to a spatial format\npoints = gpd.points_from_xy(df[\"Longitude\"], df[\"Latitude\"], crs=\"EPSG:4326\")\ngdf = gpd.GeoDataFrame(df, geometry=points)\n\n# Visualise the crop yield data on a web map\nbasemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nattribution = \"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\ngdf.explore(column=\"DryYield\", cmap=\"plasma\", tooltip=\"DryYield\", vmax=2, tiles=basemap, attr=attribution)\n</pre> # Transform the yield data to a spatial format points = gpd.points_from_xy(df[\"Longitude\"], df[\"Latitude\"], crs=\"EPSG:4326\") gdf = gpd.GeoDataFrame(df, geometry=points)  # Visualise the crop yield data on a web map basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\" attribution = \"Tiles \u00a9 Esri \u2014 Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\" gdf.explore(column=\"DryYield\", cmap=\"plasma\", tooltip=\"DryYield\", vmax=2, tiles=basemap, attr=attribution) In\u00a0[\u00a0]: Copied! <pre>print(type(0.227))\n</pre> print(type(0.227)) In\u00a0[\u00a0]: Copied! <pre>print(type(\"a string\"))\nprint(\"a string\")\n</pre> print(type(\"a string\")) print(\"a string\") <p>Let's check how the sample id names in the canola yield dataset are represented. First, let's remind ourselves what the <code>DataFrame</code> of the yield dataset looks like.</p> In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre># get the first field name value\nfield_name = df.loc[:, \"sample_id\"].to_list()[0]\nprint(type(field_name))\nprint(field_name)\n</pre> # get the first field name value field_name = df.loc[:, \"sample_id\"].to_list()[0] print(type(field_name)) print(field_name) In\u00a0[\u00a0]: Copied! <pre>print(type(0.277 &gt; 0.2))\nprint(0.227 &gt; 0.2)\n</pre> print(type(0.277 &gt; 0.2)) print(0.227 &gt; 0.2) <p>We can use the <code>isinstance(value, type)</code> function to test if a data value matches a data type. Let's test if our crop yield value is numeric or string.</p> In\u00a0[\u00a0]: Copied! <pre>print(isinstance(0.227, float))\nprint(isinstance(0.227, str))\n</pre> print(isinstance(0.227, float)) print(isinstance(0.227, str)) In\u00a0[\u00a0]: Copied! <pre>[\"bf66_sample_1\", \"bf66_sample_2\"]\n</pre> [\"bf66_sample_1\", \"bf66_sample_2\"] <p>Lists allow us to store duplicate values. The following is a valid list.</p> In\u00a0[\u00a0]: Copied! <pre>[\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"]\n</pre> [\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"] <p>A key feature of lists are that they can be modified in place. This makes lists useful data structures for tasks when the number of objects that we want to store in a collection can change during our program's execution. We can use functions such as <code>.append()</code> to add objects to the end of a list.</p> In\u00a0[\u00a0]: Copied! <pre>sample_id_list = [\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"]\nsample_id_list.append(\"bf66_sample_3\")\nprint(sample_id_list)\n</pre> sample_id_list = [\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"] sample_id_list.append(\"bf66_sample_3\") print(sample_id_list) Using lists in our Python program to read in CSV files on harvester crop yield data <p>In our Python program above, we took advantage of the fact that lists can be modified in place to loop over the CSV files in a directory, import them in as pandas <code>DataFrames</code>, and append the <code>DataFrames</code> to the list.</p> <pre># Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n</pre> <p>Here, we start with an empty list <code>dfs</code> denoted by just square brackets <code>[]</code>. Then, successively, a new object is added to <code>dfs</code> using the <code>append()</code> function. We finish with a list storing three pandas <code>DataFrames</code>. This demonstrates how we change the length and contents of a list during the execution of a program. We have gone from a list with zero elements to a list a list with three elements.</p> <p></p> <p>We can store any Python object in a list. We can also mix the types of objects stored in lists. The following is a valid list.</p> In\u00a0[\u00a0]: Copied! <pre>[1, 2, \"not a number\", None]\n</pre> [1, 2, \"not a number\", None] <p>Lists are ordered collections of data. If you add an element to a list it will be appended to the last position. List items can be accessed by their index location with the first element having index <code>0</code>.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_id_list)\nprint(\"The first element in sample_id_list is at index 0: \", sample_id_list[0])\nprint(\"The second element in sample_id_list is at index 1: \", sample_id_list[1])\nprint(\"The third element in sample_id_list is at index 2: \", sample_id_list[2])\n</pre> print(sample_id_list) print(\"The first element in sample_id_list is at index 0: \", sample_id_list[0]) print(\"The second element in sample_id_list is at index 1: \", sample_id_list[1]) print(\"The third element in sample_id_list is at index 2: \", sample_id_list[2]) <p>We can use index locations of elements in a list to create slices of list elements. For example, <code>sample_id_list[0:2]</code> would slice out the first two elements of the list. Note, the element referenced by the index in the final number of the slice is excluded.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_id_list[0:2])\n</pre> print(sample_id_list[0:2]) In\u00a0[\u00a0]: Copied! <pre>(116.804075, -33.889203)\n</pre> (116.804075, -33.889203) <p>Here, we have created a tuple with two numeric objects. Similar to lists we can access tuple elements by their index locations. Note the use of the <code>[]</code> operator to access elements by their index location.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"First tuple element: \", (116.804075, -33.889203)[0])\n</pre> print(\"First tuple element: \", (116.804075, -33.889203)[0]) In\u00a0[\u00a0]: Copied! <pre>print(\"Second tuple element: \", (116.804075, -33.889203)[1])\n</pre> print(\"Second tuple element: \", (116.804075, -33.889203)[1]) <p>As tuples are fixed-length and unchangeable, we cannot append elements to them in the same way we could with lists. This makes them useful data structures for storing data values which we don't want to change. For example, coorindate pairs that describe a location's x and y values (e.g. longitude and latitude) have two elements. Therefore, a tuple could be a suitable data format to store coordinate pairs.</p> <p>The shape of pandas <code>DataFrames</code> is also a tuple. A <code>DataFrame</code> has two dimensions: number of rows and number of columns. Thus, a tuple is a sensible data structure for storing the shape of <code>DataFrame</code> objects.</p> In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape <p>To demonstrate that we cannot change tuple values, let's try and update the number of rows in the tuple storing the shape of <code>df</code>.</p> In\u00a0[\u00a0]: Copied! <pre>df.shape[0] = 5\n</pre> df.shape[0] = 5 <p>We have returned a <code>TypeError</code> informing us the tuple objects do not support item assigment (adding new items to the tuple).</p> <p>Similar to lists, elements of a tuple are ordered and can be duplicated.</p> In\u00a0[\u00a0]: Copied! <pre>{\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\", \"bf66_sample_3\"}\n</pre> {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\", \"bf66_sample_3\"} In\u00a0[\u00a0]: Copied! <pre>{1, 2, 3, 3, 4}\n</pre> {1, 2, 3, 3, 4} <p>As sets are not ordered, we cannot access their elements by numeric index locations.</p> In\u00a0[\u00a0]: Copied! <pre># This fails as set objects are not subscriptable\n{\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}[0]\n</pre> # This fails as set objects are not subscriptable {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}[0] <p>We can access set elements by looping over them or checking if a value is in the set.</p> In\u00a0[\u00a0]: Copied! <pre>for i in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}:\n    print(i)\n</pre> for i in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}:     print(i) In\u00a0[\u00a0]: Copied! <pre>print(\"Checking if 'bf66_sample_1' is in the set:\")\nprint(\"bf66_sample_1\" in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"})\n</pre> print(\"Checking if 'bf66_sample_1' is in the set:\") print(\"bf66_sample_1\" in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}) <p>Similar to tuples, sets are unchangeable (immutable). Once created, we cannot change the set's values in our programs.</p> In\u00a0[\u00a0]: Copied! <pre>{\n\"Elevation(m)\": [213, 222, 214, 254],\n\"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]\n}\n</pre> { \"Elevation(m)\": [213, 222, 214, 254], \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"], \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"] } <p>Keys of dictionary objects cannot be duplicated. For example:</p> In\u00a0[\u00a0]: Copied! <pre>{\n\"Elevation(m)\": [213, 222, 214, 254],\n\"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]\n}\n</pre> { \"Elevation(m)\": [213, 222, 214, 254], \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"], \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"], \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"] } <p>To access elements of a dictionary object we can refer to its key. The following code snippet extracts a list of dates from the dictionary object.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict = {\"Elevation(m)\": [213, 222, 214, 254],\n    \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n    \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]}\ndates = demo_dict[\"Date\"]\ndates\n</pre> demo_dict = {\"Elevation(m)\": [213, 222, 214, 254],     \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],     \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]} dates = demo_dict[\"Date\"] dates <p>Dictionary objects have a <code>get()</code> function that we can use to extract elements.</p> In\u00a0[\u00a0]: Copied! <pre>dates = demo_dict.get(\"Date\")\ndates\n</pre> dates = demo_dict.get(\"Date\") dates <p>Dictionary objects also have a <code>keys()</code> function that returns a list of keys.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict.keys()\n</pre> demo_dict.keys() <p>There is also a <code>values()</code> function that we can use to return a list of values.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict.values()\n</pre> demo_dict.values() <p>And, the <code>items()</code> function returns a tuple of key-value pairs.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict.items()\n</pre> demo_dict.items() <p>We can add elements to a dictionary object by providing a new key with corresponding values.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict[\"yield\"] = [1.45, 2.5, 3, 2.8, 5.5]\ndemo_dict\n</pre> demo_dict[\"yield\"] = [1.45, 2.5, 3, 2.8, 5.5] demo_dict <p>You will notice that the list of yield values we just added to the dictionary has a different number of elements to the other elements in the values slots.</p> <p>Edit the following code snippet to retrieve and print the 3rd element in <code>x</code>.</p> In\u00a0[\u00a0]: Copied! <pre>x = [1, 2, 3, 4]\n# add code here #\n</pre> x = [1, 2, 3, 4] # add code here # answer <pre>print(x[2])\n</pre> <p>Edit the following code snippet to retrieve and print the first element in <code>z</code>.</p> In\u00a0[\u00a0]: Copied! <pre>z = (4, 5)\n# add code here #\n</pre> z = (4, 5) # add code here # answer <pre>print(z[0])\n</pre> <p>What is the data type of the value associated with the <code>field</code> key in the dict <code>z</code>? Retrieve this value from the dict and print its type.</p> In\u00a0[\u00a0]: Copied! <pre>z = {\n\"name\": \"farm 1\",\n\"field\": 439,\n\"crop\": \"canola\"\n}\n# add code here #\n</pre> z = { \"name\": \"farm 1\", \"field\": 439, \"crop\": \"canola\" } # add code here # answer <pre>print(type(z[\"field\"]))\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\")\n</pre> print(\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\") <p>Now, let's assign the data object storing string URL to the name <code>basemap</code>.</p> In\u00a0[\u00a0]: Copied! <pre>basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nprint(basemap)\n</pre> basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\" print(basemap) <p>Calling <code>print()</code> on the variable name returns the data object that the variable name refers to.</p> <p>We can assign any Python objects to variable names. When the variable name is used in a Python statement, the data value which the variable points to is used in the operations.</p> In\u00a0[\u00a0]: Copied! <pre>x = 1\ny = 2\nz = x + y\nprint(z)\n</pre> x = 1 y = 2 z = x + y print(z) <p>Variables make a program's code easier to organise, write, and understand. We could have performed the above addition operation by just writing <code>1 + 2</code>. However, this doesn't provide us with a way to capture the result of that operation and use it again in our program without re-running <code>1 + 2</code>.</p> <p>Variables provide us with a mechanism by which we retrieve and use data objects (that are stored in the computer's memory) at various places in our program. A variable name points to the location in the computer's memory where a data object is stored.</p> <p>While using the result of <code>1 + 2</code> is a trivial example, there are many cases where using variables is important. This statement reads a CSV file into our Python program: <code>tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))</code>. To access the data stored in the CSV file we can use variable name <code>tmp_df</code> which points to where the data in the CSV file was loaded into memory. We don't need to read the CSV file from disk each time we want to access its data.</p> <p>Let's make these concepts concrete.</p> <p>When we assign the string <code>\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"</code> to the variable name <code>basemap</code>, the variable name is a pointer to the string's data object.</p> <p></p> <p>If we create another variable called <code>basemap_1</code> and assign it to <code>basemap</code>, both variables will point to the same data object (the string URL).</p> <p></p> <p>Let's check this.</p> In\u00a0[\u00a0]: Copied! <pre>basemap_1 = basemap\nprint(basemap_1)\n</pre> basemap_1 = basemap print(basemap_1) <p>If we assign a new data object to <code>basemap</code>, <code>basemap_1</code> will still point to the original string data.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>basemap = [1, 2, 3, 4, 5]\nprint(basemap)\nprint(basemap_1)\n</pre> basemap = [1, 2, 3, 4, 5] print(basemap) print(basemap_1) <p>If we assign <code>basemap_1</code> to a new data object the string data for URL will no longer be accessible in our program and will eventually be removed from the computer's memory.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"this statement is executed first\")\nprint(\"and this statement is executed second\")\n</pre> print(\"this statement is executed first\") print(\"and this statement is executed second\") In\u00a0[\u00a0]: Copied! <pre>x = [1, 2, 3]\n\nfor i in x:\n    print(i + 10)\n</pre> x = [1, 2, 3]  for i in x:     print(i + 10) <p>It is important to note that the statements inside the for loop must be indented.</p> <p>Let's refer back to our program to read in crop yield data and see a use of for loops.</p> <pre># Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        print(f\"Loading file {i} into a Pandas DataFrame\")\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n</pre> <p>Here, the for loop is iterating over a list of file paths to CSV files storing crop yield data. For each of the CSV files in the list, the data is read into a variable <code>tmp_df</code> in turn, and, then appended to the list <code>dfs</code>. This is an example of how we can loop over a series of files in a folder on our computer and read their data into our Python program.</p> <p>Let's modify the for loop to illustrate this.</p> In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\")\n\n# Get a list of canola yield data\nharvester_data_files = os.listdir(harvester_data_path)\n\n# Check we have a list of csv files\nprint(harvester_data_files)\n\n# loop over elements in harvester_data_files\n# i takes on the value of a path to a csv file\nfor i in harvester_data_files: \n    if i.endswith(\".csv\"):\n        print(\" \")\n        print(\"**********************************************************\")\n        print(f\"We are currently loading file {i} into a Pandas DataFrame\")\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        print(tmp_df.head())\n        dfs.append(tmp_df)\n\nprint(\" \")\nprint(f\"We have a list of {len(dfs)} Pandas DataFrames read from csv files\")\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\")  # Get a list of canola yield data harvester_data_files = os.listdir(harvester_data_path)  # Check we have a list of csv files print(harvester_data_files)  # loop over elements in harvester_data_files # i takes on the value of a path to a csv file for i in harvester_data_files:      if i.endswith(\".csv\"):         print(\" \")         print(\"**********************************************************\")         print(f\"We are currently loading file {i} into a Pandas DataFrame\")         tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))         print(tmp_df.head())         dfs.append(tmp_df)  print(\" \") print(f\"We have a list of {len(dfs)} Pandas DataFrames read from csv files\")    In\u00a0[\u00a0]: Copied! <pre>x = 11\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre> x = 11  if x &gt;= 10:     print(\"x is greater than or equal to 10\") In\u00a0[\u00a0]: Copied! <pre># nothing should be printed\nx = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre> # nothing should be printed x = 9  if x &gt;= 10:     print(\"x is greater than or equal to 10\") <p>Similar to for loops, the statements inside the <code>if</code> block should be indented.</p> <p>We can use <code>else</code> blocks for statements that can be executed if the <code>if</code> statement evaluates to <code>False</code>.</p> <pre>x = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\nelse:\n    print(\"x is less than 10\")\n</pre> In\u00a0[\u00a0]: Copied! <pre>x = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\nelse:\n    print(\"x is less than 10\")\n</pre> x = 9  if x &gt;= 10:     print(\"x is greater than or equal to 10\") else:     print(\"x is less than 10\") <p>There are many uses for <code>if</code> statements in Python programs. An <code>if</code> statement was used when we read in CSV files of harvester data.</p> <pre># Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        print(f\"Loading file {i} into a Pandas DataFrame\")\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n</pre> <p>Here, the <code>if</code> statement is being used to check the file path is referring to a CSV file. Only if this is <code>True</code> does our program try to read the file into a pandas <code>DataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre>s = (1, 2, 3)\n# add code here #\n</pre> s = (1, 2, 3) # add code here # answer <pre>for i in s: \n    print(i - 3)\n</pre> How would you iterate over list of file paths? Using a for loop.  <p>Write a for loop to subtract 3 from every element in the tuple <code>s</code> and append the result to a list <code>q</code>?</p> In\u00a0[\u00a0]: Copied! <pre>q = []\ns = (1, 2, 3)\n# add code here #\n</pre> q = [] s = (1, 2, 3) # add code here # answer <pre>for i in s: \n    q.append(i - 3)\n\nprint(q)\n</pre> <p>Write a loop to subtract 3 from every element in the tuple <code>s</code> and append the result to a list <code>q</code> if the result is less than 0?</p> In\u00a0[\u00a0]: Copied! <pre>q = []\ns = (1, 2, 3)\n# add code here #\n</pre> q = [] s = (1, 2, 3) # add code here # answer <pre>for i in s:\n    if i -3 &lt; 0:\n        q.append(i - 3)\n\nprint(q)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\", \"canola-yield-df-1.csv\")\n\ndf = pd.read_csv(harvester_data_path)\n\n# check that df is a DataFrame\ntype(df)\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\", \"canola-yield-df-1.csv\")  df = pd.read_csv(harvester_data_path)  # check that df is a DataFrame type(df) In\u00a0[\u00a0]: Copied! <pre># Get the shape property of the DataFrame\ndf.shape\n</pre> # Get the shape property of the DataFrame df.shape <p>A tuple data structure is used to store the <code>shape</code> property of a <code>DataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Get the values of the DataFrame\ndf.values\n</pre> # Get the values of the DataFrame df.values <p>The <code>DataFrame</code> referenced by <code>df</code> has an <code>info()</code> method. Let's use it.</p> In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\", \"canola-yield-df-1.csv\")\n\ndf = pd.read_csv(harvester_data_path)\n\ndf.info()\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\", \"canola-yield-df-1.csv\")  df = pd.read_csv(harvester_data_path)  df.info() <p>The <code>info()</code> method has printed out a summary of the <code>DataFrame</code> for us. A <code>DataFrame</code> also has a <code>mean()</code> method to quickly compute the average of values in a column.</p> In\u00a0[\u00a0]: Copied! <pre>df.mean(numeric_only=True)\n</pre> df.mean(numeric_only=True) <p>You will notice that we passed <code>numeric_only=True</code> when using the <code>mean()</code> method. Head the the mean() documentation to see what this does.</p> In\u00a0[\u00a0]: Copied! <pre>farm_name = \"my farm\"\n# uncomment below to see string methods\n# farm_name.&lt;press_tab&gt;\n</pre> farm_name = \"my farm\" # uncomment below to see string methods # farm_name. In\u00a0[\u00a0]: Copied! <pre>a_list = [1, 2, 3]\n# uncomment below to see list methods\n# a_list.&lt;press_tab&gt;\n</pre> a_list = [1, 2, 3] # uncomment below to see list methods # a_list. In\u00a0[\u00a0]: Copied! <pre>harvester_data_path = os.path.join(os.getcwd(), \"data\", \"data_lab-1_2\", \"canola-yield-df-1.csv\")\nprint(harvester_data_path)\n</pre> harvester_data_path = os.path.join(os.getcwd(), \"data\", \"data_lab-1_2\", \"canola-yield-df-1.csv\") print(harvester_data_path) <p>Inside the <code>os.path.join()</code> function you will see that we use the <code>os.getcwd()</code> method from the os module. This returns the current working directory.</p> In\u00a0[\u00a0]: Copied! <pre>os.getcwd()\n</pre> os.getcwd() <p>You will see that we imported pandas as <code>pd</code>. This means we can refer to pandas as <code>pd</code> in our script. For example, to access the <code>read_csv()</code> function we write <code>pd.read_csv()</code> and not <code>pandas.read_csv()</code>. This is just a convenience to make the script less cluttered. The following would also be valid.</p> <pre>import pandas\ndf = pandas.read_csv(\"file.csv\")\n</pre>"},{"location":"notebooks/week-1_2/#python-programs","title":"Python programs\u00b6","text":"<p>This lab will provide an introduction to programming concepts in Python. The concepts this lab will cover are:</p> <ul> <li>data types and structures</li> <li>variables and bindings</li> <li>control flow, loops, and conditional execution</li> <li>classes and objects</li> </ul> <p>This lab will start by running a short Python program that loads some crop yield data collected by harvesters from disk and visualises this data on interactive maps and charts. We will then work through the program exploring how it uses a range of Python programming concepts to complete its task.</p>"},{"location":"notebooks/week-1_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-1_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and, if required, you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-1_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-1_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-1_2/#import-modules","title":"Import modules\u00b6","text":"<p>We'll introduce modules in detail later. But, for now, modules are Python code that we can <code>import</code> and use in our program. For example, pandas provides a <code>DataFrame</code> structure for storing tabular data and specific functions for working with tabular data. Plotly Express <code>plotly.express</code> provides functions for generating visualisations, importing Plotly Express means we don't need to write our own code to generate visualisations, we can just use existing functions which makes our lives easier.</p>"},{"location":"notebooks/week-1_2/#a-first-python-program","title":"A first Python program\u00b6","text":"<p>Python is a programming language.</p>"},{"location":"notebooks/week-1_2/#programs","title":"Programs\u00b6","text":"<ul> <li>A program is a series of statements that are executed to complete a task</li> <li>Programs complete tasks by loading, transforming, and visualising data</li> <li>Data in programs can be of different types (numeric, text, boolean)</li> <li>Data can be combined into data structures to represent complex concepts and objects</li> <li>Functions operate on data to complete tasks</li> </ul> Detailed notes on Python programs <p>As stated above, a program is a series of statements that are executed to complete a task. The ultimate goal of the program here is to generate visualisations of crop yield data recorded by harvesters. However, to achieve this the program consists of a series of sub-tasks that include reading the harvester data from CSV files on disk, inspecting the data, transforming it to a spatial data structure, and then rendering web maps.</p> <p>Our program is loading, transforming, and visualising data. This means our program needs to be able store and represent different types and structures of data. If you look in the display of the <code>DataFrame</code> there are numeric, text, and date type data in the columns. Python provides tools for representing different types of data and structures for combining data to represent more complex concepts. For example, the <code>DataFrame</code> that is displayed is a data structure well suited to storing tabular data.</p> <p>In short, a Python program stores data as objects and then uses this data in a range functions that operate on this data to complete tasks.</p> <p></p> <p>Here, a short Python program is demonstrated that:</p> <ol> <li>reads in crop yield data collected by a harvester in Western Australia's Wheatbelt</li> <li>converts the data into a spatial format</li> <li>visualises the data on a web map</li> </ol>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"In a Python program, are data types just used to store observations of scientific variables such as crop yield or temperature? <p>No. In a Python program built-in data types are used to store a range of information relevant to the execution of a program. This could include \"scientific\" data such as an array of crop yield values. However, data in a Python program could refer to file paths and directory structures, URLs to websites or data stored in the cloud, or credentials to log in to databases.</p> <p></p> <p>In this lab we will break down the Python program that generates visualisations of crop yield data recorded by harvesters into its fundamental building blocks, identify how different types of data are represented in Python programs, and demonstrate how we can do things with this data to produce useful outputs.</p>"},{"location":"notebooks/week-1_2/#computational-thinking","title":"Computational thinking\u00b6","text":"<p>A useful skill to develop when writing or analysing Python programs is computational thinking. This refers to breaking a larger task down into small sub-tasks (and possibly breaking sub-tasks into sub-tasks). Often, it is easier to reason with smaller more focused tasks than to grapple with a large complex problem as a whole.</p> <p>A good homework exercise to help build your understanding of Python concepts is to work through the program above and break it down into series of smaller tasks (e.g. find csv files, read in csv data, make data spatial)</p>"},{"location":"notebooks/week-1_2/#statements-and-comments","title":"Statements and comments\u00b6","text":"<p>A Python program consists of statements. A Python statement is a line of Python code which can be executed by the computer.</p> <p>A comment is a line of text that starts with the <code>#</code> symbol. It is not executed by the computer. We can use comments to make notes in our script to help us understand what the program is trying to achieve.</p> <p>For example, this code snippet contains two comments and one statement:</p> <pre># Inspect the yield data format\n# Display the first n rows\ndf.head()\n</pre>"},{"location":"notebooks/week-1_2/#objects","title":"Objects\u00b6","text":"<p>Everything in a Python program is an object. Built-in scalar data types are the most fundamental building blocks of a Python program. We can use these scalar data types to represent single numbers, words, and sentences. This is where we'll start unpicking this Python program.</p>"},{"location":"notebooks/week-1_2/#data-types","title":"Data types\u00b6","text":"<p>Python programs perform operations on data to complete tasks. This can be scientific data such as crop yield or temperature measurements or it can be other forms of data necessary for the program to execute such as a file path to where data is stored on disk.</p>"},{"location":"notebooks/week-1_2/#built-in-data-types","title":"Built-in data types\u00b6","text":"<p>Python comes with built-in data types that can be used to store different values. These are sometimes called scalar types as they store single values.</p> <ul> <li><code>float</code> - storing floating point numeric values.</li> <li><code>int</code> - storing integer numeric values.</li> <li><code>str</code> - storing text data as strings.</li> <li><code>bool</code> - storing <code>True</code> or <code>False</code> as boolean values.</li> <li><code>None</code> - storing null values.</li> <li><code>bytes</code> - storing raw binary data.</li> </ul>"},{"location":"notebooks/week-1_2/#numeric-data-types","title":"Numeric data types\u00b6","text":"<p><code>int</code> and <code>float</code> are numeric data types in Python and are used to store integer and floating point values, respectively.</p> <p>The canola yield data that we have imported is of numeric type with units of tonnes/ha. Let's represent a crop yield measurement of 0.227 tonnes/ha as data in our program and inspect its type.</p>"},{"location":"notebooks/week-1_2/#string-data-types","title":"String data types\u00b6","text":"<p>We also need to represent text data in our programs. In the crop yield dataset the sample id is text. In Python, text data is stored as a <code>str</code> type (or string type).</p> <p>Text data is stored as string types by enclosing the characters in double<code>\"</code> or single <code>'</code> quotes.</p>"},{"location":"notebooks/week-1_2/#boolean-data-types","title":"Boolean data types\u00b6","text":"<p>Boolean (<code>bool</code>) data types are used for storing <code>True</code> or <code>False</code> values. In Python, <code>True</code> or <code>False</code> are Boolean values and not string data types.</p> <p>Boolean data types are used to store the result of testing a condition that evaluates to true or false. For example, greater than and less than operations evaluate to true or false. We could test if our crop yield value of 0.227 is greater than 0.2 (it is and this expression should evaluate to true).</p>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"If you execute <code>x = 3.4</code>, what data type will <code>x</code> be? <code>float</code> Which data type would be most suited to record the number of apples harvested from an orchard? <code>int</code> -  we should not be able to harvest fractions of apples.  Which data type would we use to record a farm name? <code>str</code> - assuming the farm name is text data.  <code>y = 4 + 5</code> and <code>z = y &gt; 10</code> - what value will <code>z</code> be? <code>False</code> - <code>y</code> evaluates to 9 which is less than 10. Therefore, <code>z</code> will be <code>False</code> and of <code>bool</code> type."},{"location":"notebooks/week-1_2/#data-structures","title":"Data structures\u00b6","text":"<p>Python provides a series of built-in data structures that can be used to group together and store related data.</p> <p>Data structures can be used to model a range of practical and real-world problems or phenomenon by combining simple data types. For example, we could use a collection of numeric data to represent a time-series of precipitation, string data to represent the weather station name, and combine string and numeric data together in a data structure to create a bespoke weather station data structure.</p> <ul> <li><code>list</code> - a variable length collection of objects that can be modified</li> <li><code>tuple</code> - a fixed length collection of objects that cannot be modified</li> <li><code>set</code> - a collection of unique objects</li> <li><code>dict</code> - a collection of objects stored as key:value pairs</li> </ul>"},{"location":"notebooks/week-1_2/#lists","title":"Lists\u00b6","text":"<p>Lists in Python:</p> <ul> <li>can be modified during a program's execution</li> <li>can store duplicate values</li> <li>are created by placing elements in square brackets <code>[]</code></li> <li>elements of a list are ordered</li> <li>elements of a list can be of different data types</li> </ul> <p>We could store the sample names in our crop yield data set as a list:</p>"},{"location":"notebooks/week-1_2/#indexing-in-python","title":"Indexing in Python\u00b6","text":"<ul> <li>An index is an element's location within a data structure</li> <li>Often, in Python, elements are accessed by their index position using the square brackets operator and passing an index in as an argument (e.g. the first element of <code>x</code> is <code>x[0]</code>)</li> <li>In Python, indexing starts at 0 - this means the index for the first element in a data structure is 0</li> <li>It is important to become familiar with the concept of indexing - you will use it often as you handle data in Python</li> </ul> <p>Let's demonstrate this by accessing the elements of the <code>sample_id_list</code> we created above.</p>"},{"location":"notebooks/week-1_2/#tuples","title":"Tuples\u00b6","text":"<p>Tuple in Python:</p> <ul> <li>elements are unchangeable (immutable)</li> <li>tuple elements are ordered</li> <li>store a fixed-length number of elements</li> <li>created by placing Python objects inside parentheses <code>()</code></li> </ul>"},{"location":"notebooks/week-1_2/#sets","title":"Sets\u00b6","text":"<p>Set in Python:</p> <ul> <li>unordered collection objects</li> <li>set elements cannot be duplicated</li> <li>sets are immutable</li> <li>sets are created by placing elements inside curly brackets <code>{}</code></li> </ul> <p>Let's create a set and demonstrate that it cannot store duplicate values.</p>"},{"location":"notebooks/week-1_2/#dictionary-objects","title":"Dictionary objects\u00b6","text":"<p>Dictionary objects, or <code>dict</code> objects:</p> <ul> <li>store data as a collection of key:value pairs</li> <li>Dictionary objects can be changed and modified</li> <li>Dictionary object elements are ordered</li> <li>We can access dictionary elements by their key</li> <li>Dictionary objects are created by placing key:value pairs inside curly brackets <code>{}</code></li> <li>Keys of a dictionary object cannot be duplicated</li> <li>Elements (values) of dictionary objects can be of a different type</li> <li>Values can be of different lengths</li> </ul>"},{"location":"notebooks/week-1_2/#using-dictionary-objects-to-represent-tabular-data","title":"Using Dictionary objects to represent tabular data\u00b6","text":"<p>Tabular data has data values stored in rows and columns. Generally, a column corresponds to a particular variable or type of data and rows correspond to observed or measured values. We can use dictionary objects to represent tabular data in Python.</p> <p>For example, we can use the key:value pair pattern to represent a column header and column values.</p> <pre>\"Elevation(m)\": [213, 222, 214, 254]\n</pre> <p>Here, we've used a string object to represent the column header (the key) and a list object to represent column values (the value). Combining one or more key:value pairs in a dictionary object is a way of representing tabular data in Python.</p> <pre>{\n\"Elevation(m)\": [213, 222, 214, 254],\n\"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]\n}\n</pre> <p>Let's create this dictionary object.</p>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<code>x = [1, 2, 3, 4]</code> - what data structure is <code>x</code>? List  <code>q = {1, 2, 3, 4}</code> - what data structure is <code>q</code>? Set  Which data structure organises its elements using key:value pairs? Dictionary objects  Are lists immutable data structures? No, we can modify the values of a list and change its size (number of elements) during the program's execution."},{"location":"notebooks/week-1_2/#variables","title":"Variables\u00b6","text":"<p>In the program to read in and visualise crop yield data, you will have noticed this syntax pattern: <code>variable_name = data</code>.</p> <pre>basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\n</pre> <p><code>=</code> is the assignment operator (not equals) which is assigning (or binding) the <code>variable_name</code> to the <code>data</code> object.</p> <p>The statement <code>basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"</code> is assigning the data <code>\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"</code> to the variable name <code>basemap</code>. The data is string which is storing the URL for a satellite imagery basemap.</p> <p>Let's see what the data looks like.</p>"},{"location":"notebooks/week-1_2/#variable-names","title":"Variable names\u00b6","text":"<p>There are rules that need to be followed when creating variable names:</p> <ul> <li>variable names can only be text letters (A-Z, a-z), numbers (0-9), or underscores (_)</li> <li>variable names cannot begin with a number</li> <li>variable names cannot include whitespace</li> <li>variable names cannot be reserved keywords such True, False, if</li> </ul> <p>There are also some conventions that can be followed to help write programs that are easy to follow:</p> <ul> <li>use a consistent style for writing variable names - snake_case where words are separated by underscores is recommended by the PEP8 style guide for Python code</li> <li>use a descriptive variable name so it helps you understand what kind of data it is referring to</li> </ul>"},{"location":"notebooks/week-1_2/#control-flow","title":"Control flow\u00b6","text":"<p>So far we have demonstrated how we can represent information and concepts in our program as different data types and using different data structures. However, we are not using our program as a data storage device. We are using it to complete tasks with this data.</p> <p>Control flow refers to the sequence of steps that our program takes from start to finish when it is run.</p> <p>The most simple form of control flow is executing statements from top to bottom as they are written in your script.</p>"},{"location":"notebooks/week-1_2/#loops","title":"Loops\u00b6","text":"<p>Loops use a <code>for</code> statement that allows us to iterate over items in a sequence. For example, if <code>x</code> is a list <code>[1, 2, 3]</code> then:</p> <pre>for i in x:\n    print(i)\n</pre> <p>will print the values of 1, 2, and 3 in turn for each iteration of the loop, <code>i</code>, takes on the value of the corresponding element in <code>x</code>.</p> <p>For loops are useful if we want to iterate over (repeat) a block of statements using different values from a sequence of items in turn. If we wanted to add 10 to each element of <code>x</code> we could write a for loop as:</p> <pre>for i in x:\n    print(i + 10)\n</pre> <p>Let's demonstrate this.</p>"},{"location":"notebooks/week-1_2/#conditional-execution","title":"Conditional execution\u00b6","text":"<p>Conditional execution is a form of control flow that allows for branches in the sequence that statements are executed. A boolean condition is tested, and, <code>if</code> it evaluates to <code>True</code> then one set of statements are executed and <code>if</code> it evaluates to <code>False</code> a different set of statements are executed.</p> <p>Conditions are tested within <code>if</code> blocks:</p> <pre>if True:\n    these statements are executed\nelse:\n    these statements are executed\n</pre> <p>A simple example:</p> <pre>x = 11\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre> <p>In this instance, the statements inside the <code>if</code> block will be executed as <code>x</code> is 11 and so greater than 10.</p> <p>However, if we change the value of x to 9 the statements inside the <code>if</code> block will not be executed.</p> <pre>x = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Write a for loop to subtract 3 from every element in a tuple <code>s</code> and print the result?</p>"},{"location":"notebooks/week-1_2/#classes-and-objects","title":"Classes and Objects\u00b6","text":"<p>We have mentioned a few times that Python programs comprise objects. We will now define what an object is and how the concept of an object relates to the data types and structures we have looked at above.</p>"},{"location":"notebooks/week-1_2/#objects","title":"Objects\u00b6","text":"<p>An object is a more general concept in programming. An object in a computer program has:</p> <ul> <li>properties or attributes (data)</li> <li>behaviour (methods or functions that do things with object data).</li> </ul> <p>Objects provide a way to add structure to a program by combining related data and behaviours.</p> <p>Alongside the built-in data types and structures, we can create objects that are customised and useful for specific tasks or representing specific kinds of data or real-world problems. Often, custom data types will combine the built-in data types and structures, or other custom objects, to create objects that represent more complex concepts.</p>"},{"location":"notebooks/week-1_2/#classes","title":"Classes\u00b6","text":"<p>A class is a definition of an object. The class defines the object's data and methods. An object is an instance of a class in our programs.</p> <p>An object's data (sometimes called its properties or attributes) can be built-in Python data types or structures or custom classes. An object's methods are functions that can be used to transform data or implement other behaviour that is relevant to the object. For example, an object representing a table is more complex than a list.</p>"},{"location":"notebooks/week-1_2/#pandas-dataframes","title":"pandas <code>DataFrame</code>s\u00b6","text":"<p>For most use cases there are existing classes that have already been developed and can be used in our programs. Keeping with the idea of working with tabular data, pandas provides a <code>DataFrame</code> class to work with tabular data. In our Python program to visualise the crop yield data we read the tabular data stored in CSV files into pandas <code>DataFrames</code>.</p> <p>A pandas <code>DataFrame</code> is a far more comprehensive class for handling tabular data. Compared to our small example of using a dictionary object to store tabular data, a pandas <code>DataFrame</code> implements well defined data structures for tabular data and provides a suite of useful methods (functions) for analysing and processing tabular data.</p>"},{"location":"notebooks/week-1_2/#dataframe-properties","title":"<code>DataFrame</code> Properties\u00b6","text":"<p>The class <code>DataFrame</code> is a data structure for \"Two-dimensional, size-mutable, potentially heterogeneous tabular data\". It has the following properties:</p> <ul> <li>data - the tabular data stored in the data frame</li> <li>index - the row index (row labels)</li> <li>columns - the column headers</li> <li>dtype - the data types of each column</li> </ul> <p></p> <p>A pandas <code>DataFrame</code> stores its values in rows and columns. There is a list of column headers (which should provide a helpful description of what kind of data is in each column) and a row index.</p> <p>The values in columns are pandas <code>Series</code> objects. A <code>Series</code> is an array of the same type data.</p> <p>The pandas <code>DataFrame</code> object stores a range of properties that are useful for working with tabular data. You can look up the documentation for a <code>DataFrame</code> here. The properties listed do not have parentheses after their name <code>()</code>. Let's look at the properties under attributes and underlying data. We can see the <code>DataFrame</code> has a <code>values</code> property which is the actual data values in the table and a <code>shape</code> property which tells us the shape of the table (number of rows and columns).</p> <p></p> <p>Most of the time we will explore classes using their documentation websites. Let's read a CSV file into a pandas <code>DataFrame</code> and explore its properties.</p>"},{"location":"notebooks/week-1_2/#dataframe-methods","title":"<code>DataFrame</code> methods\u00b6","text":"<p>However, a <code>DataFrame</code> is more than just an object for storing data. It is also where we can find a wide range of functions for working with tabular data - the class methods. Go back to the documentation for a <code>DataFrame</code> here. The <code>DataFrame</code> methods are indicated by the parentheses after their name <code>()</code>.</p> <p>There are methods to help us inspect the <code>DataFrame</code> values. The <code>info()</code> method prints a concise summary table of the <code>DataFrame</code>, there a range of methods for performing mathemtatical computations / descriptive stats, and methods for reading data on disk into <code>DataFrame</code>s.</p> <p>Let's explore how we can use some of these methods to work with tabular data in <code>DataFrames</code>.</p>"},{"location":"notebooks/week-1_2/#methods-and-functions","title":"Methods and functions\u00b6","text":"<p>Functions are executed to perform a task using data. You can spot functions by <code>()</code> appearing after the function name. Functions can take input data - this is the function's arguments -  and use these inputs to complete it's task.</p> <p>Functions can return data back to the Python program, display data on your screen, or save data to disk.</p> <p>For example, the <code>print()</code> function takes data as an argument and prints it on your display as text. For example, <code>print(\"hello\")</code> would print <code>hello</code> on your screen. The <code>df = pd.read_csv(\"a_csv_file.csv\")</code> is a function within the pandas package the takes in a path to a CSV file as an argument and reads the data from that file into a <code>DataFrame</code> referenced by <code>df</code>.</p> <p>Methods are functions that belong to a class. For example, the <code>mean()</code> method of the pandas <code>DataFrame</code> can be called by accessing it via a <code>DataFrame()</code> object (e.g <code>df.mean()</code>). Methods often provide functionality related to the \"topic\" of the class they're associated with. For example, <code>DataFrame</code> methods provide functions related to working tabular data in <code>DataFrame</code> objects; dictionary objects have specialist methods geared for retrieving and setting data elements in dictionaries; string objects have methods for manipulating text (e.g. the <code>lower()</code> of a string object will convert all text characters to lower case).</p> <p>The general pattern for executing a function / method is:</p> <ol> <li>The function is called and any data are passed in as arguments inside <code>()</code></li> <li>The function performs operations on the data passed in</li> <li>The function returns the result of operating on the data</li> </ol> <p>As everything in Python is an object, lists and string objects have methods. You can see the methods associated with an object by typing the object name, dot operator, and pressing tab.</p> <p>For example, if <code>farm_name = \"my farm\"</code>, <code>farm_name.&lt;press_tab&gt;</code> will print a list of string object methods.</p> <p>Try this below and explore string and list methods, and how they can be used to transform your data.</p>"},{"location":"notebooks/week-1_2/#instances","title":"Instances\u00b6","text":"<p>Above we said that a class is a definition of an object. The class never has any data stored in it or executes any of its methods - it's just a template for a particular type of object. Objects are instances of a class.</p> <p>When we executed the statement <code>df = pd.read_csv(\"csv_file.csv\")</code> we are creating an instance object of the <code>DataFrame</code> class which is stored in the computer's memory.</p>"},{"location":"notebooks/week-1_2/#dot-notation","title":"Dot notation\u00b6","text":"<p>We use the <code>.</code> operator to access properties and methods of class. For example to access the <code>shape</code> property of a <code>DataFrame</code> we write <code>df.shape</code> and to access the <code>info()</code> method we write <code>df.info()</code>.</p>"},{"location":"notebooks/week-1_2/#modules","title":"Modules\u00b6","text":"<p>There is one final concept to cover that is used in our program to read and visualise harvester yield data. The very first lines of the program.</p> <pre># Import modules\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n</pre> <p>Here, we are importing modules into our script.  A module is a Python file (a file with a <code>.py</code> ending) that contains class definitions and functions. When we import a module into our program we can use the classes defined in the module.</p> <p>For example, we have used the method <code>os.path.join()</code> from the os module in our program. This function takes string data describing parts of a file path and joins them together to create path to a directory or file.</p>"},{"location":"notebooks/week-1_practice/","title":"Week 1 practice","text":"In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\na = 1.34\nprint(a)\n</code></pre> <p>Can you add 8 and 2 and assign the result to the variable name <code>z</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nz = 8 + 2\nprint(z)\n</code></pre> <p></p> <p>Can you check what data type <code>4</code> is?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nprint(type(4))\n</code></pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nprint(type(\"hello\"))\n</code></pre> <p>We can represent multiline strings by enclosing the text values inside three <code>\"\"\"</code> or <code>'''</code>.</p> <p>For example:</p> <pre><code>{python}\na_longer_string = '''\nThis is a multiline\nstring object.\n'''\n</code></pre> <p>String objects have a range of methods for transforming text data. You can see a list of the string methods here.</p> <p>Can you use the <code>upper()</code> method to convert this string's text characters to upper case: <code>\"Hello!\"</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nprint(\"Hello!\".upper())\n</code></pre> <p>Can you use the <code>find()</code> method to find the index location of <code>\"object\"</code> in this multiline string?</p> <pre><code>{python}\na_longer_string = '''\nThis is a multiline\nstring object.\n'''\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\na_longer_string = '''\nThis is a multiline\nstring object.\n'''\nprint(a_longer_string.find(\"object\"))\n</code></pre> <p>The <code>endswith()</code> method is a useful tool for checking if a string ends with a certain sequence of text values. We use this often to check if a filename ends with a certain format.</p> <p>Can you use the <code>endswith()</code> method to check if the string <code>\"an_image.tif\"</code> ends with the <code>\".tif\"</code> extension?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nprint(\"an_image.tif\".endswith(\".tif\"))\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\na_list = [1, 2, 3]\nprint(len(a_list))\n</code></pre> <p></p> <p>Can you select the first element of this list: <code>a_list = [1, 2, 3]</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\na_list = [1, 2, 3]\nprint(a_list[0])\n</code></pre> <p></p> <p>Can you slice out the last two elements of this list: <code>a_list = [1, 2, 3]</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer Remember, the last index location in the slice is exclusive!  <pre><code>{python}\na_list = [1, 2, 3]\nprint(a_list[1:3]) \n</code></pre> <p></p> <p>Can you add the string <code>\"modified\"</code> to the list: <code>text_list = [\"lists\", \"can\", \"be\"]</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer Remember, lists are mutable data structures!  <pre><code>{python}\ntext_list = [\"lists\", \"can\", \"be\"]\nprint(text_list.append(\"modified\"))\n</code></pre> <p></p> <p>Can you replace the first animal in this list with the string <code>\"cow\"</code>: <code>animal_list = [\"sheep\", \"horse\", \"pig\"]</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer Remember, lists are mutable data structures!  <pre><code>{python}\nanimal_list = [\"sheep\", \"horse\", \"pig\"]\nanimal_list[0] = \"cow\"\nprint(animal_list) \n</code></pre> <p></p> <p>Along with using index locations and slices, we can also use list methods to modify list elements. The <code>insert()</code> method takes an index location and value as arguments, and inserts the value into the location specified by the index.</p> <p>For example,</p> <pre><code>{python}\na_list = [1, 2, 3]\na_list.insert(0, 4)\nprint(a_list)\n</code></pre> <p>Should return <code>[4, 1, 2, 3]</code>.</p> <p>Can you add the number <code>5</code> to the second index location in the list <code>a_list = [4, 1, 2, 3]</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer Remember, lists are mutable data structures!  <pre><code>{python}\na_list = [4, 1, 2, 3]\na_list.insert(1, 5)\nprint(a_list)\n</code></pre> <p></p> <p>Note, the use of the <code>.</code> operator to access an object's methods.</p> <p>The <code>pop()</code> method of a list removes a value at the specified index location, which is passed in as an argument.</p> <p>Can you use the pop() method to remove the third element of <code>a_list = [1, 2, 3, 4, 5]</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\na_list = [1, 2, 3, 4, 5]\na_list.pop(2)\nprint(a_list)\n</code></pre> <p>How to you create a tuple with one element?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <p>You must include a comma after the data value. Otherwise it would be interpreted as a integer if a single digit, for example.</p> <pre><code>{python}\na_tuple = (1,)\n</code></pre> <p></p> <p>Can you use the <code>len()</code> function to find out the length (number of elements) in this tuple: <code>a_tuple = (0, 1, 2, 3)</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\na_tuple = (0, 1, 2, 3)\nprint(len(a_tuple))\n</code></pre> <p></p> <p>Can you get the value of the second element in this tuple: <code>a_tuple = (0, 1, 2, 3)</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code ##\n</pre> ## add code ## answer <pre><code>{python}\na_tuple = (0, 1, 2, 3)\nprint(a_tuple[1])\n</code></pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nweather_station_data = {\n\"name\": \"York\",\n\"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\", \"2020-01-04\"],\n\"temperature\": [34, 33, 38, 27],\n\"precipitation\": [0, 0, 0, 0.1]\n}\n\nprint(weather_station_data[\"temperature\"])\n</code></pre> <p>Can you add this list of wind speeds (<code>[23, 10, 5, 15]</code>) to the <code>weather_station_data</code> dictionary under the <code>\"wind_speed\"</code> key?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nweather_station_data = {\n\"name\": \"York\",\n\"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\", \"2020-01-04\"],\n\"temperature\": [34, 33, 38, 27],\n\"precipitation\": [0, 0, 0, 0.1]\n}\nweather_station_data[\"wind_speed\"]=[23, 10, 5, 15]\nprint(weather_station_data)\n</code></pre> <p>Can you write a for loop that iterates over the tuple <code>(0, 1, 2, 3, 4)</code> and adds two to each value in-turn and prints the result?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nfor i in (0, 1, 2, 3, 4):\n    print(i + 2)\n</code></pre> <p></p> <p>Can you create an empty list using the square brackets (<code>my_list = []</code>), then iterate over the tuple <code>(0, 1, 2, 3, 4)</code>, add two to each value in-turn, and append it to <code>my_list</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre><code>{python}\nmy_list = []\nfor i in (0, 1, 2, 3, 4):\n    my_list.append(i)\n\nprint(my_list)\n</code></pre> <p></p>"},{"location":"notebooks/week-1_practice/#week-1-practice-exercises","title":"Week 1 - Practice Exercises\u00b6","text":"<p>This notebook contains a series of practice exercises related to the content in week 1's lab. It is designed to help you become more familiar with the Python programming concepts introduced in week 1 and practice writing simple Python statements to achieve tasks.</p>"},{"location":"notebooks/week-1_practice/#data-types","title":"Data types\u00b6","text":""},{"location":"notebooks/week-1_practice/#numeric-data-types","title":"Numeric data types\u00b6","text":"<p>1.34 is assigned to the variable <code>a</code>, can you print the data referenced by variable <code>a</code> on your display?</p>"},{"location":"notebooks/week-1_practice/#string-data-types","title":"String data types\u00b6","text":"<p>Can you check what data type <code>\"hello\"</code> is?</p>"},{"location":"notebooks/week-1_practice/#data-structures","title":"Data Structures\u00b6","text":"<p>When pracitcing using data structures to store collections of values, think about how you can use data structures to model a range of data formats or real-world datasets (e.g. spreadsheets, data from weather stations, image pixels).</p>"},{"location":"notebooks/week-1_practice/#lists","title":"Lists\u00b6","text":"<p>Can you use the <code>len()</code> function to find out the length (number of elements) in this list: <code>a_list = [1, 2, 3]</code>?</p>"},{"location":"notebooks/week-1_practice/#tuples","title":"Tuples\u00b6","text":""},{"location":"notebooks/week-1_practice/#dictionaries","title":"Dictionaries\u00b6","text":"<p>Can you access the list of temperature values from this dict?</p> <pre><code>{python}\nweather_station_data = {\n\"name\": \"York\",\n\"date\": [\"2020-01-01\", \"2020-01-02\", \"2020-01-03\", \"2020-01-04\"],\n\"temperature\": [34, 33, 38, 27],\n\"precipitation\": [0, 0, 0, 0.1]\n}\n</code></pre>"},{"location":"notebooks/week-1_practice/#control-flow","title":"Control flow\u00b6","text":""},{"location":"notebooks/week-1_practice/#for-loops","title":"For loops\u00b6","text":""},{"location":"notebooks/week-2_1/","title":"Week 2 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-2_1\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_1\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-2_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-2_1\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-2_1\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-2_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre># Import modules\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport numpy as np\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> # Import modules import os import pandas as pd import geopandas as gpd import plotly.express as px import numpy as np import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># Load the crop yield data\ncrop_yield_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\")\n\n# Read the canola and wheat crop yield data\ncanola_fpath = os.path.join(crop_yield_data_path, \"bf66-canola-yield-max-vi_sampled.geojson\")\ncanola_gdf = gpd.read_file(canola_fpath)\nwheat_fpath = os.path.join(crop_yield_data_path, \"bf66-wheat-yield-max-vi_sampled.geojson\")\nwheat_gdf = gpd.read_file(wheat_fpath)\n\n# Combine (stack) the geojson files into one GeoDataFrame\ngdf = pd.concat([canola_gdf, wheat_gdf], axis=0)\ngdf.head()\n</pre> # Load the crop yield data crop_yield_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\")  # Read the canola and wheat crop yield data canola_fpath = os.path.join(crop_yield_data_path, \"bf66-canola-yield-max-vi_sampled.geojson\") canola_gdf = gpd.read_file(canola_fpath) wheat_fpath = os.path.join(crop_yield_data_path, \"bf66-wheat-yield-max-vi_sampled.geojson\") wheat_gdf = gpd.read_file(wheat_fpath)  # Combine (stack) the geojson files into one GeoDataFrame gdf = pd.concat([canola_gdf, wheat_gdf], axis=0) gdf.head() <p>Displaying the <code>head</code> of the <code>GeoDataFrame</code> <code>gdf</code> demonstrates that we are working with tabular data. There is a <code>geometry</code> column,  which stores the geographic location that each row in the table's attributes correspond to. Other columns of note are:</p> <ul> <li><code>DryYield</code> - crop yield values for each location (tonnes / ha)</li> <li><code>Variety</code> - 43Y23 RR indicates canola  ninja indicates wheat</li> <li><code>gndvi</code> - green normalised difference vegetation index, a satellite derived measure of greenness</li> <li><code>ndyi</code> - normalised difference yellowness index, a satellite derived measure of yellowness</li> </ul> In\u00a0[\u00a0]: Copied! <pre>fig = px.histogram(\n    data_frame=gdf, \n    x=\"DryYield\", \n    color=\"Variety\", \n    marginal=\"box\", \n    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\nfig.show()\n</pre> fig = px.histogram(     data_frame=gdf,      x=\"DryYield\",      color=\"Variety\",      marginal=\"box\",      hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"]) fig.show() <p>There are more options that you can use to configure a histogram here.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>fig = px.histogram(\n    data_frame=gdf, \n    x=\"DryYield\", \n    color=\"Variety\", \n    marginal=\"box\", \n    range_x=[0, 7],\n    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\nfig.show()\n</pre> <p>Let's have a go at generating a scatter plot to consolidate our understanding of how to map variables in our data to elements of a graphic. The documentation for scatter plots is here and you should notice similarities in how we set up a scatter plot to a histogram.</p> <p>Let's use a scatter plot to see if there is a relationship beetween crop yield and elevation. We are plotting two variables here so we need to use the <code>y</code> parameter to specify what column in our <code>GeoDataFrame</code> will be mapped onto the y-axis.</p> <p>We can use the <code>marginal_x</code> and <code>marginal_y</code> parameters to attach plots to the x- and y-axes that show the distributions of variables mapped to each axis.</p> <p>Finally, we're going to use the <code>opacity</code> argument here to make the point elements on the figure semi-transparent; this will help reveal more information about the density of data values.</p> <p>Both canola and wheat crop yield data is displayed. To see the relationship between one crop type's yield and elevation, click on the variety in the legend.</p> In\u00a0[\u00a0]: Copied! <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"DryYield\", \n    y=\"Elevation\", \n    color=\"Variety\", \n    opacity=0.25, \n    marginal_x=\"box\", \n    marginal_y=\"violin\")\nfig.show()\n</pre> fig = px.scatter(     data_frame=gdf,      x=\"DryYield\",      y=\"Elevation\",      color=\"Variety\",      opacity=0.25,      marginal_x=\"box\",      marginal_y=\"violin\") fig.show() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"DryYield\", \n    y=\"Elevation\", \n    color=\"Variety\", \n    range_x=[0,10],\n    opacity=0.25,\n    marginal_y=\"violin\")\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"gndvi\", \n    y=\"DryYield\", \n    color=\"Variety\", \n    opacity=0.05, \n    range_y=[0.1, 6], \n    range_x=[0.3, 0.9], \n    marginal_x=\"box\", \n    marginal_y=\"box\", \n    trendline=\"ols\"\n)\nfig.show()\n</pre> fig = px.scatter(     data_frame=gdf,      x=\"gndvi\",      y=\"DryYield\",      color=\"Variety\",      opacity=0.05,      range_y=[0.1, 6],      range_x=[0.3, 0.9],      marginal_x=\"box\",      marginal_y=\"box\",      trendline=\"ols\" ) fig.show() In\u00a0[\u00a0]: Copied! <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"ndyi\", \n    y=\"DryYield\", \n    facet_col=\"Variety\", \n    opacity=0.05, \n    range_y=[0.1, 5], \n    range_x=[0.1, 0.6], \n    trendline=\"ols\"\n)\nfig.show()\n</pre> fig = px.scatter(     data_frame=gdf,      x=\"ndyi\",      y=\"DryYield\",      facet_col=\"Variety\",      opacity=0.05,      range_y=[0.1, 5],      range_x=[0.1, 0.6],      trendline=\"ols\" ) fig.show() In\u00a0[\u00a0]: Copied! <pre>fig = px.colors.sequential.swatches_continuous()\nfig.show()\n</pre> fig = px.colors.sequential.swatches_continuous() fig.show() <p>Let's use a sequential colour palette to visualise monthly precipitation over the field since 1981. The precipitation data is obtained from the TerraClimate: Monthly Climate and Climatic Water Balance for Global Terrestrial Surfaces dataset.</p> <p>Use the pandas <code>read_csv()</code> function to read in the precipitation data. Inside the <code>CSV</code> file each row represents a month-year combination and stores a monthly precipitation total in mm.</p> In\u00a0[\u00a0]: Copied! <pre># visualise monthly precipitation using a diverging palette\nclimate_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\")\nprecip_df = pd.read_csv(os.path.join(climate_data_path, \"bf66-terra-precip-monthly.csv\"))\nprecip_df[\"month\"] = precip_df[\"month\"].astype(str)\nprecip_df[\"year\"] = precip_df[\"year\"].astype(str)\nprecip_df.head()\n</pre> # visualise monthly precipitation using a diverging palette climate_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\") precip_df = pd.read_csv(os.path.join(climate_data_path, \"bf66-terra-precip-monthly.csv\")) precip_df[\"month\"] = precip_df[\"month\"].astype(str) precip_df[\"year\"] = precip_df[\"year\"].astype(str) precip_df.head() <p>We can create a heatmap to visualise monthly precipitation across time and using a colour palette where darker blue shades indicate wetter months. Note how we pass in the colour palette <code>Blues</code> as an argument to the <code>color_continuous_scale</code> parameter.</p> In\u00a0[\u00a0]: Copied! <pre>fig = px.density_heatmap(\n    precip_df,\n    x=\"year\", \n    y=\"month\", \n    z=\"pr\", \n    histfunc=\"sum\",\n    nbinsy=12,\n    color_continuous_scale=\"Blues\",\n    range_color=(0, 75),\n)\nfig.show()\n</pre> fig = px.density_heatmap(     precip_df,     x=\"year\",      y=\"month\",      z=\"pr\",      histfunc=\"sum\",     nbinsy=12,     color_continuous_scale=\"Blues\",     range_color=(0, 75), ) fig.show() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>fig = px.density_heatmap(\n    precip_df,\n    x=\"year\", \n    y=\"month\", \n    z=\"pr\", \n    histfunc=\"sum\",\n    nbinsy=12,\n    color_continuous_scale=\"YlGnBu\",\n    range_color=(0, 75),\n)\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"gndvi\", \n    y=\"DryYield\", \n    facet_col=\"Variety\", \n    opacity=0.25, \n    range_y=[0.1, 5], \n    range_x=[0.4, 0.9], \n    color=\"gndvi\",\n    color_continuous_scale=\"Greens\",\n    range_color=(0.4, 0.8),\n)\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig = px.colors.diverging.swatches_continuous()\nfig.show()\n</pre> fig = px.colors.diverging.swatches_continuous() fig.show() <p>We can use a diverging colour palette to visualise the same precipitation data. Monthly precipitation values are converted to z-scores, which represent deviations in monthly precipitation away from the mean. A z-score of zero represents average rainfall and can be used as the mid-point for a diverging colour palette. Here, we can use red-to-blue colour palette, with drier months represented by red shades.</p> In\u00a0[\u00a0]: Copied! <pre># compute average rainfall and standard deviation of rainfall to compute z scores\n# use z score as 0 for the mid-point of a diverging colour palette\navg_pr = precip_df[\"pr\"].mean()\nstd_pr = precip_df[\"pr\"].std()\nprecip_df.loc[:, \"z_score\"] = (precip_df.loc[:, \"pr\"] - avg_pr) / std_pr\n\nfig = px.density_heatmap(\n    precip_df,\n    x=\"year\", \n    y=\"month\", \n    z=\"z_score\", \n    histfunc=\"sum\",\n    nbinsy=12,\n    color_continuous_scale=\"RdBu\",\n    color_continuous_midpoint=0,\n)\nfig.show()\n</pre> # compute average rainfall and standard deviation of rainfall to compute z scores # use z score as 0 for the mid-point of a diverging colour palette avg_pr = precip_df[\"pr\"].mean() std_pr = precip_df[\"pr\"].std() precip_df.loc[:, \"z_score\"] = (precip_df.loc[:, \"pr\"] - avg_pr) / std_pr  fig = px.density_heatmap(     precip_df,     x=\"year\",      y=\"month\",      z=\"z_score\",      histfunc=\"sum\",     nbinsy=12,     color_continuous_scale=\"RdBu\",     color_continuous_midpoint=0, ) fig.show()"},{"location":"notebooks/week-2_1/#data-visualisation","title":"Data visualisation\u00b6","text":"<p>This lab will generate interactive visualisations of crop yield data for wheat and canola collected by a harvester from a field in Western Australia. This lab will provide an introduction to:</p> <ul> <li>interactive visualisations using Plotly Express</li> <li>using figures to represent and explore different features of a dataset</li> <li>using colour to visualise patterns in a dataset</li> </ul>"},{"location":"notebooks/week-2_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-2_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-2_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-2_1/#what-is-a-figure","title":"What is a figure?\u00b6","text":"<p>Data visualisation is the process of relating data values to elements of a figure on a computer display.</p> <p>The Grammar of Graphics is an underlying model that describes the mapping of data values to the visual elements of a figure. It provides a consistent framework for guiding us in how to take our data values and convert them into a figure that effectively represents the data and conveys the messages and insights we seek to communicate.</p> <p>In the Grammar of Graphics a plot comprises data and a mapping. The mapping (not cartographic here) is a formal description of how data values map onto elements of a figure. The elements of a figure are termed aesthetics and consist of:</p> <ul> <li>layers - geometric elements that represent data values such as points (e.g. for scatter plots), lines (e.g. for lines of best fit), and polyons (e.g. for histograms or bar plots).</li> <li>scales - relate data values to visual display properties such as colour (e.g. a blue to red colour palette for temperature), size (e.g. larger points for larger numbers), position (e.g. location on axes), or shapes (e.g. using triangles for group A and circles for group B). Scales are used to draw axes and legends for figures.</li> <li>coords - coordinate systems are used to map data values onto locations on the figure. On most 2D figures the x- and y-axes describe the coordinate space and on maps latitude and longitude describe the coordinate space (or you can use different coordinate reference systems).</li> <li>theme - the background styling of the figure such as fonts for labels and background colours.</li> </ul> <p></p> <p>Reading the A Layered Grammar of Graphics paper by Hadley Wickham provides a detailed description of the core concepts for designing high-quality data visualisations.</p>"},{"location":"notebooks/week-2_1/#interactive-visualisations","title":"Interactive visualisations\u00b6","text":"<p>Interactive visualisations are important tools for exploring complex and multidimensional data. They enable users to quickly develop an understanding of a dataset's structure and patterns by enabling them to interactively generate different views of the dataset.</p> <p>Generally, interactive visualisations are controlled by user input from mouse events (click, drag, hover), and, in response to mouse events, change what data and information is rendered on the computer display.</p> <p>Interactive visualisations are important tools for both exploratory analysis and for communicating the results of analysis to a wider audience. For exploratory analysis the quick feedback provided by interactive visualisations allows analysts to quickly build up an understanding of the datasets they are working with, spot noise or missing data, refine and develop hypotheses and research questions, and select suitable analytical and statistical tools for further work. Interactive visualisations are useful for communication as they enable active engagement with your datasets and the message you are conveying in a user friendly and non-technical manner.</p> <p>Here, we will be using Plotly Express to develop interactive visualisations. Plotly Express is a Python module that contains functions that convert data in Python programs into interactive visualisations that can be rendered in web browser based environments.</p> <p>Plotly Express has several useful features for developing interactive visualisations:</p> <ul> <li>functions to generate a range of figure types to explore spatial and non-spatial data (see the gallery)</li> <li>consistent API for functions used to generate the figures (i.e. if you learn the syntax and format to generate scatter plots it can be applied to generate histograms, density plots, bar plots, violin plots, web maps, etc.)</li> <li>simple and intuitive functions to generate the figures (i.e. produce complex interactive figures with a single line of code)</li> </ul>"},{"location":"notebooks/week-2_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-2_1/#data-input","title":"Data input\u00b6","text":"<p>Let's read in some wheat and canola yield data collected by a harvester into a GeoPandas <code>GeoDataFrame</code>. The canola data corresponds to variety 43Y23 RR and the wheat data corresponds to variety ninja. We'll demonstrate how to create interactive visualisations using Plotly Express by generating a simple widget that displays the distribution of wheat and canola yields.</p>"},{"location":"notebooks/week-2_1/#interactive-visualisations-with-plotly-express","title":"Interactive visualisations with Plotly Express\u00b6","text":"<p>Now, let's unpick the syntax for specifying a Plotly Express visualisation. The functions to generate interactive figures are part of the plotly.express module which we've imported into our program as <code>px</code>.</p> <p><code>px.&lt;function name&gt;()</code> is how we'll access the function to generate a given figure. For example, to generate a histogram we call <code>px.histogram()</code> (if we wanted to generate a scatter plot we'd call <code>px.scatter()</code>, if we wanted to generate a line chart we'd call <code>px.line()</code>, if we wanted to generate a pie chart we'd call <code>px.pie()</code> - you get the pattern ...).</p> <p>Next, we need to pass data into the function that will be rendered on the computer display and specify arguments to map data values to elements on the figure. The Plotly Express documentation lists functions that can be used to generate figures and their parameters.</p> <p>Paramters for the <code>px.histogram()</code> function inclue:</p> <ul> <li><code>data_frame</code> - a <code>DataFrame</code> object containing the data to render on the histogram</li> <li><code>x</code> - specifies the column in the <code>DataFrame</code> to be mapped on the x-axis of the figure</li> <li><code>color</code> - a column whose values are used to assign colours to marks (elements) on the display</li> <li><code>marginal</code> - either violin, box, rug, or histogram that shows the distribution of the data</li> <li><code>hover_data</code> - list of column names with values that will be shown in a popup when the cursor hovers over a record on the display</li> </ul> <p>Use the Zoom tool to control what data is visualised and focus the figure on where most of the data is distributed.</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Look up the <code>range_x</code> paramter and consider how it could be used to remove the influence of outliers on the figure. Have a go at using it to restrict the range of values mapped to the x-axis.</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you limit the range of x-axis values to focus the figure on where most of the data is concentrated and remove the effect of outliers? (hint, you'll need to remove the <code>marginal_x</code> argument).</p>"},{"location":"notebooks/week-2_1/#adding-layers","title":"Adding layers\u00b6","text":"<p>The scatter plot we have generated above has layers of points for the scatter plot and layers of geometric elements for the box plot and violin plots. However, each of these layers are all rendered on their own sub-plot.</p> <p>There are often times when we want to overlay layers on the same plot. A common example of this is adding a trendline to a scatter plot to help the viewer see patterns and relationships in the data. If we refer back to the documentation for scatter plots we can see there is a <code>trendline</code> parameter. We can use this parameter to specify the kind of trendline we'd like to draw on our scatter plot:</p> <ul> <li><code>ols</code>: ordinary least squares (or linear line of best fit)</li> <li><code>loess</code>: locally weighted scatterplot smoothing line</li> <li><code>rolling</code>: rolling average or rolling median line</li> </ul> <p>Let's generate a scatter plot with a trendline to explore the relationship between the green normalised difference vegetation index (GNDVI, a satellite derived measure of vegetation greenness) and crop yield. Generally, higher maximum growing season GNDVI values are correlated with higher crop yields.</p> <p>If you hover your cursor over the trendline it will show you the equation for the trendline. You will also notice that we've used the the <code>range_x</code> and <code>range_y</code> parameters to focus the figure on the region where most of the data is concentrated and clip the outliers from the view.</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"Generally, it seems that maximum growing season GNDVI is higher for the wheat (Ninja) crop than canola (43Y23 RR). Can you think of an explanation for this? Canola canopies are characterised by yellow flowers which could reduce their greenness during the growing season."},{"location":"notebooks/week-2_1/#facets","title":"Facets\u00b6","text":"<p>So far we have distinguished groups of data points on the same figure by using a unique colour per-group. However, this can lead to cluttered figures which obscures important variation in the data. To avoid clutter we can create faceted figures where mutliple subplots of the same type are generated, which share axes, and different subsets (groups) of the data are rendered on each subplot.</p> <p>Wilke (2019) distinguish between faceted figures and compound figures. Compound figures are multiple figure types (e.g. scatter plots, histograms, maps), possibly of different datasets, combined into one figure. A key feature of a compound figure is that the subplots do not need to be arranged in a regular grid. The figures above with violin and box plots aligned on the margins of a scatter plot are examples of compound figures.</p> <p>In contrast, facet plots consist of subplots of the same type, showing subsets of the same dataset, and are arranged on a regular grid. You might see the term trellis or lattice plots used to describe facet plots. To ensure correct interpretation of faceted figures it is important that the axes on all plots share the same range and scalings.</p> <p>Let's create a faceted figure that shows the relationship between crop yield and the normalised difference yellowness index (NDYI) side-by-side. The NDYI is a spectral index computed from remote sensing data as a mathematical combination of green and blue reflectance values. Higher NDYI values are associated with a yellower land surface. The NDYI is often used to monitor canola flowering.</p> <p>We can use the <code>facet_row</code> parameter to align subplots on separate rows or the <code>facet_col</code> parameter to align the subplots on separate columns. We specify a column in our <code>GeoDataFrame</code> to use to create the facets. The dataset is split into subsets using unique values in the specified column and each subset is rendered on a subplot. Here, we pass in the <code>Variety</code> column to split the data by crop type.</p>"},{"location":"notebooks/week-2_1/#selecting-the-right-figure","title":"Selecting the \"right\" figure\u00b6","text":"<p>Chapter 5 of Wilke (2019) provides a directory of visualisations which serves as a useful guide for selecting the correct visualisation for different types of data.</p>"},{"location":"notebooks/week-2_1/#using-colour","title":"Using Colour\u00b6","text":"<p>A colour scale is used to map data values to colours on the display. Wilke (2019) outline three uses of colour on figures:</p> <ul> <li>colour to represent data values (e.g. using red shades for low precipitation and blue shades for high precipitation)</li> <li>colour to distinguish groups (e.g. using green for forest on a land cover map, blue for water, orange-red for desert, etc.)</li> <li>colour to highlight (e.g. using colour to highlight particular features on your visualisation)</li> </ul> <p>We can broadly characterise colour scales as being either continuous or qualitative.</p>"},{"location":"notebooks/week-2_1/#continuous-palettes","title":"Continuous palettes\u00b6","text":"<p>Continuous colour scales can be either sequential or diverging and are typically used when using colour to represent data values (often numeric continuous variables). Continuous colour scales can be used to visualise variation in attributes of vector geospatial data on chloropleth maps and variation in attributes of raster data as surfaces.</p>"},{"location":"notebooks/week-2_1/#sequential-palettes","title":"Sequential palettes\u00b6","text":"<p>A sequential colour scale is a palette which consists of single hue such as light green to dark green or light red to dark red. Multi-hue sequential colour scales often consist of hues that imply an intuitive and increasing order to the colours such as light yellows to dark red.</p> <p>Plotly express provides a range of inbuilt sequential colour scales:</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you create a heatmap of monthly precipitation over time using a <code>YlGnBu</code> colour palette?</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Using the <code>GeoDataFrame</code> <code>gdf</code> of crop yield values, can you create a scatter plot of crop yield (the <code>DryYield</code> column) and GNDVI (the <code>gndvi</code> column) and assign green shades to the points which reflect their GNDVI values? Tips: look up the <code>color</code>, <code>color_continuous_scale</code>, and <code>range_color</code> parameters of the <code>scatter()</code> function in the API docs.</p>"},{"location":"notebooks/week-2_1/#diverging-palettes","title":"Diverging palettes\u00b6","text":"<p>Diverging colour scales are used to represent data values deviating in two directions. Often a light colour (e.g. white) is used as the mid-point of a diverging colour scale with gradients of intensifying colour away from this mid-point. A common example of diverging colour scales are climate or weather anomalies where dry or hot years are represented with red colours and wet and cool years are represented with blue colours. Average conditions are often a pale red, pale blue, or white.</p> <p>Plotly also provides a range of diverging colour palettes we can use:</p>"},{"location":"notebooks/week-2_1/#qualitative-palettes","title":"Qualitative palettes\u00b6","text":"<p>Qualitative (or discrete) colour scales should be used to represent groups or categorical data (i.e. data where there is no logical ordering). Thus, qualitative colour scales should not represent gradients of light to dark or use colours that can be interpreted as having an implied ordering. Often, it is sensible to select colours that relate to the category (e.g. on land cover maps using green for vegetated categories, blue for water etc.).</p>"},{"location":"notebooks/week-2_2/","title":"Week 2 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-2_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_2\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-2_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-2_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-2_2\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-2_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport rioxarray as rxr\nimport xarray as xr\nimport plotly.express as px\nimport numpy as np\nimport geopandas as gpd\n\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os  import rioxarray as rxr import xarray as xr import plotly.express as px import numpy as np import geopandas as gpd  import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>data_path = os.path.join(os.getcwd(), \"data_lab-2_2\")\n</pre> data_path = os.path.join(os.getcwd(), \"data_lab-2_2\") In\u00a0[\u00a0]: Copied! <pre># create a 2D ndarray\narr2d = np.array([[1, 2, 3], [4, 5, 6]])\narr2d\n</pre> # create a 2D ndarray arr2d = np.array([[1, 2, 3], [4, 5, 6]]) arr2d <p>The rank (or number of dimensions) of a <code>ndarray</code> is the number of axes.</p> In\u00a0[\u00a0]: Copied! <pre># the rank (ndim) of an ndarry is the number of axes \nprint(f\"the rank of the ndarray is {arr2d.ndim}\")\n</pre> # the rank (ndim) of an ndarry is the number of axes  print(f\"the rank of the ndarray is {arr2d.ndim}\") <p>The <code>shape</code> of an <code>ndarray</code> tells us the size of each axis (how many elements are arranged along that axis).</p> In\u00a0[\u00a0]: Copied! <pre># the shape of the ndarray \nprint(f\"the shape of the ndarray is {arr2d.shape}\")\n</pre> # the shape of the ndarray  print(f\"the shape of the ndarray is {arr2d.shape}\") <p><code>ndarray</code>s can be multidimensional. They can have more than two dimensions. Remote sensing images typically comprise multiple 2-dimensional arrays with each array corresponding to a raster of reflectance measured in a particular wavelength. This 3-dimensional raster data structure can be represented as a NumPy <code>ndarray</code> with the bands dimension on axis 0 (each band is a raster for a given wavelength), rows (height of each raster) on axis 1, and columns (width of each raster) on axis 2.</p> <p>Let's create a <code>ndarray</code> with 3-dimensions.</p> In\u00a0[\u00a0]: Copied! <pre># create a 3D ndarray\narr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\narr3d\n</pre> # create a 3D ndarray arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) arr3d In\u00a0[\u00a0]: Copied! <pre>print(f\"the rank of the ndarray is {arr3d.ndim}\")\nprint(f\"the shape of the ndarray is {arr3d.shape}\")\n</pre> print(f\"the rank of the ndarray is {arr3d.ndim}\") print(f\"the shape of the ndarray is {arr3d.shape}\") <p>The concept of N-dimensional arrays can be extended further. For example, a 4-dimensional <code>ndarray</code> could store a sequence of 3-dimensional <code>ndarray</code>s where the fourth dimension is time and the object represents remote sensing images captured across multiple dates.</p> In\u00a0[\u00a0]: Copied! <pre>first_element = arr2d[0, 0]\nprint(first_element)\n</pre> first_element = arr2d[0, 0] print(first_element) <p>We can use the <code>:</code> symbol to specify slices of a NumPy <code>ndarray</code> to subset. For example, the following are three different ways of slicing the first two rows.</p> <p>Note that the slice is not inclusive of the index location after the <code>:</code> symbol. So, <code>arr2d[0:2, ]</code> would select the first two rows of <code>arr2d</code> - row 0 and row 1 (remember Python indexes from 0).</p> In\u00a0[\u00a0]: Copied! <pre>two_rows_1 = arr2d[0:2, ]\nprint(two_rows_1)\n\ntwo_rows_2 = arr2d[0:2]\nprint(two_rows_2)\n\ntwo_rows_3 = arr2d[:2]\nprint(two_rows_3)\n</pre> two_rows_1 = arr2d[0:2, ] print(two_rows_1)  two_rows_2 = arr2d[0:2] print(two_rows_2)  two_rows_3 = arr2d[:2] print(two_rows_3) <p>We can use multiple slices for different axes. For example, if we wanted to subset values from a selection of rows and columns.</p> In\u00a0[\u00a0]: Copied! <pre>two_rows_cols = arr2d[:2, 1:]\nprint(two_rows_cols)\n</pre> two_rows_cols = arr2d[:2, 1:] print(two_rows_cols) <p>It is important to remember that subsetting a NumPy <code>ndarray</code> using index locations and slicing only considers the location of elements within an array. You will need to consider and understand how locations within an array relate to \"real world\" dimensions such as geographic location or time.</p> In\u00a0[\u00a0]: Copied! <pre>s2_summer_path = os.path.join(data_path, \"week-2-s2-summer-2020.tif\")\nrds = rxr.open_rasterio(s2_summer_path)\n</pre> s2_summer_path = os.path.join(data_path, \"week-2-s2-summer-2020.tif\") rds = rxr.open_rasterio(s2_summer_path) <p>We have used <code>rioxarray</code> to read raster data stored in a GeoTIFF file into our program as an <code>xarray.DataArray</code> object referenced by the variable <code>rds</code>. We can print the <code>xarray.DataArray</code> object to inspect its metadata.</p> In\u00a0[\u00a0]: Copied! <pre>rds\n</pre> rds <p>The raster data stored in the GeoTIFF file is a satellite image of a field in the Wheatbelt. The data is captured by the Sentinel-2 satellite and it measures reflectance in many spectral bands (e.g. blue light, green light, red light, and near infrared light ...). This raster dataset has 23 bands, including some ancillary bands providing information about image quality and atmospheric conditions when the image was captured (we want to know if clouds are obscuring the satellite's view of the land surface).</p> <p>The attributes property of <code>rds</code> stores geospatial metadata such as the coordinate reference system (CRS) and the extent of the dataset. We can access this information via the <code>rds</code> object's <code>rio</code> accessor</p> In\u00a0[\u00a0]: Copied! <pre>print('CRS:', rds.rio.crs)\nprint('Resolution:', rds.rio.resolution())\nprint('Bounds:', rds.rio.bounds())\nprint('Width:', rds.rio.width)\nprint('Height:', rds.rio.height)\n</pre> print('CRS:', rds.rio.crs) print('Resolution:', rds.rio.resolution()) print('Bounds:', rds.rio.bounds()) print('Width:', rds.rio.width) print('Height:', rds.rio.height) <p>The raster data values are stored as arrays within the <code>xarray.DataArray</code> object and can be accessed via the <code>values</code> property.</p> In\u00a0[\u00a0]: Copied! <pre>arr = rds.values\nprint(f\"the shape of the array is {arr.shape}\")\narr\n</pre> arr = rds.values print(f\"the shape of the array is {arr.shape}\") arr <p>In subsequent labs we will explore <code>xarray</code> and raster data formats in more detail. For now, let's focus on visualising raster data stored in <code>xarray.DataArray</code> objects.</p> In\u00a0[\u00a0]: Copied! <pre>rds.sel(band=4).plot.imshow()\n</pre> rds.sel(band=4).plot.imshow() <p>The default visualisation using <code>plot.imshow()</code> returns a static image with <code>xarray.DataArray</code> labels and coordinates plotted. The default colour palette is viridis (yellow-green-blue shades). However, we can change this to a colour palette that relates to red reflectance using the <code>cmap</code> parameter of the <code>plot.imshow()</code> method. Let's use the <code>\"Reds\"</code> colour palette here.</p> In\u00a0[\u00a0]: Copied! <pre>rds.sel(band=4).plot.imshow(cmap=\"Reds\")\n</pre> rds.sel(band=4).plot.imshow(cmap=\"Reds\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>rds.sel(band=2).plot.imshow(cmap=\"Blues\")\n</pre> <p>The plots that you have generated so far have the <code>xarray.DataArray</code> labels attached to them (e.g. plot titles and all the band names listed by the colourbar). Can you look at the different parameters of the <code>plot.imshow()</code> and identify which parameter we can use to stop labels being rendered? Generate a plot of green reflectance (band 3) without labels and using the <code>\"Greens\"</code> colour palette.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>Here, we need to use the <code>add_labels</code> parameter and pass in False as an argument.</p> <pre>rds.sel(band=3).plot.imshow(cmap=\"Greens\", add_labels=False)\n</pre> In\u00a0[\u00a0]: Copied! <pre>rds.sel(band=[4, 3, 2]).plot.imshow(vmin=0, vmax=0.4, add_labels=False)\n</pre> rds.sel(band=[4, 3, 2]).plot.imshow(vmin=0, vmax=0.4, add_labels=False) <p>Using the Plotly Express <code>imshow()</code> function we can create interactive visualisations of raster data stored in <code>xarray.DataArray</code> objects.</p> <p>The <code>px.imshow()</code> function takes in a 2D or 3D (for RGB images) NumPy <code>ndarray</code> as its first argument. The <code>values</code> of a <code>xarray.DataArray</code> are stored in NumPy <code>ndarray</code> objects, so we can select a band and pass it into <code>px.imshow()</code>. Let's visualise red band reflectance as an interactive image (hovering your cursor over the image will return a text popup with the red reflectance value at that location).</p> In\u00a0[\u00a0]: Copied! <pre>px.imshow(rds.sel(band=4).values)\n</pre> px.imshow(rds.sel(band=4).values) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>The <code>height</code> parameter lets you set the figure height in pixels.</p> <p>The <code>color_continuous_scale</code> parameter lets you set the colour palette.</p> <pre>px.imshow(rds.sel(band=4).values, color_continuous_scale=\"Reds\", height=500)\n</pre> In\u00a0[\u00a0]: Copied! <pre>s2_gndvi_path = os.path.join(data_path, \"gndvi_2020_bf66_fitted.tif\")\ngndvi_rds = rxr.open_rasterio(s2_gndvi_path)\n</pre> s2_gndvi_path = os.path.join(data_path, \"gndvi_2020_bf66_fitted.tif\") gndvi_rds = rxr.open_rasterio(s2_gndvi_path) In\u00a0[\u00a0]: Copied! <pre>gndvi_rds\n</pre> gndvi_rds <p>If we inspect the dataset, we can see that it has 52 bands. Each band is an array of GNDVI corresponding to a week of the year. In 2020, canola was grown in this field.</p> <p>In a previous lab we introduced the concept of a subplot. This is a good use case for a faceted figure. We can represent each week as a subplot, align the subplots sequentially through time, and use the same mapping of data values to colour to make comparisons of greenness across weeks easy.</p> <p>The <code>xarray.DataArray</code> <code>plot.imshow()</code> method has <code>col</code> and a <code>col_wrap</code> parameters. The <code>col</code> parameter can be passed a <code>dim</code> for which the faceted subplots are created. Passing in the <code>\"band\"</code> <code>dim</code> here will generate a separate image for each of the arrays along the dimension specified. Here, that will create a new GNDVI image for each week. The <code>col_wrap</code> parameter specifies how many images are laid out along one row on the display.</p> In\u00a0[\u00a0]: Copied! <pre>gndvi_rds.plot.imshow(col=\"band\", col_wrap=10)\n</pre> gndvi_rds.plot.imshow(col=\"band\", col_wrap=10) <p>You should be able to see the green up and green down dynamics of vegetative crop growth in this facet plot.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_ndyi_path = os.path.join(data_path, \"ndyi_2020_bf66_fitted.tif\")\nndyi_rds = rxr.open_rasterio(s2_ndyi_path)\ngndvi_rds.plot.imshow(col=\"band\", col_wrap=10, cmap=\"afmhot\")\n</pre> In\u00a0[\u00a0]: Copied! <pre>elev_gdf_path = os.path.join(os.getcwd(), \"data_lab-2_2\", \"week-2-bf66-elevation.geojson\")\nelev_gdf = gpd.read_file(elev_gdf_path)\nelev_gdf.head()\n</pre> elev_gdf_path = os.path.join(os.getcwd(), \"data_lab-2_2\", \"week-2-bf66-elevation.geojson\") elev_gdf = gpd.read_file(elev_gdf_path) elev_gdf.head() <p>Printing out the <code>head()</code> of the <code>GeoDataFrame</code> <code>elev_gdf</code> clearly illustrates the tabular structure for representing vector data. Attributes are stored in columns, the locational information which is a <code>POINT</code> geometry object is stored in a <code>geometry</code> column, and each row corresponds to one geographic feature.</p> In\u00a0[\u00a0]: Copied! <pre>elev_gdf.explore()\n</pre> elev_gdf.explore() <p>This clearly shows the location of points within the field. However, it is not very informative about each point's elevation value. We can change the colour of each point to represent the variability in elevation across the field. To do this we need to use the <code>column</code> paramter of the <code>explore()</code> method to specify the column in the <code>GeoDataFrame</code> that we wish to represent using colour. We can also specifiy a colour palette to use with the <code>cmap</code> parameter.</p> <p>Executing the following code will render the elevation data with low elevations in blue shades and higher locations in yellow shades.</p> In\u00a0[\u00a0]: Copied! <pre>elev_gdf.explore(column=\"Elevation\", cmap=\"cividis\")\n</pre> elev_gdf.explore(column=\"Elevation\", cmap=\"cividis\") In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020 = gpd.read_file(os.path.join(data_path, \"fao_wheat_crop_yield_2020.geojson\"))\n</pre> gdf_wheat_yield_2020 = gpd.read_file(os.path.join(data_path, \"fao_wheat_crop_yield_2020.geojson\")) In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020.head()\n</pre> gdf_wheat_yield_2020.head() <p>We'll get a simple chloropleth map with the default viridis colour palette if we pass in the column label for wheat yield <code>\"yield_100g_ha\"</code> as an argument to the <code>column</code> parameter.</p> In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\")\n</pre> gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\") <p>You might find that the thick borders for country outlines obscures spotting spatial patterns and trends in wheat crop yields. Let's remove the borders. If you look at the docs for <code>explore()</code> you will see there is a <code>style_kwds</code> parameter we can pass a <code>dict</code> of styling configurations.</p> <p>A <code>dict</code> is a data structure with key:pairs. The <code>dict</code> passed to <code>style_kwds</code> contains a keys that descibe a visual element of the display we wish to adjust and a value that specifies how it should be adjusted. For example, the <code>stroke</code> key determines how the border of geometric features on the map is represented. We can set this to <code>False</code> to remove the border. Note, <code>dict</code> objects are specified by enclosing key:value pairs in braces <code>{}</code>.</p> <pre>{\"stroke\": False}\n</pre> In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\", style_kwds={\"stroke\": False})\n</pre> gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\", style_kwds={\"stroke\": False}) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\", style_kwds={\"stroke\": False, \"fillOpacity\": 0.75})\n</pre> In\u00a0[\u00a0]: Copied! <pre>clum_carnarvon_path = os.path.join(data_path, \"clum_land_use_carnarvon.geojson\")\nclum_gdf = gpd.read_file(clum_carnarvon_path)\n</pre> clum_carnarvon_path = os.path.join(data_path, \"clum_land_use_carnarvon.geojson\") clum_gdf = gpd.read_file(clum_carnarvon_path) In\u00a0[\u00a0]: Copied! <pre>clum_gdf.head()\n</pre> clum_gdf.head() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>clum_gdf.explore(column=\"Commod_dsc\", categorical=\"set2\")\n</pre>"},{"location":"notebooks/week-2_2/#geospatial-data-visualisation","title":"Geospatial data visualisation\u00b6","text":"<p>This lab will demonstrate how to generate exploratory and interactive visualisations of geospatial data including multispectral satellite images, point-based samples with in a field, global maps of crop yield, and crop types in orchards and plantations. This lab will provide an introduction to:</p> <ul> <li>raster and vector geospatial data</li> <li>generating interactive visualisations of vector datasets</li> <li>generating static and interactive visualisations of raster data</li> </ul> <p>This week the focus will be on quick, exploratory, and interactive visualisations of geospatial data. For a more detailed discussion of visualising geospatial data please read Wilke (2019). For a focus on generating cartographic quality static maps please see the open course Mapping and Data Visualization with Python.</p>"},{"location":"notebooks/week-2_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-2_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-2_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-2_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-2_2/#geospatial-data","title":"Geospatial data\u00b6","text":"<p>Geospatial data represents geographic features or phenomenon as data in computer program or file.</p> <p>There are two main types of geospatial data:</p> <ul> <li>vector data - point, line, or polygon geometries</li> <li>raster data - images and arrays</li> </ul> <p>There are two components to geospatial data:</p> <ul> <li>Positional information describing location, shape, and extent (e.g. an <code>(x, y)</code> coordinate pair representing the location of a weather station)</li> <li>Attribute information describing characteristics of the phenomenon or entity (e.g. a name:value pair recording the name of the weather station <code>name:'Perth Airport'</code>)</li> </ul>"},{"location":"notebooks/week-2_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-2_2/#raster-data-in-python","title":"Raster data in Python\u00b6","text":""},{"location":"notebooks/week-2_2/#raster-data-model","title":"Raster data model\u00b6","text":"<p>Raster data breaks the Earth's surface up into a grid of cells (pixels). Each pixel is assigned a value that corresponds to the geographic feature or phenomenon of interest. For example, pixels in a raster precipitation dataset would be assigned a numeric value that represents the amount of precipitation that fell at that location. Pixels in a land cover map would have an integer value that corresponds to a land cover class label. The values assigned to pixels in a raster dataset are the attribute information.</p> <p></p> <p>The size of the pixels relative to their position on the Earth's surface determines the spatial detail that can be resolved. A land cover map with pixels that represent a 1 km x 1 km portion of the Earth's surface will not be able to identify features such as individual buildings.</p> <p>The figure below shows the 2018 European Space Agency (ESA) Climate Change Initiative (CCI) land cover map. Each pixel represents a 300 m x 300 m area on the Earth\u2019s land surface and a pixel can only represent a single land cover type. If you look at the bottom two zoomed in maps you can see some limitations of representing land cover using 300 m x 300 m spatial resolution pixels. The shape of land cover features are poorly represented by the \u201cblock-like\u201d arrangement of pixels and there is variation in land cover within a single pixel (a mixed pixel problem).</p> <p></p> <p>Raster data represents geographic features and variables (e.g. elevation, reflectance in UAV images) as a grid of values (pixels). In Python, a data structure called an array is used to store and organise pixels in a raster dataset. Typically, NumPy <code>ndarray</code> objects are used for storing raster data in Python programs.</p>"},{"location":"notebooks/week-2_2/#arrays-numpy-ndarrays","title":"Arrays: NumPy <code>ndarray</code>s\u00b6","text":"<p>NumPy is a library used for scientific and numerical computing and is based around an N-dimensional <code>ndarray</code> object. An <code>ndarray</code> is a grid of elements of the same data type. The dimensions of a NumPy <code>ndarray</code> are called axes. NumPy <code>array</code>s can be created from sequences of values (e.g. stored in lists, tuples, other <code>ndarray</code>s).</p> <p></p> <p>We can create a simple 2-dimensional <code>ndarray</code> using the <code>array()</code> function. A <code>ndarray</code> with 2-dimensions is a matrix with rows arranged on the 0 axis and columns arranged on the 1 axis.</p>"},{"location":"notebooks/week-2_2/#subsetting-numpy-ndarrays","title":"Subsetting NumPy ndarrays\u00b6","text":"<p>A subsetting opertation is when you select values from a NumPy <code>ndarray</code> object based on their index locations. These operations are generally referred to as indexing and slicing when working with NumPy <code>ndarray</code> objects.</p> <p></p> <p>We can extract a value from a NumPy <code>ndarray</code> based on its index location. For example, the first element of a 2-Dimensional <code>ndarray</code> is at location <code>[0, 0]</code> (i.e. the 0th row and 0th column).</p>"},{"location":"notebooks/week-2_2/#xarray","title":"Xarray\u00b6","text":"<p>Xarray is a Python package that builds on top of NumPy's array-based data structures, but provides extra tools and functions that are useful for working with geospatial and Earth Science datasets. For example, <code>xarray.DataArray</code> data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values.</p> <p><code>xarray</code> also provides convenient functions for reading raster data from geospatial data files on disk into memory as <code>xarray.DataArray</code> objects which we can use in our Python programs while retaining geographic and temporal information about the raster values stored in the array.</p> <p>Specifically, while a NumPy <code>ndarray</code> stores just the raster values and has some properties such as the <code>shape</code> (number of elements along each axis) and <code>ndim</code> (the dimensions of the array) it does not explicitly store any geospatial, temporal, or other geographic metadata. <code>xarray</code> solves this problem by reading raster data into an <code>xarray.DataArray</code> object with:</p> <ul> <li><code>values</code>: the multidimensional array of raster values</li> <li><code>dims</code>: a list of names for the dimensions of the array (e.g. instead of axis 0 describing the 0th (row) dimension of an array, that dimension can have a descriptive label such as longitude)</li> <li><code>coordinates</code>: a <code>list</code> of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)</li> <li><code>attrs</code>: a <code>dict</code> of metadata attributes describing the dataset</li> </ul> <p><code>xarray.DataArray</code> objects can be stored within a larger container called <code>xarray.Dataset</code>. An <code>xarray.Dataset</code> can store many <code>xarray.DataArray</code> objects that share <code>dims</code> and <code>coordinates</code>. This is useful if you have different arrays of different <code>Variables</code> that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single <code>xarray.Dataset</code>).</p> <p></p> <p>Schematic of an xarray.Dataset (source: xarray Getting Started)</p> <p>Why is <code>xarray</code> useful for geospatial data?</p> <ul> <li>The <code>dims</code> and <code>coordinates</code> of an <code>xarray.DataArray</code> mean we can subset values from an array using latitude, longitude, time, or whatever a coordinate describes; we're not just restricted to subsetting values based on their index location within an array</li> <li><code>xarray.Dataset</code> objects provide a container to store multidimensional arrays (e.g. many variables and time points) that are common in geography, Earth Sciences, meteorology, and agriculture. For example, multispectral satellite images of the same location over time; arrays of different meteorological variables)</li> <li>useful functions for reading, analysing and visualising raster or array-like geospatial data that are common across many spatial data science workflows</li> </ul>"},{"location":"notebooks/week-2_2/#data-input","title":"Data input\u00b6","text":"<p>The <code>rioxarray</code> package provides tools for reading and writing raster geospatial data files into <code>xarray.DataArray</code> objects.</p> <p>Let's pass the path to a GeoTIFF file of raster data into the <code>rioxarray</code> <code>open_rasterio()</code> function:</p>"},{"location":"notebooks/week-2_2/#static-images","title":"Static images\u00b6","text":"<p><code>xarray.DataArray</code> objects have a <code>plot.imshow()</code> method that will render array based data as an image. Band 4 of the <code>xarray.DataArray</code> stores reflectance of red light off the Earth's surface. Let's plot it:</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>The blue band (storing reflectance of blue light) in Sentinel-2 images is band 2 in the <code>xarray.DataArray</code> object referenced by <code>rds</code>. Can you plot blue band reflectance and select a sensible colour palette for visualing the spatial variation in blue reflectance?</p> <p>Hint: you can find a list of colour palettes here.</p>"},{"location":"notebooks/week-2_2/#colour-and-composite-images","title":"Colour and composite images\u00b6","text":"<p>A particular colour is defined by the intensity of light in different parts of the visible spectrum (e.g. yellow is a mixture of light in red and green wavelengths).</p> <p>Colour is represented by combinations (addition) of red, green, and blue light. Red, green, and blue are primary colours and combine to form white. An absence of red, green, and blue is black. Secondary colours can be formed by the addition of primary colours of varying intensities (e.g. yellow is the addition of red and green, magenta is the addition of red and blue, and cyan is the addition of green and blue).</p> <p>Computer displays consist of red, green, and blue sub-pixels, which when activated with different intensities, are perceived as different colours. The range of colours that can be displayed on a computer display is called the gamut. Colour in computer programs is represented as a three byte hexadecimal number with byte 1 corresponding to red, byte 2 corresponding to green, and byte 3 corresponding to blue. Each byte can take the range of 0 to 255 in decimal. 0 indicates the absence of a colour and 255 indicates saturation of that colour:</p> <ul> <li>white: 255 255 255</li> <li>black: 0 0 0</li> <li>red: 255 0 0</li> <li>green: 0 255 00</li> <li>blue: 0 0 255</li> </ul> <p></p> <p>Computer displays represent colour through varying the intensity of sub-pixel displays of red, green, and blue light. Variability in data values in multiband rasters can be visualised by relating data values in one band to the intensity of one of the primary colours on the computer display. Visualising a multiband raster in this way creates an additive RGB or colour composite image - it is called a composite image because each pixel is a composite of red, green, and blue light.</p> <p>Above we rendered the red, green, and blue band reflectance from the Sentinel-2 image separately. However, if we combine these reflectance measures into a composite image (e.g. where red reflectance is represented by sub-pixel intensity of red light) we can create a true colour image as if we were looking down on the Earth's surface with our eyes.</p> <p>We can select multiple bands from the <code>xarray.DataArray</code> object that correspond to red, green, and blue reflectance to render a true colour image. We need to pass a <code>list</code> of bands into the <code>sel()</code> method of the <code>xarray.DataArray</code> object referenced by the variable <code>rds</code>.</p>"},{"location":"notebooks/week-2_2/#interactive-raster-visualisations","title":"Interactive raster visualisations\u00b6","text":""},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Above, we have an interactive image of red band reflectance. However, the image is quite small and the colour palette could be more intuitive to indicate it's an image of red light reflectance. Can you look at the parameters in the <code>px.imshow()</code> docs that we could use to visualise reflectance using a red colour palette and increase the height of the plot?</p>"},{"location":"notebooks/week-2_2/#plotting-raster-time-series","title":"Plotting raster time series\u00b6","text":"<p>In many cases, we'll have raster data that covers the same location on the Earth's surface but is captured on different dates. A good example of this is remotely sensed satellite data which captures spectral reflectance data for the same location each time the satellite overpasses a location. In this instance, each band (or 2D array) might represent an observation for a variable on different dates. For example, green normalised difference vegetation index (GNDVI) values use green and near infrared reflectance to represent the greenness of a location. We can visualise GNDVI through time to represent vegetation growth dynamics (e.g. the green up of a crop after planting).</p> <p>Let's read in a raster dataset of GNDVI values for different dates.</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>There is a GeoTIFF file <code>ndyi_2020_bf66_fitted.tif</code> in the <code>data_lab-2_2</code> folder. It stores normalised difference yellowness index (NDYI) values for each week of the year. Can you read this file into your program and plot each week's NDYI values as an image to visualise change in yellowness of the canola crop canopy through the growing season? Generate this figure as a faceted plot and select a suitable colour map to visualise change in yellowness.</p> <p>Hint: you can find a list of colour palettes here.</p>"},{"location":"notebooks/week-2_2/#vector-data-in-python","title":"Vector data in Python\u00b6","text":""},{"location":"notebooks/week-2_2/#vector-data-model","title":"Vector data model\u00b6","text":"<p>Vector data uses point, line, or polygon geometries to represent geographic features.</p> <p>Coordinate pairs: point locations or the vertices in lines and polygons are represented using coordinate pairs. The coordinate pairs indicate where that feature is located on the Earth's surface (relative to an origin); longitude and latitute are commonly used as coordinate pairs.</p> <p>Attribute information: vector data also stores non-spatial attribute information which describe characteristics of the geographic phenomenon or entity represented by the geometry feature.</p> <p></p>"},{"location":"notebooks/week-2_2/#geopandas-geodataframe","title":"GeoPandas GeoDataFrame\u00b6","text":"<p>A GeoPandas <code>GeoDataFrame</code> is a tabular data structure for storing vector geospatial data and is based on a regular pandas <code>DataFrame</code>.</p> <p>A <code>GeoDataFrame</code> consists of columns of non-spatial attributes similar to a pandas <code>DataFrame</code>. However, a <code>GeoDataFrame</code> also has a <code>geometry</code> column which is a <code>GeoSeries</code> of geometries for the spatial data associated with each row.</p> <p>In Python, geometries are represented as Shapely <code>Geometry</code> objects. The <code>geometry</code> column in a GeoPandas <code>GeoDataFrame</code> is a <code>Series</code> of Shapely <code>Geometry</code> objects. Printing a Shapely <code>Geometry</code> object returns a Well Known Text (WKT) string description of the geometry (e.g. <code>POINT (0, 1)</code>). The <code>geometry</code> column of a <code>GeoDataFrame</code> (or a <code>GeoSeries</code>) can be viewed as a sequence of Shapely <code>Geometry</code> objects:</p> <pre><code>a_geoseries = [POINT (0, 1), POINT (0, 2), POINT (2, 3)]\n</code></pre> <p>Shapely provides tools for representing geometries in Python programs. It does not provide tools for reading geometry data from disk or handling attribute data. GeoPandas <code>GeoDataFrame</code> and <code>GeoSeries</code> combine Shapely's functionality for handling geometries with tools for reading and writing vector data, handling attributes, and visualisation. Therefore, we will focus on using <code>GeoDataFrame</code>s in these labs.</p> <p>Let's read in a GeoJSON file storing the elevation of points sampled across the same field in Western Australia that we have been exploring using raster data.</p>"},{"location":"notebooks/week-2_2/#interactive-vector-visualisations","title":"Interactive vector visualisations\u00b6","text":"<p><code>GeoDataFrame</code>s have a helpful <code>explore()</code> method for rendering spatial data on a \"slippy\" web map.</p>"},{"location":"notebooks/week-2_2/#chloropleth-mapping","title":"Chloropleth mapping\u00b6","text":"<p>Chloropleth maps use a feature's fill colour to visualise spatial variation in a variable. A continuous colour palette is used to represent variation in the values of attributes of a vector spatial dataset. For a more detailed review of chloropleth maps please see Rey et al. (2020) and Wilke (2019).</p> <p>Let's create a chloropleth map of wheat crop yields at the national level in 2020 downloaded from FAOSTAT.</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>The opacity of the fill colour on the chloropleth maps is set to 0.5. Can you find a parameter in the  <code>explore()</code> docs to change the <code>fillOpacity</code> to 0.75?</p> <p>Hint: you can use search tools and find tools to find words on the <code>explore()</code> docs page.</p>"},{"location":"notebooks/week-2_2/#categorical-vector-maps","title":"Categorical vector maps\u00b6","text":"<p>Sometimes the variable that we wish to visualise on a map using colour is not continuous but is categorical. An example could be the crop type associated with a polygon feature of field boundaries. In these cases we're mapping a categorical value to a colour and a change in colour does not represent an increase or decrease in a numeric value, but a change in group or class.</p> <p>In these cases qualitative (or discrete) colour scales should be used to represent groups (i.e. data where there is no logical ordering). Thus, qualitative colour scales should not represent gradients of light to dark or use colours that can be interpreted as having an implied ordering. Often, it is sensible to select colours that relate to the category (e.g. on land cover maps using green for vegetated categories, blue for water etc.).</p> <p>Let's make a categorical map of the crop type in a field for a selection of fields near Canarvon in Western Australia. The data is derived from the Catchment scale land use of Australia product.</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you make a categorical interactive map of the crop type for each field? You will need to use the <code>explore()</code> docs to identify which parameter to use to let <code>explore()</code> know it is visualising categorical data. You should pass in a qualitative colour palette as an argument to the this parameter. A list of qualitative colour palettes can be found here.</p>"},{"location":"notebooks/week-2_practice/","title":"Week 2 practice","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-2_practice\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_practice.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-2_practice.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_practice\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-2_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-2_practice\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_practice.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-2_practice.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-2_practice\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-2_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import pandas as pd import geopandas as gpd import plotly.express as px  import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>pop_continents = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"population_projections_1800_2100_continents.csv\"))\npop_continents.head()\n</pre> pop_continents = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"population_projections_1800_2100_continents.csv\")) pop_continents.head() In\u00a0[\u00a0]: Copied! <pre>## Make visualisation here ##\n</pre> ## Make visualisation here ##  answer <pre>px.line(\n    pop_continents,\n    x=\"Year\",\n    y=\"Population\",\n    color=\"Continent\"\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>crop_yields_pop_continents = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"population_yield_continents.csv\"))\ncrop_yields_pop_continents.head()\n</pre> crop_yields_pop_continents = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"population_yield_continents.csv\")) crop_yields_pop_continents.head() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>px.line(\n    crop_yields_pop_continents,\n    x=\"Year\",\n    y=\"Cereal_Yield_Tn/ha\",\n    color=\"Continent\"\n)\n</pre> <p>You should note the flat line for Africa indicating that crop yields have not grown over the past few decades. This is concerning as Africa is where most population growth is projected to occur. Without increases in productivity, to ensure food demand it met natural habitats will need to be brought under cultivation.</p> <p>We can use Plotly Express's visualisation tools to create animations that show how population growth and crop yields have changed over time.</p> <p>Can you create an animation of how the relationship between population growth and crop yields has changed for each continent since 1961?</p> <p>You will need to use <code>px.scatter()</code> for this task. Set the <code>x</code> argument to <code>\"Population\"</code>, <code>y</code> to <code>\"Cereal_Yield_Tn/ha\"</code>, <code>animation_frame</code> to <code>\"Year\"</code>, and <code>color</code> to <code>\"Continent\"</code>. To make sure the axes ranges are suitable set <code>range_x</code> to <code>[0, 5000000000]</code> and <code>range_y</code> to <code>[0, 6]</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ##  answer <pre>px.scatter(\n    crop_yields_pop_continents, \n    x=\"Population\", \n    y=\"Cereal_Yield_Tn/ha\", \n    size=\"Cereal_Yield_Tn/ha\",\n    color=\"Continent\",\n    animation_frame=\"Year\",\n    range_x=[0, 5000000000],\n    range_y=[0, 6]\n)\n</pre> <p>Can you spot how Asia's population grows and crop yields increase, how there is increasing crop yields but much less rapid population growth in Europe, and how Africa's population is growing but crop yields remain low.</p> <p>Plotly Express also has a convenience function, <code>px.chloropleth()</code>, that we can use to make animated chloropleth maps. We can read in the following CSV file of country-level crop yields since 1961 and make an animated map of changing global crop yields from 1961 until the present day.</p> In\u00a0[\u00a0]: Copied! <pre>crop_yields_pop_countries = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"population_yield_countries.csv\"))\ncrop_yields_pop_countries.head()\n</pre> crop_yields_pop_countries = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"population_yield_countries.csv\")) crop_yields_pop_countries.head() <p>Can you use the <code>px.chloropleth()</code> function to make animate changes in crop yields at the country-level since 1961?</p> <p>To get a good visualisation set the <code>range_color</code> argument to <code>[0,8]</code> and set the <code>height</code> argument to <code>500</code> (this makes the figure 500 pixels tall). Use the <code>px.chloropleth()</code> docs to look up what the <code>range_color</code> argument does.</p> <p>The <code>px.chloropleth()</code> function has a <code>locations</code> argument, set this to <code>\"Code\"</code> (<code>Code</code> in a country code to map rows in the <code>DataFrame</code> to country geometries on a map). Set the <code>color</code> argument to <code>\"Cereal_Yield_Tn/ha\"</code> so each country's fill colour represents crop yield, set the <code>animation_frame</code> argument to <code>\"Year\"</code> and set the <code>color_continuous_scale</code> argument to <code>\"viridis\"</code>.</p> <p>Notice how over time the fill colour of countries in Europe, America, and Asia becomes lighter indicating increasing crop yields but Africa remains in darker purple shades.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ##  answer <pre>px.choropleth(\n    crop_yields_pop_countries, \n    locations=\"Code\", \n    color=\"Cereal_Yield_Tn/ha\", \n    hover_name=\"Entity\", \n    animation_frame=\"Year\", \n    color_continuous_scale=\"viridis\",\n    range_color=[0,8],\n    height=500\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>williams_2020_continents = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"williams_2020_continents.csv\"))\nwilliams_2020_continents.head()\n</pre> williams_2020_continents = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"williams_2020_continents.csv\")) williams_2020_continents.head() <p>Can you make a bar plot using the <code>px.bar()</code> function to show percentage change in cropland area by by 2050 to meet food demand under the business as usual scenario?</p> <p>You should set the <code>x</code> argument to <code>bau_change_pct</code> and the <code>y</code> argument to <code>\"Continent\"</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>px.bar(\n    williams_2020_continents, \n    x=\"bau_change_pct\", \n    y=\"Continent\", \n    labels = {\n        \"bau_change_pct\": \"% change in cropland area by 2050 (BAU)\"\n    }\n)\n</pre> <p>Note, the large increase in projected cropland expansion in Africa. The region where crop yields are lowest (i.e. where agricultural land is least productive). Let's explore the same data on a map at the country level.</p> In\u00a0[\u00a0]: Copied! <pre>williams_2020_countries = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"williams_2020_countries.csv\"))\nwilliams_2020_countries.head()\n</pre> williams_2020_countries = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-2_practice\", \"williams_2020_countries.csv\")) williams_2020_countries.head() <p>Can you make a chloropleth map that visulises the percentage change in cropland area by 2050 to meet food demand under the business as usual scenario?</p> <p>Use the <code>px.chloropleth()</code> function without setting the <code>animation_frame</code> argument. You should set the <code>location</code> argument to <code>\"Code\"</code>, <code>color</code> to <code>\"bau_change_pct\"</code>, <code>hover_name</code> to <code>\"Entity\"</code>, <code>range_color</code> to <code>[-10,120]</code>, and <code>height</code> to <code>500</code>.</p> <p>You should see in bright colours the countries of Africa where cropland area is projected to increase substantially over the coming decades.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>px.choropleth(\n    williams_2020_countries, \n    locations=\"Code\", \n    color=\"bau_change_pct\", \n    hover_name=\"Entity\",  \n    range_color=[-10,120],\n    height=500,\n    labels = {\n        \"bau_change_pct\": \"% change in cropland area by 2050 (BAU)\"\n    }\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>px.bar(\n    williams_2020_continents, \n    x=\"bau_habitat_loss_all\", \n    y=\"Continent\", \n    range_x=[-11,11],\n    labels = {\n        \"bau_habitat_loss_all\": \"% change in habitat area for all species by 2050 under BAU scenario\"\n    }\n)\n</pre> <p>Let's map this data at the country level on a chloropleth map. Use the pandas <code>DataFrame</code> <code>williams_2020_countries</code> for this task. You should use the <code>px.chloropleth()</code> function, set the <code>locations</code> argument to <code>\"Code\"</code>, set the color argument to <code>\"bau_habitat_loss_all\"</code>, set the <code>hover_name</code> to <code>\"Entity\"</code>, set the <code>range_color</code> to <code>[-11, 0]</code>, and set <code>height</code> to <code>500</code>.</p> <p>You should see the Sub-Saharan African countries in darker purple shades indicating where most habitat loss will occur as a result of cropland expansion by 2050. There are also some countries in South America in moderate-dark purple shades where some habitat loss could occur. This is also concerning considering the important ecosystems in South America.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>px.choropleth(\n    williams_2020_countries, \n    locations=\"Code\", \n    color=\"bau_habitat_loss_all\", \n    hover_name=\"Entity\",  \n    range_color=[-11,0],\n    height=500,\n    labels = {\n        \"bau_habitat_loss_all\": \"% change in habitat by 2050 (BAU)\"\n    }\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## BAR PLOT: CHANGE IN CROPLAND AREA BAU ##\n</pre> ## BAR PLOT: CHANGE IN CROPLAND AREA BAU ## answer <pre>px.bar(\n    williams_2020_continents, \n    x=\"bau_change_pct\", \n    y=\"Continent\", \n    labels = {\n        \"bau_change_pct\": \"% change in cropland area by 2050 (BAU)\"\n    }\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## BAR PLOT: CHANGE IN CROPLAND AREA CLOSING YIELD GAPS ##\n</pre> ## BAR PLOT: CHANGE IN CROPLAND AREA CLOSING YIELD GAPS ## answer <pre>px.bar(\n    williams_2020_continents, \n    x=\"yields_change_pct\", \n    y=\"Continent\", \n    labels = {\n        \"yields_change_pct\": \"% change in cropland area by 2050 (Close yield gaps)\"\n    }\n)\n</pre> <p>There are a few things to note when comparing these bar plots. One is that generally across all continents the change in area allocated to cropland when yield gaps close reduces. Second, that in some continents we can meet demand for food while taking land out of cultivation. Third, note the rapid drop in cropland expansion in Africa when closing yield gaps.</p> <p>Let's viualise this with respect to change in habitat loss. Can you make a chloropleth map visualising the column <code>\"yields_habitat_loss_all\"</code> in the <code>DataFrame</code> <code>williams_2020_countries</code>? This will be a map showing the projected habitat loss for all species that will occur by 2050 due to cropland expansion if yield gaps are closed. Again, use the <code>px.chloropleth()</code> function. Set the <code>range_color</code> argument to <code>[-5,5]</code> (play around with changing this and see how this affects patterns visualised on the map). Also, set <code>height</code> to <code>500</code>, <code>color_continuous_scale</code> to <code>\"RdYlGn\"</code>, and <code>color_continuous_midpoint</code> to <code>0</code> (look up what the <code>color_continuous_midpoint</code> parameter does in the docs and think about why setting this to 0 makes sense).</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>px.choropleth(\n    williams_2020_countries, \n    locations=\"Code\", \n    color=\"yields_habitat_loss_all\", \n    hover_name=\"Entity\",  \n    range_color=[-5,5],\n    height=500,\n    color_continuous_scale=\"RdYlGn\",\n    color_continuous_midpoint=0,\n    labels = {\n        \"yields_habitat_loss_all\": \"% change in habitat by 2050 (Close yield gaps)\"\n    }\n)\n</pre> <p>Compare the habitat loss due to cropland expansion (particularly over Sub-Saharan Africa and South America) under a scenario of closing yield gaps to a business as usual scenario.</p> <p>Run the following code to generate the business as usual habitat loss figure for comparison and note the different colour scales used.</p> In\u00a0[\u00a0]: Copied! <pre>px.choropleth(\n    williams_2020_countries, \n    locations=\"Code\", \n    color=\"bau_habitat_loss_all\", \n    hover_name=\"Entity\",  \n    range_color=[-10,10],\n    height=500,\n    color_continuous_scale=\"RdYlGn\",\n    color_continuous_midpoint=0,\n    labels = {\n        \"bau_habitat_loss_all\": \"% change in habitat by 2050 (BAU)\"\n    }\n)\n</pre> px.choropleth(     williams_2020_countries,      locations=\"Code\",      color=\"bau_habitat_loss_all\",      hover_name=\"Entity\",       range_color=[-10,10],     height=500,     color_continuous_scale=\"RdYlGn\",     color_continuous_midpoint=0,     labels = {         \"bau_habitat_loss_all\": \"% change in habitat by 2050 (BAU)\"     } ) <p>Williams et al. (2020) also looked at other options for meeting demand for food which reduce the need for cropland expansion. Among the options they looked at were changing diets to a less meat intensive diet (the EAT-Lancet diet) and halving food waste.</p> <p>Let's make bar plots to compare the effect of closing yield gaps, adopting healthy diets, or halving food waster on cropland expansion.</p> <p>Note how there is still a large increase in cropland area in Africa under scenarios of adopting healthy diets and halving food waste. Also note how different solutions might be appropriate on different continents.</p> <p>Can you edit the below bar plots to use a consistent range of values on the x-axis to aid comparison?</p> In\u00a0[\u00a0]: Copied! <pre>px.bar(\n    williams_2020_continents, \n    x=\"yields_change_pct\", \n    y=\"Continent\", \n    labels = {\n        \"yields_change_pct\": \"% change in cropland area by 2050 (Close yield gaps)\"\n    }\n)\n</pre> px.bar(     williams_2020_continents,      x=\"yields_change_pct\",      y=\"Continent\",      labels = {         \"yields_change_pct\": \"% change in cropland area by 2050 (Close yield gaps)\"     } ) In\u00a0[\u00a0]: Copied! <pre>px.bar(\n    williams_2020_continents, \n    x=\"diets_change_pct\", \n    y=\"Continent\", \n    labels = {\n        \"diets_change_pct\": \"% change in cropland area by 2050 (adopt healthy diets)\"\n    }\n)\n</pre> px.bar(     williams_2020_continents,      x=\"diets_change_pct\",      y=\"Continent\",      labels = {         \"diets_change_pct\": \"% change in cropland area by 2050 (adopt healthy diets)\"     } ) In\u00a0[\u00a0]: Copied! <pre>px.bar(\n    williams_2020_continents, \n    x=\"waste_change_pct\", \n    y=\"Continent\", \n    labels = {\n        \"waste_change_pct\": \"% change in cropland area by 2050 (halve food waste)\"\n    }\n)\n</pre> px.bar(     williams_2020_continents,      x=\"waste_change_pct\",      y=\"Continent\",      labels = {         \"waste_change_pct\": \"% change in cropland area by 2050 (halve food waste)\"     } )"},{"location":"notebooks/week-2_practice/#week-2-practice-exercises","title":"Week 2 - Practice Exercises\u00b6","text":"<p>This notebook contains exercises related to week 2's content on generating interactive visualisations using Plotly Express. These exercises are based on an Our World in Data article: To protect the world\u2019s wildlife we must improve crop yields \u2013 especially across Africa.</p> <p>This article contains several visualisations, based on the paper by Williams et al. (2020), which illustrate that projected cropland expansion to meet future demands for food will result in substantial habitat loss, threats to biodiversity, and extinction. In part, this is due to growing populations in Africa where i) crop yields are low and so increasing the area under cultivation is required to generate more food, and ii) habitat for many key species is located. They also explore potential scenarios for meeting demand for food that avert the need for cropland expansion. Of the scenarios they explore, closing yield gaps in Africa looks most promising and with potential food security and economic co-benefits.</p>"},{"location":"notebooks/week-2_practice/#download-data","title":"Download data\u00b6","text":""},{"location":"notebooks/week-2_practice/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-2_practice/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-2_practice/#population-growth","title":"Population growth\u00b6","text":"<p>Cropland expansion will result in habitat loss for many species. This cropland expansion is required to meet demand for food. One of the reasons for an increasing demand for food is population growth.</p> <p>Over the coming decades, population growth is projected to be greatest in Africa which is also the home to habitats for many key species.</p> <p>Read in the following CSV file which has annual population figures for each continent from 1800 through to 2100 (projections) and generate a line chart using Plotly Express visualising how each continents population has grown historically and is projected to grow into the future. This data is from Our World in Data with the source being Gapminder and the UN.</p> <p>You should use the <code>px.line()</code> function for this task and the <code>color</code> argument should be set to <code>\"Continent\"</code>.</p> <p>See what happens when you hover your cursor over each line.</p> <p>Can you spot Africa's projected rapid population growth through till 2100?</p>"},{"location":"notebooks/week-2_practice/#crop-yield-growth","title":"Crop yield growth\u00b6","text":"<p>Over the past few decades, as populations have grown so have crop yields. Crop yields are an indicator of agricultural productivity (i.e. how much food do we get from a unit area of land). Increasing the productivity of farms has reduced the need to expand the area under cultivation to meet demand from growing populations.</p> <p>Read in the following CSV file to plot the change in the average cereal crop yields (tonnes/ha) per continent since 1960 on a line chart. This data is downloaded from Our World in Data.</p> <p>You should use the <code>px.line()</code> function for this task and the <code>color</code> argument should be set to <code>\"Continent\"</code>.</p>"},{"location":"notebooks/week-2_practice/#projected-change-in-cropland-area","title":"Projected change in cropland area\u00b6","text":"<p>Williams et al. (2020) project the change in cropland area necessary to meet demand for food under a business as usual scenario (crop yields increasing following their historical rates of improvement and diets changing in line with projected income changes).</p> <p>Read in some of the data from the Williams et al. (2020) paper as used in Our World in Data (this data is downloaded from Our World in Data).</p> <p>The column headers are:</p> <ul> <li><code>bau_habitat_loss_all</code> - business as usual % change in habitat due to cropland expansion by 2050</li> <li><code>bau_change_pct</code> - business as usual % change in cropland area to meet demand by 2050</li> <li><code>yields_change_pct</code> - % change in cropland area to meet demand by 2050 when closing yield gaps</li> <li><code>diets_change_pct</code> - % change in cropland area to meet demand by 2050 by transitioning to healthier and lower meat diets</li> <li><code>waste_change_pct</code> - % change in cropland area to meet demand by 2050 by halving food waste</li> <li><code>yields_habitat_loss_all</code> - % change in habitat loss for all species due to cropland expansion to meet demand by 2050 when closing yield gaps</li> <li><code>yields_habitat_loss_mammals</code> - % change in habitat loss for mammals due to cropland expansion to meet demand by 2050 when closing yield gaps</li> </ul>"},{"location":"notebooks/week-2_practice/#projected-habitat-loss","title":"Projected habitat loss\u00b6","text":"<p>We can repeat the above visualisations but swap out <code>bau_change_pct</code> for <code>bau_habitat_loss_all</code> to visualise the effect of business as usual cropland expansion on habitat loss for all species.</p> <p>Can you make a bar plot visualising the percentage change in habitat for all species under the business as usual cropland expansion scenario?</p> <p>You should use the <code>px.bar()</code> function and set the argument <code>x</code> to <code>\"bau_habitat_loss_all\"</code>, <code>y</code> to <code>\"Continent\"</code>, and set <code>range_x</code> to <code>[-11,11]</code>. Use the pandas <code>DataFrame</code> <code>williams_2020_continents</code> for this task. Why are we setting the <code>range_x</code> argument to these values?</p> <p>You should see that a large proportion of projected habitat loss is going to occur on the African contient.</p>"},{"location":"notebooks/week-2_practice/#options-for-preventing-loss-of-wildlife-habitats","title":"Options for preventing loss of wildlife habitats\u00b6","text":"<p>The above visualisations tell us that growing populations increase demand for food. This extra demand can be met by i) increasing area under cultivation, or ii) increasing crop yields. Much of the projected population growth is expected to occur in Africa in the coming decades. The same continent where crop yields have remained persistently low and habitat is located for many species.</p> <p>However, what would cropland expansion and habitat loss scenarios in 2050 look like if crop yield gaps were closed. That is, bringing crop yields up to their potential limits under optimum management. Williams et al. (2020) looked into this scenario.</p> <p>Let's make two bar plots to visualise i) projected change in cropland area through to 2050 under a business as usual, and ii) projected change in cropland area through to 2050 under a closing yield gaps scenario. Use the <code>williams_2020_continents</code> <code>DataFrame</code> for this task. The projected change in cropland area when yield gaps are closed is the column <code>yields_change_pct</code>.</p>"},{"location":"notebooks/week-2_practice/#summary","title":"Summary\u00b6","text":"<p>These exercises demonstrate how we can use data visualisations and animations to combine datasets from a range of subject areas (population, agriculture, biodiversity), explore them, and identify solutions to complex problems.</p> <p>Here, these datasets highlight the challenge of meeting food demand while protecting the habitat of many species under business-as-usual agricultural and demographic scenarios. However, visualising this data spatially illustrates how closing crop yield gaps in Africa could realise many benefits simultaneously of meeting food demand, achieving food security and development goals, and protecting habitats from conversion to cropland.</p> <p>This exercise also demonstrates the importance of data visualisations as a communication tool (see the Our World in Data article: To protect the world\u2019s wildlife we must improve crop yields \u2013 especially across Africa as a good example of this ). Data visualisations can be used to summarise the complexity of big datasets to tell an important story to a range of audiences.</p> <p>Finally, think about all the datasets used in these exercises: global historical and projected population time-series, global crop yield time-series, data on cropland area, habitats for species, diets, and food waste. These datasets are derived from a range of underlying data sources and models. Considerable skills in (spatial) data processing and analysis are required to generate all these underlying datasets that permit analyses such as that in Williams et al. (2020). This is where the skills you are learning in this course come in.</p>"},{"location":"notebooks/week-3_1/","title":"Week 3 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-3_1\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-3_1\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-3_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-3_1\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-3_1\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-3_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport plotly.express as px\nimport plotly.io as pio\nimport pprint\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import pandas as pd import numpy as np import json import os import plotly.express as px import plotly.io as pio import pprint  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># path to csv file\nelev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")\nprint(\"path to elevation csv file:\", elev_csv_path)\n\n# open the file and read the first 1000 characters\nf = open(elev_csv_path, \"r\", encoding=\"utf-8\")\ndata = f.read(1000)\nf.close()\n\nprint(\"\")\nprint(\"The first 1000 characters of the csv file area:\")\nprint(data)\n</pre> # path to csv file elev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\") print(\"path to elevation csv file:\", elev_csv_path)  # open the file and read the first 1000 characters f = open(elev_csv_path, \"r\", encoding=\"utf-8\") data = f.read(1000) f.close()  print(\"\") print(\"The first 1000 characters of the csv file area:\") print(data) In\u00a0[\u00a0]: Copied! <pre>elev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")\nfile_stats = os.stat(elev_csv_path)\nprint(\"File Size in Bytes is:\",  file_stats.st_size)\n</pre> elev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\") file_stats = os.stat(elev_csv_path) print(\"File Size in Bytes is:\",  file_stats.st_size) In\u00a0[\u00a0]: Copied! <pre>os.getcwd()\n</pre> os.getcwd() <p>We can get a list of files and sub-directories within the current working directory by calling the <code>os.listdir()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>os.listdir()\n</pre> os.listdir() <p>A path describes the location of a file within the computer system's directory structure. We can create paths to files using the <code>os.path.join()</code> function. We pass in string data representing sub-directories and filenames and the <code>os.path.join()</code> function creates a file path.</p> <p>Let's get the file path for this notebook file: <code>week-3_1.ipynb</code>.</p> In\u00a0[\u00a0]: Copied! <pre>os.path.join(os.getcwd(), \"week-3_1.ipynb\")\n</pre> os.path.join(os.getcwd(), \"week-3_1.ipynb\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>print(os.path.join(os.getcwd(), \"data_lab-3-1\", \"week-3-bf66-wheat-yield-max-vi_sampled.geojson\"))\n</pre> <p>Is <code>data_lab-3_1</code> a directory or a file?</p> answer <p><code>data_lab-3_1</code> is a directory which can store files or sub-directories within the file system. You can print out the list of <code>data_lab-3_1</code>'s contents using the <code>os.listdir(os.path.join(os.getcwd(), \"data_lab-3_1\"))</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># open a connection to a csv file\n# path to csv file\nelev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")\n\n# open the file \nf = open(elev_csv_path, \"r\", encoding=\"utf-8\")\n</pre> # open a connection to a csv file # path to csv file elev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")  # open the file  f = open(elev_csv_path, \"r\", encoding=\"utf-8\") <p>We've opened a connection to a CSV file in read mode <code>\"r\"</code> and with a <code>utf-8</code> encoding. This has returned to us <code>f</code>, a variable pointing to a <code>file</code> object.</p> <p>What kind of object is <code>f</code> pointing to?</p> In\u00a0[\u00a0]: Copied! <pre>type(f)\n</pre> type(f) <p>Variable <code>f</code> is pointing at a <code>file</code> object, or, more specifically, an <code>_io.TextIOWrapper</code> type object. The <code>IO</code> stands for input / output which is another way of saying reading and writing data. The <code>Text</code> refers to the fact that <code>f</code> will be able to read data to string Python objects or write data from string Python objects to binary in files (i.e. it's reading in text mode). The conversion from string to binary and binary to string is determined by the encoding (e.g. UTF-8 or ASCII).</p> <p>The <code>file</code> object <code>f</code> is a connection to a file and it provides the methods to read (write) data from (to) this file via the connection.</p> In\u00a0[\u00a0]: Copied! <pre># read first 1000 characters from the csv file connected to by f\nchars_1000 = f.read(1000)\nprint(chars_1000)\n</pre> # read first 1000 characters from the csv file connected to by f chars_1000 = f.read(1000) print(chars_1000) In\u00a0[\u00a0]: Copied! <pre># read 10 lines of the csv file \nfor i in range(0, 10):\n    print(f.readline())\n</pre> # read 10 lines of the csv file  for i in range(0, 10):     print(f.readline()) <p>You might notice that the call to <code>readline()</code> does not start with the row of column headers. This is because the <code>file</code> object <code>f</code> keeps a record of a position in the file up to where it has read bytes from. The call to <code>readline()</code> will start reading lines of characters from the file where the previous call to <code>read(1000)</code> finished.</p> <p>We can use the <code>tell()</code> method to see a <code>file</code> object's current position in a file.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"current position in the file is {f.tell()}\")\n</pre> print(f\"current position in the file is {f.tell()}\") <p>The reason that <code>read()</code> or <code>readline()</code> provide the option to read data in <code>n</code> characters at a time or line by line is to help you avoid reading in more data than can fit in your computer's memory. If you had a large file and called <code>read()</code> on it, without specifying the number of characters to read, it could fill up your memory.</p> <p>As we're reading from the file in text mode, the binary data from the file should be converted to Python string objects. We can check this.</p> In\u00a0[\u00a0]: Copied! <pre>type(chars_1000)\n</pre> type(chars_1000) In\u00a0[\u00a0]: Copied! <pre>f2 = open(\"write_demo.csv\", \"w\", encoding=\"utf-8\")\n</pre> f2 = open(\"write_demo.csv\", \"w\", encoding=\"utf-8\") <p>Now, we can call the <code>write()</code> method of the <code>file</code> object <code>f2</code> and pass <code>write()</code> the variable <code>chars_1000</code> which points to 1000 characters stored as a string object. If the write is successful, it should return a number telling us how many characters were written (it should be 1000) and you should be able to see the file write_demo.csv in your current working directory.</p> In\u00a0[\u00a0]: Copied! <pre>f2.write(chars_1000)\n</pre> f2.write(chars_1000) In\u00a0[\u00a0]: Copied! <pre>f.close()\nf2.close()\n</pre> f.close() f2.close() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>f = open(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-terra-precip-monthly.csv\"), \"r\")\n         \nfor line in range(0, 20):\n    print(f.readline())\n\nf.close()\n</pre> In\u00a0[\u00a0]: Copied! <pre>png_file_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-low-res.png\")\n\nwith open(png_file_path, \"rb\") as f:\n    print(f.read(1)) # read and print the first byte\n    print(f.read(3)) # read and print the second, third, and fourth bytes\n</pre> png_file_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-low-res.png\")  with open(png_file_path, \"rb\") as f:     print(f.read(1)) # read and print the first byte     print(f.read(3)) # read and print the second, third, and fourth bytes <p>You will note that we opened a connection to the PNG file in <code>rb</code> mode. This indicates we are reading data in binary mode. Here, instead of reading in the data a character at a time (as we did when reading data in text mode) we are reading in <code>n</code> bytes of data.</p> <p>You will also note when printing the binary data that is read from the PNG file there is a <code>b</code> in front of the text. This indicates the data being printed is of bytes type.</p> <p>Finally, you will notice that we did not need to <code>close()</code> the file connection <code>f</code> as this is handled for us by working within the context of the <code>with</code> statement.</p> <p>This is what the data in the PNG file we have just been reading looks like.</p> <p></p> <p>Imagery (c) 2022 CNES | Airbus, Imagery (c) 2022 | Airbus, Landsat | Copernicus, Maxar Technologies, Map Data (c) 2022</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>with open(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\"), \"r\") as f:\n    print(f.read())\n</pre> In\u00a0[\u00a0]: Copied! <pre>canola_yield_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\"))\nprint(f\"The shape of the canola yield DataFrame is {canola_yield_df.shape}\")\n</pre> canola_yield_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\")) print(f\"The shape of the canola yield DataFrame is {canola_yield_df.shape}\") In\u00a0[\u00a0]: Copied! <pre>canola_yield_df.head()\n</pre> canola_yield_df.head() <p>Pandas provides a range of convenient functions for reading and writing data - you can find a list of them here.</p> In\u00a0[\u00a0]: Copied! <pre>%%HTML\n&lt;div style=\"padding:110.6% 0 0 0;position:relative;\"&gt;&lt;iframe src=\"https://player.vimeo.com/video/911479339?badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"faostat-csv\"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=\"https://player.vimeo.com/api/player.js\"&gt;&lt;/script&gt;\n</pre> %%HTML  In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>You will need to change the filename to match the file that you have downloaded.</p> <pre>df_faostat = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_1\", \"FAOSTAT_data_en_2-9-2024.csv\"))\ndf_faostat.head()\n</pre> In\u00a0[\u00a0]: Copied! <pre>canola_yield_df.to_parquet(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\"))\n</pre> canola_yield_df.to_parquet(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\")) <p>Let's compare the size of the CSV file storing the canola yield data and the parquet file we just saved. The parquet file should be much smaller.</p> In\u00a0[\u00a0]: Copied! <pre>file_stats_csv = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\"))\nprint(\"CSV file size in bytes is:\",  file_stats_csv.st_size)\nfile_stats_parquet = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\"))\nprint(\"Parquet file size in bytes is:\",  file_stats_parquet.st_size)\n</pre> file_stats_csv = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\")) print(\"CSV file size in bytes is:\",  file_stats_csv.st_size) file_stats_parquet = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\")) print(\"Parquet file size in bytes is:\",  file_stats_parquet.st_size)"},{"location":"notebooks/week-3_1/#data-io-and-file-formats","title":"Data I/O and file formats\u00b6","text":"<p>In this lab we will write Python programs that can read crop yield data collected from harvesters and satellite images of the same field into data structures that we can analyse and visualise.</p> <p>A solid understanding of how to read and write different types of data from and to files is a key skill for data analysis. This week's lab will build these skills and provide an introduction to:</p> <ul> <li>files, directories, and data storage</li> <li>reading and writing files in Python</li> <li>tabular, image, and geospatial file formats</li> <li>specialist file formats for the web, big data, and cloud computing</li> <li>selecting different data formats for specific analysis or storage tasks</li> </ul>"},{"location":"notebooks/week-3_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-3_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-3_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-3_1/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-3_1/#data-io","title":"Data I/O\u00b6","text":"<p>Data analysis tasks involve reading data stored in files on disks, servers in the cloud, or recorded by sensors. Also, we need to save the results of our analysis or datasets we have generated to files.</p> <p>There are a range of data types (e.g. string / text, numeric, datetime) and ways of characterising data such as tabular data, images and arrays, and spatial and non-spatial data. This necessitates storing data with different file formats. It's important to be able to read and write data from and to different file formats into and out of Python data structures that we can analyse in our programs.</p> <p>There are costs involved in storing and transferring data. These can be time costs associated with the time taken to read data from disk into the computer's memory or transferring data from one computer to another over a network. Or, they can be financial costs associated with storing the data (the cost of hard drives increases with storage capacity and cloud storage providers charge by the byte).</p> <p>The term big data refers to the increasing volume, variety, and velocity of data. Larger and more diverse datasets are being generated more quickly. To be able to handle big data it is important to select appropriate file formats for efficient storage and reading / writing (or input / output - I/O).</p>"},{"location":"notebooks/week-3_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-3_1/#a-python-program-to-read-a-file","title":"A Python program to read a file\u00b6","text":"<p>Let's start with a simple program to open a CSV file, read some data from it into memory, and then close the connection to the file.</p>"},{"location":"notebooks/week-3_1/#files-and-io","title":"Files and I/O\u00b6","text":""},{"location":"notebooks/week-3_1/#files","title":"Files\u00b6","text":"<p>A file is data that is stored on a disk. Data in files is stored as a sequence bytes in binary format (values of zero or one).</p> <p></p>"},{"location":"notebooks/week-3_1/#binary","title":"Binary\u00b6","text":"<p>A binary number is represented using only the digits 1 or 0. The binary number system is a base-2 number system (as it has only two symbols).</p> <p>The decimal number <code>0</code> in binary is <code>0</code>:</p> <p>$0 = (0 \\cdot 2^{0})$</p> <p>The decimal number <code>1</code> in binary is <code>1</code>:</p> <p>$1 = (1 \\cdot 2^{0})$</p> <p>The decimal number <code>2</code> in binary is <code>10</code>:</p> <p>$2 = (1 \\cdot 2^{1}) + (0 \\cdot 2^{0})$</p> <p>The decimal number <code>3</code> in binary is <code>11</code>:</p> <p>$3 = (1 \\cdot 2^{1}) + (1 \\cdot 2^{0})$</p> <p>The decimal number <code>4</code> in binary is <code>100</code>:</p> <p>$4 = (1 \\cdot 2^{2}) + (0 \\cdot 2^{1}) + (0 \\cdot 2^{0})$</p> <p>You don't need to know the details of the binary number system here, but the key things to take away are:</p> <ul> <li>numbers are stored in binary using the digits 1 or 0</li> <li>larger numbers require more binary digits</li> <li>larger numbers, therefore, require more storage space</li> </ul> <p>There are plenty of resources online to learn more about number systems. This is a short article on The History of Numbers.</p> <p>Bits</p> <p>A single binary digit is a bit. Looking at the pattern above we can store the numbers 0 and 1 using a 1-bit binary number. We can store the numbers 2 and 3 using a 2-bit binary number. We can store the number 4 using a 3-bit binary number. For every extra bit we double the numbers that can be stored in binary.</p> <ul> <li>1 bits = 2 numbers (0 and 1)</li> <li>2 bits = 4 numbers (0 to 3)</li> <li>3 bits = 8 numbers (0 to 7)</li> <li>4 bits = 16 numbers (0 to 15)</li> <li>5 bits = 32 numbers (0 to 31)</li> <li>6 bits = 64 numbers (0 to 63)</li> <li>7 bits = 128 numbers (0 to 128)</li> <li>8 bits = 256 numbers (0 to 255)</li> </ul> <p>Bytes</p> <p>A byte is an 8-bit binary number. With a single byte we can represent 256 different numbers. Computer storage is measured in bytes:</p> <ul> <li>1 Kilobyte (KB) is about 1,000 bytes.</li> <li>1 Megabyte (MB) is about 1,000,000 bytes.</li> <li>1 Gigabyte (GB) is about 1,000,000,000 bytes.</li> </ul> <p>A greyscale image file can store each pixel's colour as an 8-bit number or as a single byte. Black is represented as the number 0 (00000000), white is the number 255 (11111111), and shades of grey are intermediate numbers (00000001 to 11111110).</p> <p>If this greyscale image has 100x100 pixels (10,000 pixels), how many bytes of storage does this image require?</p> <p>$10000 bytes = 10000 pixels \\cdot 1 byte$ as each pixel requires 1 byte of storage.</p> <p>Generally, a common text character such as upper and lower case letters (A-Z, a-z) and symbols (!, @, # etc.) requires a byte of storage. The CSV file we read above stores text characters. We can use the <code>os.stats()</code> function to inspect the stats of this file in storage. Let's pass the path to the <code>week-3-bf66-elevation.csv</code> file into the <code>os.stats()</code> function and see how many bytes are required to store this file.</p>"},{"location":"notebooks/week-3_1/#file-formats","title":"File formats\u00b6","text":"<p>A file format describes how data is encoded as binary sequences in files (sequences of 1 and 0 digits). The filename's extension indicates the file format used (e.g. .jpg is a JPEG file, .tif is a TIFF or GeoTIFF file, .csv is a CSV file). Some file formats also include a header or magic number inside the file that indicates what the file format is. The header can also include some metadata information about the file.</p> <p></p> <p>A Portable Network Graphics (PNG) file is a common file format for storing image data. It is identified by a .png ending and consists of sections of bytes on disk arranged as:</p> <ol> <li>PNG signature including a magic number for the start of the file and a PNG file identifier.</li> <li>Image header with image metadata such as the size of the image.</li> <li>Image data.</li> <li>Image end to indicate the end of the PNG file on disk.</li> </ol> <p></p> <p>Different file formats encode data in different ways, and, thus, have different strengths and weaknesses. Some file formats prioritise efficient storage (compression) of data on disk (small file sizes and quick transfer), other prioritise quick read and writing of data, and others prioritise cross-platform compatibility or interpretation.</p> <p>The various file formats for storing vector geospatial data provide a good illustration of how different formats store the same data but in ways that are better suited for different applications and uses. This is discussed here and here.</p> <p>Many of you will have used shapefiles as a format for storing vector geospatial data. A strength of shapefiles is the range of software applications that can read and write data from and to them. However, they have drawbacks which mean they're not suited to some use cases:</p> <ul> <li>Multifile format which makes data handling harder (you need a .shp, .shx, .prj, and .dbf file).</li> <li>File size is limited to 2 GB which is prohibits their use for storing large datasets.</li> <li>Attribute names are limited to 10 characters which can preclude using descriptive attribute and column names.</li> <li>Each shapefile can only store one type of geometry (point or line or polygon) which prohibits representing complex geographic features in a single file.</li> <li>No null value which can introduce complications for handling missing data.</li> <li>Data types are limited to 256 characters which precludes storing large numbers / text strings.</li> </ul> <p>It is important to be aware of the characteristics of particular file formats and what their limits or benefits mean for your analysis. A simple example of why this is important: we've processed several harvester yield datasets from different fields into a single dataset and want to save this dataset to file. If this dataset is larger than 2GB and we try and save to a shapefile there will be data loss.</p>"},{"location":"notebooks/week-3_1/#directories-and-file-systems","title":"Directories and file systems\u00b6","text":"<p>Files are organised within a hierarchy of directories and sub-directories (or folders) in a computer system. We're working in a Linux environment so the directory hierachy starts at the root denoted by <code>/</code>. Sub-directories are separated by <code>/</code>.</p> <p>A program has a current working directory which is its current location within the directory hierarchy.</p> <p>We can get the current working directory by calling the <code>os.getcwd()</code> function.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>In the directory <code>data_lab-3_1</code> there is a file named <code>week-3-bf66-wheat-yield-max-vi_sampled.geojson</code>. Can you create a file path for this file using <code>os.path.join()</code> and print the path on the display?</p>"},{"location":"notebooks/week-3_1/#files-in-python","title":"Files in Python\u00b6","text":"<p>In Python, it is possible to read and write files in text and binary modes.</p>"},{"location":"notebooks/week-3_1/#text-mode","title":"Text mode\u00b6","text":"<p>Text mode involves reading and writing string data from and to the file.</p> <p>A text file contains encoded characters. ASCII and Unicode are character sets that define how characters (e.g. 1, 2, 3, 66, A, b, !) are encoded in binary (sequences of 1 and 0 digits) in a file.</p> <p>A character set translates characters into numbers and an encoding translates numbers into binary.</p>"},{"location":"notebooks/week-3_1/#ascii","title":"ASCII\u00b6","text":"<p>ASCII stands for the American Standard Code for Information Interchange and has encodings for 128 English characters, numbers, and some special characters. ASCII characters are encoded using 7-bits. You can see the full ASCII character set here.</p> <p>In ASCII, uppercase G is represented by the number 71, uppercase I is represented by the number 73, and uppcase S is represented by the number 83.</p> <p>Thus, GIS in ASCII is written as <code>71 73 83</code> in its numeric representation and <code>01000111 01001001 01010011</code> in its binary representation (i.e. how it would be stored in a file).</p>"},{"location":"notebooks/week-3_1/#unicode","title":"Unicode\u00b6","text":"<p>Unicode is a more modern and comprehensive character set of text symbols covering modern and ancient languages. Common encodings of the Unicode character set are UTF-8 and UTF-16. The Unicode character set includes over 1,000,000 characters and aims to be a universal system for representing and storing text in computer systems.</p> <p>Unicode characters can be encoded in UTF-8 using one to four bytes. More common symbols (e.g. the ASCII character set) are encoded using one byte for efficient storage.</p> <p>The Python docs suggest UTF-8 is the modern de-facto standard so it is often the default encoding or a good one to choose if you are not sure how your data is encoded.</p>"},{"location":"notebooks/week-3_1/#binary-mode","title":"Binary mode\u00b6","text":"<p>Reading files in binary mode does not assume that specific bytes represent human readable characters. When reading files in binary mode, sequences of bytes are read from the file into Python bytes objects in memory. Images such as JPEG files would be read in binary mode.</p> <p>To be clear, both text and binary data is stored in binary format on disks. However, when reading in text data the binary data on disk is converted to text characters based on the encoding scheme used and read into string objects. When reading data in binary mode the binary data is read straight into memory as bytes objects. Reading and writing data in text mode will be slower because of the encoding overhead.</p>"},{"location":"notebooks/week-3_1/#opening-files","title":"Opening files\u00b6","text":"<p>The <code>open()</code> function opens a connection to a file on disk, or creates a new file if it does not exist, and returns a <code>file</code> connection object.</p> <p>Typically, <code>open()</code> is called by specifying a filename, mode, and encoding as arguments: <code>open(filename, mode, encoding)</code>.</p> <ul> <li>filename: the path and filename of the file to be opened.</li> <li>mode: the mode to open the connection to file in. To open files in text mode use <code>r</code> for read only, <code>w</code> for write only, <code>a</code> for appending data to the file, <code>r+</code> for reading and writing. To open files in binary mode use <code>rb</code> for read only, <code>wb</code> for write only, and <code>rb+</code> for reading and writing.</li> <li>the default is to open connections in text mode - be careful if you are opening a connection to a file that is not text data.</li> <li>encoding: the encoding of the data in the file.</li> </ul> <p>Let's open up a connection to a CSV file and explore the file object that's returned.</p>"},{"location":"notebooks/week-3_1/#reading-files","title":"Reading files\u00b6","text":"<p>A read operation will copy bytes from the file on disk to the computer's memory. The <code>file</code> object,<code>f</code>, has the <code>read()</code> and <code>readline()</code> methods.</p> <p>The <code>read(size=n)</code> method in text mode will read <code>n</code> characters from the file. If <code>n</code> is omitted or is a negative number the <code>read()</code> function will read all of the characters in the file.</p> <p>The <code>readline()</code> will read until a newline in the text file. Text files have newline characters that denote the end of a line. On Windows the newline character is <code>\\r\\n</code> and on Linux / MacOS it is <code>\\n</code>.</p> <p>Let's test out the <code>read()</code> and <code>readline()</code> methods.</p>"},{"location":"notebooks/week-3_1/#writing-files","title":"Writing files\u00b6","text":"<p>The <code>write()</code> method of the <code>file</code> object writes Python objects (e.g. strings) to files. The data will then be stored on disk in a specified format until it is read again by another program.</p> <p>When writing data in text mode Python string objects (characters) are encoded (e.g. using ASCII or UTF-8) and stored as bytes on the disk.</p> <p>Let's demonstrate a write operation by writing the 1000 characters stored in <code>chars_1000</code> to a new file. First, we need to open a connection to the new file in write mode and specify an encoding.</p>"},{"location":"notebooks/week-3_1/#closing-files","title":"Closing files\u00b6","text":"<p>After you have finished reading or writing data from and to the file, it is important to <code>close()</code> the connection to the file. The <code>file</code> object's <code>close()</code> method does this.</p> <p>Once the <code>close()</code> method has been called on a <code>file</code> object it is no longer possible to read or write data from and to the file. This is important to prevent accidental data loss or corruption.</p> <p>We have two open file connections, let's close them.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you open a connection to the file <code>\"week-3-bf66-terra-precip-monthly.csv\"</code>, read and print the first 20 lines, and close the connection the file?</p>"},{"location":"notebooks/week-3_1/#context-managers","title":"Context managers\u00b6","text":"<p>Context managers - the \"correct\" way to read and write data from and to files in Python.</p> <p>Above we have gone through the process of opening connections to files, reading and writing data, and closing connections to files. However, there are lots of moving parts to this approach as you need to keep track of which connections are open to which files and to ensure you close connections when they are no longer needed. As applications grow and work with more data this can require handling many file connections which adds complexity and increases potential for mistakes / errors. Such mistakes / errors can result in data loss, corrupting files, or reduced security if file connections to private data are leaked.</p> <p>There are two \"better\" ways to read and write data in Python:</p> <ol> <li>using context managers.</li> <li>using methods and functions provided by packages (e.g. pandas <code>read_csv()</code> function).</li> </ol> <p>Let's open a file and read data from it using a context manager. A context manager ensures that connections to files are properly closed without explicitly having to code for it.</p> <p>To create a context, use the <code>with</code> statement.</p> <p>We've already demonstrated how to open, read, and write data in text mode from a CSV file. Let's use a context manager to demonstrate how to work with files in binary mode. Above we introduced the PNG file format for storing image data. We have a PNG file showing a Google Earth aerial image of the field we're working in. The first few bytes of the PNG file should be the PNG signature including a magic number for the start of the file and an ASCII representation of the letters PNG.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you read in the file <code>\"week-3-bf66-elevation.csv\"</code> using a context manager?</p>"},{"location":"notebooks/week-3_1/#pandas-io","title":"Pandas I/O\u00b6","text":"<p>Many Python packages provide functions and methods to read and write data that safely open and close connections to files.</p> <p>The pandas <code>read_csv()</code> function reads CSV data from disk into a pandas <code>DataFrame</code> object in a single line of code without us needing to explicitly close a connection to the file.</p> <p>Let's read some canola yield data collected by a harvester and stored as a CSV file into our program using the pandas <code>read_csv()</code> function.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Go to the FAOSTAT Data Crop and Livestock page and pick a country, element (e.g. yield, area, production), item (e.g. crop - maize), and selection years. Download the data as a CSV file. Then, using <code>pd.read_csv()</code> read the data into a <code>DataFrame</code> object referenced by the variable <code>df_faostat</code>. Print the <code>head()</code> of the <code>DataFrame</code>.</p> <p>Hint: you will need to move the CSV file you download from FOASTAT into your working or data directory.</p>"},{"location":"notebooks/week-3_1/#parquet-files","title":"Parquet files\u00b6","text":"<p>So far we have been working with data in CSV format. The CSV format has many strengths for storing tabular data including:</p> <ul> <li>many software applications provide tools to read and write CSV data.</li> <li>the data structure is relatively intuitive with human readable characters encoded in binary, data values (fields) comprise binary representations of characters and are separated by comma symbols (hence the name), and rows (records) are separated by newline symbols.</li> <li>flexibility to choose different encodings of the text data.</li> </ul> <p>However, CSV files require that each data value is stored even if there is lots of repetition. For example, if there is a column that denotes the field name or id, for every row in the table the field name or id value would be repeated. As datasets get large, this can cause CSV files to increase in size which has subsequent storage costs.</p> <p>An alternative file format for storing tabular is parquet. Parquet files are optimised for storage. This provides more efficient use of hard drives, cheaper cloud storage costs, and quicker transmission of data over the internet.</p> <p>Parquet files have several optimisations for storing tabular data. Whereas CSV files are based around row storage, parquet files are based on column storage.</p> <p></p> <p>Parquet files can optimise storage of tabular data using run length encoding and dictionary encoding.</p> <p>Run length encoding is useful for reducing storage when there are runs of the same value within a column. For example, in the <code>canola_yield_df</code> <code>DataFrame</code> the values in the Crop column repeat. Instead of storing every value, we can store the two values: the value that repeats and the number of repetitions in the column (e.g. <code>(5, 80755)</code> - instead of storing the number 5 80,755 times as would be the case in a CSV file we can just store two numbers 5 and 80,755). Run length encoding is not suited for CSV files as the data is stored by row, and, often, within a row you'll have data of different types (e.g. string / text, numeric, and dates). This structure doesn't lend itself to encoding runs of the same value - repitition in tabular data generally runs down columns not across rows.</p> <p>Dictionary encoding is useful when we need to store large values (e.g. long names or large numbers). Instead of writing the large value repeatedly in the file a smaller value is written and there is a dictionary which acts as a look up table to correspond the small value to the actual large value. This means the large value only needs to be stored once.</p> <p>For a single field, we have 80,755 records (rows) in the <code>DataFrame</code>. If we scaled up this analysis to work with data collected from harvesters across many fields in Western Australia we would quickly accumulate a large volume of data. At some stage we will hit issues with storing and transferring the data and it might be appropriate to switch from CSV to parquet files for data storage.</p>"},{"location":"notebooks/week-3_2/","title":"Week 3 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-3_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-3_2\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-3_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-3_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-3_2\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-3_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import os\nimport pprint\nimport rioxarray as rxr\nimport xarray as xr\nimport pandas as pd\nimport geopandas as gpd\n</pre> import os import pprint import rioxarray as rxr import xarray as xr import pandas as pd import geopandas as gpd In\u00a0[\u00a0]: Copied! <pre># path to the GeoTIFF file\ns2_summer_path = os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-s2-summer-2020.tif\")\n</pre> # path to the GeoTIFF file s2_summer_path = os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-s2-summer-2020.tif\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_summer = rxr.open_rasterio(s2_summer_path)\n</pre> <p>Now we've opened the GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code> as an <code>xarray.DataArray</code> referenced by the variable <code>s2_summer</code>, let's explore the dataset using the attributes and methods of the <code>xarray.DataArray</code> object.</p> <p>What are the <code>dims</code> for the <code>xarray.DataArray</code> object storing the raster values from the GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p><code>xarray.DataArray</code> objects store descriptive dimension labels as a tuple under the <code>dims</code> attribute. These dimension labels are more descriptive and informative than the axis numbering of NumPy <code>ndarray</code>s.</p> <pre>s2_summer.dims\n</pre> <p>What is the size, in terms of the number of elements along each dimension, of the <code>xarray.DataArray</code> object storing the raster values from the GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>We can return the raster values as a NumPy <code>ndarray</code> object and then access the <code>shape</code> property of the <code>ndarray</code>.</p> <pre>s2_summer.values.shape\n</pre> In\u00a0[\u00a0]: Copied! <pre>s2_summer.rio.crs\n</pre> s2_summer.rio.crs <p>We can see the CRS of the dataset is EPSG 4326, which is representing the raster data using latitude and longitude with the WGS84 ellipsoid and datum. It can be tricky to measure distance or compute area, as the data's positional units are in decimal degrees on 3D surface as opposed to metric units on a 2D surface. Therefore, often, we want to reproject raster data a projected CRS on a 2D surface.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_summer_utm = s2_summer.rio.reproject(\"EPSG:32750\")\n# check it reprojected OK\ns2_summer_utm.rio.crs\n</pre> <p>Above, we knew the EPSG code for the CRS we wished to reproject our <code>xarray.DataArray</code> object to. What happens if we don't have this information? Can you look in the rio accessor docs and see if you can spot a method, and implement it, that will estimate a suitable UTM CRS for an <code>xarray.DataArray</code> object?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>est_utm = s2_summer.rio.estimate_utm_crs()\nest_utm\n</pre> In\u00a0[\u00a0]: Copied! <pre>era5 = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"era-5-western-australia-monthly-2023.nc\"))\nera5\n</pre> era5 = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"era-5-western-australia-monthly-2023.nc\")) era5 <p>You might spot that <code>era5</code> references a <code>xarray.Dataset</code> object with two data variables: <code>tp</code> and <code>t2m</code>. Each of <code>tp</code> and <code>t2m</code> are <code>xarray.DataArray</code> objects that store precipitation and air temperature at 2 m respectively. We can access each of these <code>xarray.DataArray</code> objects by their name. For example, to retrieve the temperature data as a <code>xarray.DataArray</code> we use dot notation to access the <code>t2m</code> variable:</p> In\u00a0[\u00a0]: Copied! <pre>era5.t2m\n</pre> era5.t2m <p>We can select a 2D slice of the multidimensional array of temperature values to visualise. <code>xarray.DataArray</code> objects have a <code>sel()</code> method which lets us conditionally select elements from the array using the <code>coordinates</code>. Let's slice out the 2D array corresponding to the time <code>\"2023-12-01T12:00:00\"</code>. We also need to select the <code>expver=5</code>, this distinguishes between the initial release of the data (<code>expver=5</code>) and a validated release (<code>expver=1</code>). Let's use <code>5</code> here.</p> <p>Finally, let's use <code>plot.imshow()</code> to visualise temperature values across Western Australia for a single time slice.</p> In\u00a0[\u00a0]: Copied! <pre>era5.t2m.sel(time=\"2023-12-01T12:00:00\", expver=5).plot.imshow()\n</pre> era5.t2m.sel(time=\"2023-12-01T12:00:00\", expver=5).plot.imshow() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>era5.tp.sel(time=\"2023-06-01T18:00:00\", expver=1).plot.imshow(cmap=\"Blues\")\n</pre> In\u00a0[\u00a0]: Copied! <pre>cmip6_wa = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"nuist_cmip6_wa_2100_tasmax.zarr\"), engine=\"zarr\")\nprint(cmip6_wa)\ncmip6_wa.tasmax.sel(time=\"2100-01-01T12:00:00\").plot()\n</pre> cmip6_wa = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"nuist_cmip6_wa_2100_tasmax.zarr\"), engine=\"zarr\") print(cmip6_wa) cmip6_wa.tasmax.sel(time=\"2100-01-01T12:00:00\").plot() answer <pre>cmip6_wa = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"nuist_cmip6_wa_2100_tasmax.zarr\"), engine=\"zarr\")\nprint(cmip6_wa)\ncmip6_wa.tasmax.sel(time=\"2100-01-01T12:00:00\").plot()\n</pre> In\u00a0[\u00a0]: Copied! <pre>elev_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.csv\"))\nelev_df.head()\n</pre> elev_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.csv\")) elev_df.head() <p>Now, let's use the longtitude and latitude columns in the <code>DataFrame</code> to convert the elevation data into a GeoPandas <code>GeoDataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Convert the elevation data to a spatial format\npoints = gpd.points_from_xy(elev_df[\"Lon\"], elev_df[\"Lat\"], crs=\"EPSG:4326\")\nprint(f\"points is of type {type(points)}\")\n\nelev_gdf = gpd.GeoDataFrame(elev_df, geometry=points)\nprint(f\"elev_gdf is of type {type(elev_gdf)}\")\n\nelev_gdf.head()\n</pre> # Convert the elevation data to a spatial format points = gpd.points_from_xy(elev_df[\"Lon\"], elev_df[\"Lat\"], crs=\"EPSG:4326\") print(f\"points is of type {type(points)}\")  elev_gdf = gpd.GeoDataFrame(elev_df, geometry=points) print(f\"elev_gdf is of type {type(elev_gdf)}\")  elev_gdf.head() <p>Let's get the first two rows of the <code>GeoDataFrame</code> and convert them to GeoJSON format. <code>GeoDataFrame</code>s have a <code>to_json()</code> method which can be used to convert the data in the <code>GeoDataFrame</code> into a string object in GeoJSON format.</p> In\u00a0[\u00a0]: Copied! <pre># Get the first two rows of the elevation GeoDataFrame and convert to GeoJSON\nelev_gdf_2 = elev_gdf.iloc[0:2, :]\nelev_gdf_2\n</pre> # Get the first two rows of the elevation GeoDataFrame and convert to GeoJSON elev_gdf_2 = elev_gdf.iloc[0:2, :] elev_gdf_2 In\u00a0[\u00a0]: Copied! <pre>elev_geojson_2 = elev_gdf_2.to_json()\nprint(f\"The GeoJSON data is stored as a {type(elev_geojson_2)} type object\")\nprint(\"\")\npprint.pprint(elev_geojson_2)\n</pre> elev_geojson_2 = elev_gdf_2.to_json() print(f\"The GeoJSON data is stored as a {type(elev_geojson_2)} type object\") print(\"\") pprint.pprint(elev_geojson_2) <p>In Python, the GeoJSON data that we have generated from our <code>GeoDataFrame</code> is stored as a string object. GeoJSON (and JSON) is a text-based data format similar to CSV files. However, unlike the CSV format where data has a tabular structure with records arranged by row the GeoJSON data is based around nested objects of key:value pairs.</p> <p>As we have subsetted the first two rows of our <code>GeoDataFrame</code> and converted them to GeoJSON we have generated a <code>FeatureCollection</code> object with two <code>Feature</code>s.</p> <p>Each row in the <code>GeoDataFrame</code> is converted to a <code>Feature</code> and each <code>Feature</code> has the column values per row stored in a <code>properties</code> object - these are the non-spatial attributes associated with each <code>Point</code> feature. The spatial information is stored in a <code>geometry</code> object which contains two key:value pairs. The value associated with the <code>type</code> key tells us this is a <code>Point</code> geometry and the array value associated with <code>coordinates</code> key defines the location.</p> <p>Compare the tabular display of the <code>GeoDataFrame</code> to the print of the GeoJSON to see how the non-spatial and spatial information in the table structure is converted to the GeoJSON nested format.</p> <p>We can save a <code>GeoDataFrame</code> to GeoJSON using the <code>GeoDataFrame</code>'s <code>to_file()</code> method and setting the driver argument to GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre># Save the elevation GeoDataFrame to a GeoJSON file\nelev_gdf.to_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\"), driver=\"GeoJSON\")\n</pre> # Save the elevation GeoDataFrame to a GeoJSON file elev_gdf.to_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\"), driver=\"GeoJSON\") <p>Check the GeoJSON file has saved to the directory specified. As it is text data, if you click on it you should be able to inspect its format in a text editor.</p> <p>You saved the elevation data to a GeoJSON file at this path: <code>os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\")</code>.</p> <p>Head to the GeoPandas documentation and look up how to read files into <code>GeoDataFrame</code> objects. Read the elevation.geojson file into a <code>GeoDataFrame</code> referenced by the variable <code>elev_from_file</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>elev_from_file = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\"))\nelev_from_file.head()\n</pre> <p>Note, this answer assumes GeoPandas has been imported as gpd.</p> <p>Write the data referenced by <code>elev_from_file</code> to disk as a GeoPackage.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>elev_from_file.to_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.gpkg\"), driver=\"GPKG\")\n</pre>"},{"location":"notebooks/week-3_2/#geospatial-data-io","title":"Geospatial data I/O\u00b6","text":"<p>Data analysis tasks involve reading geospatial data stored in files on disks, servers in the cloud, or recorded by sensors. Also, we need to save the results of our analysis or datasets we have generated to files.</p> <p>This lab will introduce:</p> <ul> <li>geospatial data file formats</li> <li>techniques for reading and writing from geospatial data from and to files</li> <li>Python data structures for representing vector and raster data</li> </ul>"},{"location":"notebooks/week-3_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-3_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-3_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-3_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-3_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-3_2/#raster-data","title":"Raster data\u00b6","text":"<p>Raster data represents geographic features and variables (e.g. elevation, reflectance in UAV images) as a grid of values (pixels).</p>"},{"location":"notebooks/week-3_2/#geotiff","title":"GeoTIFF\u00b6","text":"<p>Many geospatial datasets are based on the raster data model where values are assigned to pixels and pixels represent locations on the Earth's surface.</p> <p>A common source of raster data are remote sensing images captured by sensors on uncrewed aerial vehicles, aircraft, or satellites. Optical remote sensing images store the measured reflectance of light off the Earth's land surface in different wavelenghts. Raster remote sensing images are often stored using the GeoTIFF format.</p> <p>A GeoTIFF file is based on the Tagged Image File Format (or .tiff file) which is a general format for storing image data. A TIFF file comprises:</p> <ul> <li>a TIFF header which includes 8 bytes that tell us that the file is in TIFF format and where in the file (what byte number / byte offset from 0) the first Image File Directory is stored.</li> <li>Image File Directories which contains image metadata, a pointer to where the image data is in the file (what byte number / byte offset from 0), and the location of the next Image File Directory if there is more than one image stored in the TIFF file. Metadata is stored as fields which comprise a TIFF tag and it's corresponding value.</li> <li>Image Data - the values associated with each pixel in the image. A single TIFF file can store multiple images.</li> </ul> <p></p> <p>GeoTIFF files include extra information (metadata) as tags which describe the coordinate reference system (CRS) of the image data (i.e. where on the Earth's surface the image data corresponds to), spatial resolution, no data values, and various other configurations described here.</p> <p>GeoTIFF files can store multiple images (i.e. raster layers) in a single file. This makes them well suited for storing remote sensing image data where each raster layer corresponds to measured reflectance in a particular wavelength.</p> <p>We can use functions provided by the rioxarray package to read and write raster data in GeoTIFF format into Python programs.</p> <p>Using rioxarray's <code>open_rasterio()</code> method we can read raster data stored on disk as a GeoTIFF file into a <code>xarray.Dataset</code> or <code>xarray.DataArray</code> object in our Python program.</p> <p>The GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code> in the <code>data_lab-3-2</code> stores remote sensing data covering a field in Western Australia. The remote sensing data was captured by the European Space Agency's Sentinel-2 satellite (10 m spatial resolution for red, green, blue, and near infrared bands). This is the path to the GeoTIFF file:</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>This is the rioxarray's <code>open_rasterio()</code> docs. Can you use this function to read in the GeoTIFF file referenced by <code>s2_summer_path</code> to a <code>xarray.DataArray</code>?</p> <p>Make sure the data is read to an <code>xarray.DataArray</code> object referenced by the variable name <code>s2_summer</code>.</p>"},{"location":"notebooks/week-3_2/#xarray-recap","title":"Xarray recap\u00b6","text":"These are notes repeated from week 2 that provide an overview of xarray and its classes for storing multidimensional arrays as objects in Python programs. (click the arrow to display notes). <p>Xarray is a Python package that builds on top of NumPy's array-based data structures, but provides extra tools and functions that are useful for working with geospatial and Earth Science datasets. For example, <code>xarray.DataArray</code> data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values.</p> <p><code>xarray</code> also provides convenient functions for reading raster data from geospatial data files on disk into memory as <code>xarray.DataArray</code> objects which we can use in our Python programs while retaining geographic and temporal information about the raster values stored in the array.</p> <p>Specifically, while a NumPy <code>ndarray</code> stores just the raster values and has some properties such as the <code>shape</code> (number of elements along each axis) and <code>ndim</code> (the dimensions of the array) it does not explicitly store any geospatial, temporal, or other geographic metadata. <code>xarray</code> solves this problem by reading raster data into an <code>xarray.DataArray</code> object with:</p> <ul> <li><code>values</code>: the multidimensional array of raster values</li> <li><code>dims</code>: a list of names for the dimensions of the array (e.g. instead axis 0 describing the 0th (row) dimension of an array that dimension can have a descriptive label such as longitude)</li> <li><code>coordinates</code>: a <code>list</code> of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)</li> <li><code>attrs</code>: a <code>dict</code> of metadata attributes describing the dataset</li> </ul> <p><code>xarray.DataArray</code> objects can be stored within a larger container called <code>xarray.Dataset</code>. An <code>xarray.Dataset</code> can store many <code>xarray.DataArray</code> objects that share <code>dims</code> and <code>coordinates</code>. This is useful if you have different arrays of different <code>Variables</code> that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single <code>xarray.Dataset</code>).</p> <p></p> <p>Why is <code>xarray</code> useful for geospatial data?</p> <ul> <li>The <code>dims</code> and <code>coordinates</code> of an <code>xarray.DataArray</code> mean we can subset values from an array using latitude, longitude, time, or whatever a coordinate describes; we're not just restricted to subsetting values based on their index location within an array</li> <li><code>xarray.Dataset</code> objects provide a container to store multidimensional arrays (e.g. many variables and time points) that are common in geography, Earth Sciences, meteorology, and agriculture. For example, multispectral satellite images of the same location over time; arrays of different meteorological variables)</li> <li>useful functions for reading, analysing and visualising raster or array-like geospatial data that are common across many spatial data science workflows</li> </ul>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>To answer these questions, you will need to look things up in the xarray docs. The user guide on Data Structures and the <code>xarray.DataArray</code> API reference will be useful.</p> <p>How do <code>xarray.DataArray</code> objects store raster data in Python programs?</p> answer <p><code>xarray.DataArray</code> objects store raster values in a multidimensional NumPy <code>ndarray</code> or array-like <code>values</code> property:</p> <pre>s2_summer.values\n</pre>"},{"location":"notebooks/week-3_2/#rio-accessor","title":"rio accessor\u00b6","text":"<p>The <code>rio</code> accessor from the rioxarray package extends the <code>xarray.DataArray</code> class with extra properties and methods that are useful for retrieving information about an <code>xarray.DataArray</code> object when it contains raster geospatial data.</p> <p>For example, the <code>rio.crs</code> property will return the coordinate reference system (CRS) of the raster data in the <code>xarray.DataArray</code> object which was retrieved when reading the data from the GeoTIFF file:</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you use the rio.reproject() method to reproject the raster data to <code>\"EPSG:32750\"</code> (UTM Zone 50S)?</p> <p>Save the reprojected <code>xarray.DataArray</code> object to a variable referenced by <code>s2_summer_utm</code>.</p>"},{"location":"notebooks/week-3_2/#netcdf","title":"NetCDF\u00b6","text":"<p>Network Common Data Form (NetCDF) is a commonly used file format in climatology, meteorology, oceanography, and geosciences and Earth sciences more broadly, where there is a need to store data as multidimensional arrays. The NetCDF file format is comprised of:</p> <ul> <li>variables - multidimensional arrays of data values (including 1D arrays for the dimensions with the same name as the corresponding dimension)</li> <li>dimensions - have a name and a value and describe the size and shape of the dataset</li> <li>attributes - additional metadata to describe the dataset</li> </ul> <p>The NetCDF format is very similar to the <code>xarray.Dataset</code> class (the NetCDF model was the basis for designing the <code>xarray.Dataset</code> class). However, <code>xarray.Dataset</code> are designed for working with multidimensional arrays in memory from within Python programs. NetCDF is a format for storing multidimensional arrays on disk. This is a useful description of the NetCDF format provided by ESRI.</p> <p>Let's open a NetCDF file which stores some data from ERA5-Land reanalysis weather data covering Western Australia. The data was downloaded from the Copernicus Climate Data Store with the following characteristics:</p> <ul> <li>Product type: Monthly averaged reanalysis by hour of day</li> <li>Variable: 2m temperature (t2m), Total precipitation (tp)</li> <li>Year: 2023</li> <li>Month: January, February, March, April, May, June, July, August, September, October, November, December</li> <li>Time: 00:00, 06:00, 12:00, 18:00</li> <li>Sub-region extraction: North -9\u00b0, West 109\u00b0, South -36\u00b0, East 129\u00b0</li> <li>Format: Zipped NetCDF-3 (experimental)</li> </ul>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you select the precipitation data from the <code>xarray.Dataset</code> <code>era5</code> and visualise precipitation data for a single time slice in June across Western Australia?</p> <p>Use a sensible colour palette for precipitation values. You will need to set <code>expver=1</code>.</p>"},{"location":"notebooks/week-3_2/#zarr","title":"ZARR\u00b6","text":"<p>Zarr is a modern cloud optimised format and specification for storing chunked and compressed multidimensional arrays. It's useful for working with big datasets which have an array-like structure and in cloud computing / web environments.</p> <p>For example, outputs from weather and climate models often comprise a large number of multidimensional arrays (e.g. dimensions for latitude, longitude, and time and arrays for a wide range of variables such as temperature, precipitation, wind speed, pressure, and so on ....). One way of conceptualising a zarr dataset is to think of it as a directory of compressed array files.</p> <p>Compressing arrays means that their storage size is reduced. This reduces the costs associated with storing large datasets and means it is quicker to transfer data over networks.</p> <p>Chunking the arrays in storage means you don't need to be able to read the entire array into memory in your Python programs. The memory limits on personal computers / laptops can prohibit reading in entire datasets stored as arrays as dataset sizes increase.</p> <p>Zarr datasets are well suited to cloud storage buckets (e.g. Google Cloud Storage, Amazon S3) and they also support parallel read and writes. This means that multiple clients (e.g. users, applications) can read data concurrently from the same zarr dataset stored in the cloud.</p> <p>In the <code>data_lab-3_2</code> directory is a zarr dataset named <code>nuist_cmip6_wa_2100_tasmax.zarr</code>. It stores an array of maximum temperature data for the year 2100 covering Western Australia extraced from the Climate Impact Lab Global Downscaled Projections for Climate Impacts Research. This data is from the World Climate Research Programme's 6th Coupled Model Intercomparison Project (CMIP6) and is generated by the NUIST NESM3 model and the ssp585 scenario (this dataset has been made available with a Creative Commons 4.0 International License.</p> <p>xarray provides functionality for reading zarr datasets (see docs here).</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you read in the zarr dataset <code>nuist_cmip6_wa_2100_tasmax.zarr</code> to an <code>xarray.Dataset</code>?</p>"},{"location":"notebooks/week-3_2/#vector-data","title":"Vector data\u00b6","text":"<p>Vector data uses point, line, or polygon geometries to represent geographic features.</p>"},{"location":"notebooks/week-3_2/#geopandas-geodataframe","title":"GeoPandas GeoDataFrame\u00b6","text":"<p>A GeoPandas <code>GeoDataFrame</code> is a tabular data structure for storing vector geospatial data and is based on a regular pandas <code>DataFrame</code>.</p> <p>A <code>GeoDataFrame</code> consists of columns of non-spatial attributes similar to a pandas <code>DataFrame</code>. However, a <code>GeoDataFrame</code> also has a <code>geometry</code> column which is a <code>GeoSeries</code> of geometries for the spatial data associated with each row.</p> <p>In Python, geometries are represented as Shapely <code>Geometry</code> objects. The <code>geometry</code> column in a GeoPandas <code>GeoDataFrame</code> is a <code>Series</code> of Shapely <code>Geometry</code> objects. Printing a Shapely <code>Geometry</code> object returns a Well Known Text (WKT) string description of the geometry (e.g. <code>POINT (0, 1)</code>). The <code>geometry</code> column of a <code>GeoDataFrame</code> (or a <code>GeoSeries</code>) can be viewed as a sequence of Shapely <code>Geometry</code> objects:</p> <pre><code>a_geoseries = [POINT (0, 1), POINT (0, 2), POINT (2, 3)]\n</code></pre> <p>Shapely provides tools for representing geometries in Python programs. It does not provide tools for reading geometry data from disk or handling attribute data. GeoPandas <code>GeoDataFrame</code> and <code>GeoSeries</code> combine Shapely's functionality for handling geometries with tools for reading and writing vector data, handling attributes, and visualisation. Therefore, we will focus on using <code>GeoDataFrame</code>s in these labs.</p> <p>Let's convert a CSV file with longitude, latitude, and elevation columns into a <code>GeoDataFrame</code>. First, let's read the CSV file in as a pandas <code>DataFrame</code>.</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>You will need to refer to the GeoPandas documentation to answer these questions.</p> What does executing the <code>GeoDataFrame</code> method <code>points_from_xy()</code> return? <code>points_from_xy()</code> expects a pandas <code>Series</code> objects for x and y coordinates and coordinate reference system. It will return to a GeoPandas <code>GeometryArray</code> object which stores a POINT geometry object for each x and y pair and can be converted into a <code>GeoSeries</code> object.  <p></p> The <code>GeoDataFrame()</code> constructor function can take three arguments. What are these arguments and how do they enable the creation of a <code>GeoDataFrame</code> object?     The <code>GeoDataFrame()</code> constructor function requires a pandas <code>DataFrame</code> as its first argument. This data is the non-spatial attributes. The second (optional) argument is a GeoPandas  object which stores <code>geometry</code> objects associated with each row (this could also be a string denoting the column of a <code>DataFrame</code> storing geometries. The third (optional) argument is a crs denoting the coordinate reference system for the geometry data."},{"location":"notebooks/week-3_2/#geojson","title":"GeoJSON\u00b6","text":"<p>JSON data (JavaScript Object Notation for its full name) is a widely used format for data interchange (exchanging data between programs, computers, clients, and servers).</p> <p>JSON represents data as key:value pairs enclosed within curly brackets <code>{}</code> (you might notice the similarity with Python's dictionary data structure).</p> <p>This is an example of JSON data:</p> <pre><code>{\n    \"title\": \"Introducing JSON\",\n    \"url\": \"https://www.json.org/json-en.html\"\n}\n</code></pre> <p>The values in JSON data can include text (strings), numbers, arrays (lists), and nested JSON objects. Like the CSV format, JSON is a text based format where human readable characters are encoded in binary using UTF-8 or UTF-16.</p> <p>GeoJSON is an extension of the JSON format for storing and exchanging spatial data. One of GeoJSON's uses is sending spatial data to web browsers to render as layers on web maps.</p> <p>GeoJSON represents geographic features as vector data (points, lines, and polygon geometries) and can also store non-spatial attribute information.</p> <p>Spatial data in GeoJSON are represented using <code>geometry</code> types which include:</p> <p><code>Point</code></p> <pre><code>{\"type\": \"Point\", \"coordinates\": [1, 1]}\n</code></pre> <p><code>LineString</code></p> <pre><code>{\"type\": \"LineString\", \"coordinates\": [[1, 1], [2, 2]]}\n</code></pre> <p><code>Polygon</code></p> <pre><code>{\"type\": \"Polygon\", \"coordinates\": [[[1, 1], [2, 2], [1, 2], [1, 1]]]}\n</code></pre> <p><code>Feature</code> types include attribute data as <code>properties</code> with <code>geometry</code> types.</p> <pre><code>{\n    \"type\": \"Feature\",\n    \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [0, 0]\n    }, \n    \"properties\": {\n        \"name\": \"Perth Airport\"\n    }\n}\n</code></pre> <p>A <code>FeatureCollection</code> is a collection of <code>Feature</code>s.</p> <pre><code>{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [0, 0]\n            }, \n            \"properties\": {\n                \"name\": \"Perth Airport\"\n            }\n        },\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [10, 1]\n            }, \n            \"properties\": {\n                \"name\": \"Broome Airport\"\n            }\n        }\n        \n    ]\n}\n</code></pre> <p>You can read More than you ever wanted to know about GeoJSON for a description of the GeoJSON format.</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Identify two differences between the GeoJSON file format and a GeoPandas <code>GeoDataFrame</code> <ul> <li>A <code>GeoDataFrame</code> is used to store geospatial data in memory for Python programs. A GeoJSON file format describes how geospatial data should be encoded when it is stored on disk.</li> <li>A <code>GeoDataFrame</code> uses a tabular structure to organise non-spatial and spatial attributes with each row corresponding to a feature. GeoJSON format uses dictionary-like structure of key:value pairs with geographic data (coordinates) stored as values with a <code>geometry</code> key and attribute data stored as values with a <code>properties</code> key.</li> </ul>"},{"location":"notebooks/week-3_cloud_native_geospatial/","title":"Week 3 cloud native geospatial","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install aiohttp dask matplotlib requests xarray zarr geopandas pyarrow fsspec mapclassify\n</pre> !pip install aiohttp dask matplotlib requests xarray zarr geopandas pyarrow fsspec mapclassify <p>NOAA global precipitation climate project (<code>gpcp</code>) data is stored as zarr files in the cloud at Pangeo-forge's object storage. This is a relatively large file (&gt;2GB) storing global precipitation measuremens for over 9000 days. As this is a cloud native geospatial file format, we can explore the structure of the file's data and retrieve subsets of the file's data without having to download the entire file.</p> <p>The <code>xarray</code> output shows the dataset information retrieved lazily. Initially, it has only downloaded information about the dataset. Data is only downloaded when you request it for a computation and you can subset only the data that you require.</p> In\u00a0[\u00a0]: Copied! <pre>import xarray as xr\n\nstore = \"https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/gpcp-feedstock/gpcp.zarr\"\nds = xr.open_dataset(store, engine=\"zarr\", chunks={}, consolidated=True)\nds\n</pre> import xarray as xr  store = \"https://ncsa.osn.xsede.org/Pangeo/pangeo-forge/gpcp-feedstock/gpcp.zarr\" ds = xr.open_dataset(store, engine=\"zarr\", chunks={}, consolidated=True) ds <p>When working with large files (i.e. files that would be too big to store on your computer / local storage, too big to load into memory, would take too long to download, or would be slow to interact with), lazy loading means you can quickly query and explore the structure without having to download the data. The ability to subset data from files in the cloud means you can download only the data that is relevant to you and avoid having to download large amounts of redundant data.</p> <p>The example below demonstrates how to subset and download only the precipitation data from January 2020. The first day's worth of data is visualised on the map.</p> In\u00a0[\u00a0]: Copied! <pre>ds.sel(time=slice('2020-01-01', '2020-01-31')).precip[0].plot(figsize=(24,12))\n</pre> ds.sel(time=slice('2020-01-01', '2020-01-31')).precip[0].plot(figsize=(24,12)) In\u00a0[\u00a0]: Copied! <pre>import fsspec\nimport geopandas as gpd\nfrom fsspec.implementations.http import HTTPFileSystem\nimport time\n\nfilesystem = HTTPFileSystem()\n\nstart = time.time()\ngdf = gpd.read_parquet(\"https://data.source.coop/fiboa/us-ca-scm/us_ca_scm.parquet\", filesystem=filesystem)\ngdf\nprint(f\"read time: {time.time() - start}\")\nprint(f\"number of features: {gdf.shape[0]}\")\n</pre> import fsspec import geopandas as gpd from fsspec.implementations.http import HTTPFileSystem import time  filesystem = HTTPFileSystem()  start = time.time() gdf = gpd.read_parquet(\"https://data.source.coop/fiboa/us-ca-scm/us_ca_scm.parquet\", filesystem=filesystem) gdf print(f\"read time: {time.time() - start}\") print(f\"number of features: {gdf.shape[0]}\") In\u00a0[\u00a0]: Copied! <pre>start = time.time()\ngdf = gpd.read_parquet(\n    \"https://data.source.coop/fiboa/us-ca-scm/us_ca_scm.parquet\",\n    columns=[\"id\", \"crop:name\", \"geometry\"],\n    bbox=(-121.69410969790363, 38.50618205905283, -121.59848230208829, 38.5900803946528),\n    filesystem=filesystem)\ngdf\nprint(f\"read time: {time.time() - start}\")\nprint(f\"number of features: {gdf.shape[0]}\")\n# gdf.explore()\n</pre> start = time.time() gdf = gpd.read_parquet(     \"https://data.source.coop/fiboa/us-ca-scm/us_ca_scm.parquet\",     columns=[\"id\", \"crop:name\", \"geometry\"],     bbox=(-121.69410969790363, 38.50618205905283, -121.59848230208829, 38.5900803946528),     filesystem=filesystem) gdf print(f\"read time: {time.time() - start}\") print(f\"number of features: {gdf.shape[0]}\") # gdf.explore()"},{"location":"notebooks/week-3_cloud_native_geospatial/#cloud-native-geospatial-file-formats","title":"Cloud native geospatial file formats\u00b6","text":"<p>These notes are adapted from the Cloud-Optimized Geospatial Formats Guide, which is published the a CC by 4.0 license.</p> <p>A key feature of cloud native geospatial file formats is that they allow partial reads (over a network). This means that a large file (e.g. a large array based dataset of weather data or vector dataset of millions of geometries and associated attributes) is located in cloud storage and you can subset a portion of this data and read it into your local environment (e.g. to work with in a Python program).</p> <p>Typically, cloud native geospatial file formats let you subset data by geographic location (e.g. a bounding box), time or variable names or column headings.</p> <p>This notebook demonstrates how to implement partial reads of raster and vector data from cloud native geospatial files stored on the cloud.</p>"},{"location":"notebooks/week-3_cloud_native_geospatial/#zarr-data","title":"Zarr data\u00b6","text":""},{"location":"notebooks/week-3_cloud_native_geospatial/#geoparquet","title":"GeoParquet\u00b6","text":"<p>GeoParquet is a file format for storing vector geospatial data. It is based on the parquet format for storage of tabular data. Each row in a GeoParquet file corresponds to a spatial feature (e.g. points, lines and polygons).</p> <p>GeoParquet files efficiently compress data, reducing storage and data transfer costs, and organise data within a file by column chunks and row groups. This means you can subset data by row group and column chunks. The data is spatially partitioned, this means you can spatially filter row groups of interest.</p> <p></p> <p>Source: Cloud Native Geospatial</p> <p>The below code snippets demonstrate reading vector data from a GeoParquet file in the cloud into a Python environment. The vector data represents field boundaries in California.</p> <p>The first code snippet reads the entire dataset from the cloud into a GeoPandas GeoDataFrame. The second code snippet reads a subset of columns and spatial features from a bounding box near Davis.</p>"},{"location":"notebooks/week-3_practice/","title":"Week 3 practice","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-3_practice\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_practice.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-3_practice.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-3_practice\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-3_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-3_practice\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_practice.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-3_practice.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-3_practice\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-3_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n</pre> import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import os In\u00a0[\u00a0]: Copied! <pre>week_3_practice_path = os.path.join(os.getcwd(), \"data_lab-3_practice\")\n</pre> week_3_practice_path = os.path.join(os.getcwd(), \"data_lab-3_practice\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>os.listdir(week_3_practice_path)\n</pre> What spatial data file formats are used to store data in this directory? <ul> <li>shapefile (.shp)</li> <li>JavaScript Object Notation or JSON (.json)</li> <li>Keyhole Markup Language Zipped (.kmz)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>obs_event_path = os.path.join(os.getcwd(), \"data_lab-3_practice\", \"EMSR489_AOI07_GRA_PRODUCT_observedEventA_r1_v1.shp\")\nobs_event_gdf = gpd.read_file(obs_event_path)\n\n# check obs event gdf is a GeoDataFrame and the data looks sensible\nprint(type(obs_event_gdf))\nobs_event_gdf.head()\n</pre> <p>If you have successfully read the observed event data into your program you should be able inspect it's structure and information. The head of a tabular dataset should be displayed with one row per-feature which has a <code>POLYGON</code> geometry and several attributes.</p> <p>GeoPandas <code>GeoDataFrame</code> objects have an <code>explore()</code> method which enables quick rendering of data on a web map. We use the <code>column</code> argument to specify which of the <code>GeoDataFrame</code> column values to map to colours on the display and we use the <code>cmap</code> (stands for colourmap) argument to pass in a list of colours to represent different column values. Here, we use blue shades to represent flood impacts and brown to represent tree damage.</p> <p>We use the variable <code>m</code> to reference the web map object that we can visualise on our display. This means we can refer to <code>m</code> later in our program and add more layers to the map or restyle it.</p> In\u00a0[\u00a0]: Copied! <pre>m = obs_event_gdf.explore(column=\"notation\", categorical=True, cmap = [\"cyan\", \"blue\", \"brown\"])\nm\n</pre> m = obs_event_gdf.explore(column=\"notation\", categorical=True, cmap = [\"cyan\", \"blue\", \"brown\"]) m <p>Transport routes form a key part of emergency response, can you read in the vector data that represents transportation routes using the JSON format?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>transportation_data_path = os.path.join(os.getcwd(), \"data_lab-3_practice\", \"EMSR489_AOI07_GRA_PRODUCT_transportationL_r1_v1.json\")\ntransportation_gdf = gpd.read_file(transportation_data_path)\nprint(type(transportation_gdf))\ntransportation_gdf.head()\n</pre> <p></p> <p>We can add the transportation layers to web map object referenced by <code>m</code>. Let's represent values in the column <code>info</code> with different colours on the map.</p> <p>Note, when you render the map the legend for the observed event impact layer and the transportation layer might overlap. This is an interactive map. So, you can easily drag the legend around the display to where they work best for you.</p> In\u00a0[\u00a0]: Copied! <pre>transportation_gdf.explore(column=\"info\", m=m, cmap=[\"red\", \"black\", \"grey\"])\nm\n</pre> transportation_gdf.explore(column=\"info\", m=m, cmap=[\"red\", \"black\", \"grey\"]) m In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/week-3_practice/#week-3-practice-exercises","title":"Week 3 - Practice Exercises\u00b6","text":"<p>This notebook contains practice exercises related to week 3's content on working with file systems and directories and reading data from files into Python programs.</p> <p>These exercises are based on vector data products that delineate the extent of disaster and emergency event impacts. These products are generated by the European Commission Copernicus Emergency Management Service (EMS) - Rapid Mapping Activations to aid with disaster response. The EMS Rapid Mapping Activations include vector geospatial data delineating the extent of the event impact and ancillary spatial data layers relevant to disaster response (e.g. infrastructure, land use). They also include a high-quality cartographic product depicting the event impact (in a PDF or JPEG format).</p> <p>Vector data for Tropical Cyclone Yasa has been obtained and stored in the folder data_lab-3_practice. In these exercises you will need to read this vector data from files into your Python program and check the data was read in correctly. Please refer to the Tropical Cyclone Yasa (EMSR489: Fiji, 2020) information page to find out about the disaster event and the data products.</p>"},{"location":"notebooks/week-3_practice/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-3_practice/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-3_practice/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-3_practice/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-3_practice/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-3_practice/#file-systems-and-directories","title":"File systems and directories\u00b6","text":"<p>Can you use the <code>os.listdir()</code> function to list all the files in the data_lab-3_practice folder?</p> <p>You will need to pass the following path into <code>os.listdir()</code> as an argument.</p>"},{"location":"notebooks/week-3_practice/#tropical-cyclone-yasa-fiji-2020","title":"Tropical Cyclone Yasa (Fiji, 2020)\u00b6","text":"<p>Tropical Cyclone Yasa struck Fiji in December 2020 causing substantial flood damage. It is denoted by the EMSR code EMSR489. The below image is the cartographic map output depicting damage and key features surrounding Labasa on Vanua Levu.</p> <p></p> <p>The folder data_lab-3_practice stores some of the vector data used to generate layers on this image. Can you identify the observedEvent layer in the list of files in this directory, create a path to this file, and read it into your program?</p> <p>Use the shapefile with a .shp ending for this task and read the data to a variable named <code>obs_event_gdf</code>.</p> <p>Tip - part of the code to generate the path to the file is:</p> <pre>obs_event_path = os.path.join(os.getcwd(), \"data_lab-3_practice\", &lt;FILENAME HERE&gt;)\n</pre>"},{"location":"notebooks/week-4_1/","title":"Week 4 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-4_1\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_1.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-4_1.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-4_1\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-4_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-4_1\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_1.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-4_1.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-4_1\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-4_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import os\nimport time\n\nimport rioxarray as rxr\nimport xarray as xr\nimport plotly.express as px\nimport numpy as np\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\n\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import time  import rioxarray as rxr import xarray as xr import plotly.express as px import numpy as np import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt import matplotlib.colors  import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>data_path = os.path.join(os.getcwd(), \"data_lab-4_1\")\n</pre> data_path = os.path.join(os.getcwd(), \"data_lab-4_1\") In\u00a0[\u00a0]: Copied! <pre>os.listdir(data_path)\n</pre> os.listdir(data_path) <ul> <li><code>s2_tc_yasa_pre_event.tif</code> is a four band Sentinel-2 image from 25 October 2020. The four bands are blue, green, red, and near infrared (NIR).</li> <li><code>s2_tc_yasa_pre_event_cloud_probability.tif</code> is a cloud probability raster for the Sentinel-2 image on the 25 October 2020. Each pixel has a value between 0 and 100 indicating the probability of that pixel being cloud covered.</li> <li><code>s2_tc_yasa_post_event.tif</code> is a four band Sentinel-2 image from 19 December 2020. The four bands are blue, green, red, and near infrared (NIR).</li> <li><code>s2_tc_yasa_post_event_cloud_probability.tif</code> is a cloud probability raster for the Sentinel-2 image on the 19 December 2020. Each pixel has a value between 0 and 100 indicating the probability of that pixel being cloud covered.</li> </ul> <p>The cloud probability rasters are generated using Sentinel Hub's sentinel2-cloud-detector machine learning algorithm.</p> In\u00a0[\u00a0]: Copied! <pre>pre_tc_yasa_s2_path = os.path.join(data_path, \"s2_tc_yasa_pre_event.tif\")\npre_s2 = rxr.open_rasterio(pre_tc_yasa_s2_path)\n</pre> pre_tc_yasa_s2_path = os.path.join(data_path, \"s2_tc_yasa_pre_event.tif\") pre_s2 = rxr.open_rasterio(pre_tc_yasa_s2_path) In\u00a0[\u00a0]: Copied! <pre>pre_s2\n</pre> pre_s2 In\u00a0[\u00a0]: Copied! <pre>pre_s2.sel(band=[3, 2, 1]).plot.imshow(vmin=0, vmax=2000, add_labels=False, aspect=3, size=4)\n</pre> pre_s2.sel(band=[3, 2, 1]).plot.imshow(vmin=0, vmax=2000, add_labels=False, aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre># Data inspection\npre_s2.sel(band=[4, 3, 2]).plot.imshow(vmin=0, vmax=2000, add_labels=False, aspect=3, size=4)\n</pre> <p>As discussed in previous labs, one of the nice features of <code>xarray.DataArray</code> objects is the ability to store information about the array data as dimensions with associated coordinates. Let's add a time dimension to store the date of image capture. This is important as our workflow involves change detection, so it's useful to keep track of which array corresponds to remote sensing images before the tropical cyclone event.</p> In\u00a0[\u00a0]: Copied! <pre># add time dimension and coords\npre_s2 = pre_s2.expand_dims(dim={\"time\": [pd.to_datetime(\"2020-10-25\")]}, axis=0)\npre_s2\n</pre> # add time dimension and coords pre_s2 = pre_s2.expand_dims(dim={\"time\": [pd.to_datetime(\"2020-10-25\")]}, axis=0) pre_s2 In\u00a0[\u00a0]: Copied! <pre>pre_s2 = pre_s2 / 10000\npre_s2\n</pre> pre_s2 = pre_s2 / 10000 pre_s2 In\u00a0[\u00a0]: Copied! <pre>pre_s2_cloud_prob_path = os.path.join(data_path, \"s2_tc_yasa_pre_event_cloud_probability.tif\")\npre_s2_cloud_prob = rxr.open_rasterio(pre_s2_cloud_prob_path)\npre_s2_cloud_prob\n</pre> pre_s2_cloud_prob_path = os.path.join(data_path, \"s2_tc_yasa_pre_event_cloud_probability.tif\") pre_s2_cloud_prob = rxr.open_rasterio(pre_s2_cloud_prob_path) pre_s2_cloud_prob In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>pre_s2_cloud_prob.sel(band=1).plot.imshow(aspect=3, size=4)\n</pre> <p>Next, we need to convert the <code>pre_s2_cloud_prob</code> <code>xarray.DataArray</code> of cloud probability scores (i.e. values between 0 and 100) to a binary array where pixels have the value of <code>True</code> if they're not cloudy. We can use a cloud probability threshold of 50, where a value less than 50 indicates no cloud. This involves applying a less than (<code>&lt;</code>) operation to every pixel in <code>pre_s2_cloud_prob</code>.</p> In\u00a0[\u00a0]: Copied! <pre>pre_s2_cloud_mask = pre_s2_cloud_prob &lt; 50\npre_s2_cloud_mask\n</pre> pre_s2_cloud_mask = pre_s2_cloud_prob &lt; 50 pre_s2_cloud_mask <p>We can plot the cloud mask <code>pre_s2_cloud_mask</code> to see which pixels are clear and which are cloudy. Note, that <code>True</code> values will be rendered as 1 and <code>False</code> as 0.</p> In\u00a0[\u00a0]: Copied! <pre>pre_s2_cloud_mask.sel(band=1).plot.imshow(aspect=3, size=4)\n</pre> pre_s2_cloud_mask.sel(band=1).plot.imshow(aspect=3, size=4) <p>Now use the cloud mask to set all pixels in the <code>pre_S2</code> <code>xarray.DataArray</code> of Sentinel-2 multispectral data to NaN (not a number - a no data indicator). We can use the <code>where()</code> method of <code>xarray.DataArray</code> objects to do this.</p> <p>The <code>where()</code> method takes in an array of bool type values (<code>True</code>, <code>False</code>) and sets all pixels to NaN (no data) where the value passed into <code>where()</code> is <code>False</code>. Or, <code>where()</code> can take a conditional or comparison statement that evaluates to <code>True</code> or <code>False</code>.</p> <p>Let's apply the cloud mask to <code>pre_s2</code>.</p> In\u00a0[\u00a0]: Copied! <pre>pre_s2_cm = pre_s2.where(pre_s2_cloud_mask.sel(band=1))\n</pre> pre_s2_cm = pre_s2.where(pre_s2_cloud_mask.sel(band=1)) <p>We also need to update the metadata for the <code>pre_s2</code> <code>xarray.DataArray</code> object to identify the no data value. This is important for when we save the array data to a file (e.g. GeoTIFF file) to keep a record of which pixels have no data. We can do this using the <code>rio</code> accessor from rioxarray.</p> In\u00a0[\u00a0]: Copied! <pre>pre_s2_cm.rio.write_nodata(np.nan, encoded=True, inplace=True)\n# check no data value has been set\nprint(f\"nodata: {pre_s2_cm.rio.nodata}\")\nprint(f\"encoded_nodata: {pre_s2_cm.rio.encoded_nodata}\")\npre_s2_cm\n</pre> pre_s2_cm.rio.write_nodata(np.nan, encoded=True, inplace=True) # check no data value has been set print(f\"nodata: {pre_s2_cm.rio.nodata}\") print(f\"encoded_nodata: {pre_s2_cm.rio.encoded_nodata}\") pre_s2_cm In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>You could print <code>pre_s2_cm</code> and inspect the array values to see if any have been converted to NaN. Or, better, you could plot the RGB image and see if the clouds have been masked out.</p> <pre>pre_s2_cm.sel(time=\"2020-10-25\", band=[3, 2, 1]).plot.imshow(vmin=0, vmax=0.2, add_labels=False, aspect=3, size=4)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>post_tc_yasa_s2_path = os.path.join(data_path, \"s2_tc_yasa_post_event.tif\")\npost_s2 = rxr.open_rasterio(post_tc_yasa_s2_path)\n</pre> <p>Next, you need to add a time dimension as the first (0th) axis. Set the time value to <code>\"2020-12-19\"</code>, which corresponds to the date of the Sentinel-2 image capture.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre># add time dimension and coords\npost_s2 = post_s2.expand_dims(dim={\"time\": [pd.to_datetime(\"2020-12-19\")]}, axis=0)\n</pre> <p>Now, can you rescale the Sentinel-2 multispectral reflectance values to be between 0 and 1 by dividing all array values by 10000?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre># rescale reflectance to 0 and 1\npost_s2 = post_s2 / 10000\n</pre> <p>Can you read in the cloud probability raster and set all values less than 50 to <code>True</code>? Use the variable name <code>post_s2_cloud_mask</code> to refer to the cloud mask array.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>post_s2_cloud_prob_path = os.path.join(data_path, \"s2_tc_yasa_post_event_cloud_probability.tif\")\npost_s2_cloud_prob = rxr.open_rasterio(post_s2_cloud_prob_path)\npost_s2_cloud_mask = post_s2_cloud_prob &lt; 50\n</pre> <p>Finally, can you apply the cloud mask to <code>post_s2</code> using the <code>where()</code> method to set all cloudy pixels to NaN (no data)?</p> <p>Remember to update the no data value in the <code>post_s2</code> <code>xarray.DataArray</code> objects metadata.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre># apply cloud mask\npost_s2_cloud_mask = post_s2_cloud_prob &lt; 50\npost_s2_cm = post_s2.where(post_s2_cloud_mask.sel(band=1))\npost_s2_cm.rio.write_nodata(np.nan, encoded=True, inplace=True)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>pre_s2_ndwi = (pre_s2_cm.sel(band=2) - pre_s2_cm.sel(band=4)) / (pre_s2_cm.sel(band=2) + pre_s2_cm.sel(band=4))\npost_s2_ndwi = (post_s2_cm.sel(band=2) - post_s2_cm.sel(band=4)) / (post_s2_cm.sel(band=2) + post_s2_cm.sel(band=4))\n</pre> <p>Let's check the NDWI was computed correctly by visualising the data. The values should be between -1 and 1, and higher values should correspnd to the presence of water. The ocean and rivers should appear in blue shades as we're using a green to blue colour palette here (low NDWI values are mapped to green shades and high NDWI values are mapped to blue shades).</p> <p>Note, here we use the <code>sel()</code> method again, but we're subsetting along the time dimension. As there is only one period along the time dimension, we're effectively removing the time informaton from the <code>xarray.DataArray</code> object to leave a two dimensional array for plotting.</p> In\u00a0[\u00a0]: Copied! <pre>pre_s2_ndwi.sel(time=\"2020-10-25\").plot.imshow(cmap=\"GnBu\", robust=True, aspect=3, size=4)\n</pre> pre_s2_ndwi.sel(time=\"2020-10-25\").plot.imshow(cmap=\"GnBu\", robust=True, aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>post_s2_ndwi.sel(time=\"2020-12-19\").plot.imshow(cmap=\"GnBu\", robust=True, aspect=3, size=4)\n</pre> post_s2_ndwi.sel(time=\"2020-12-19\").plot.imshow(cmap=\"GnBu\", robust=True, aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>diff_ndwi = post_s2_ndwi.sel(time=\"2020-12-19\") - pre_s2_ndwi.sel(time=\"2020-10-25\")\ndiff_ndwi\n</pre> <p>Let's plot the difference image. We'll use the <code>\"Blues\"</code> colour palette so areas of an increase in water should appear as darker blue shades.</p> In\u00a0[\u00a0]: Copied! <pre>diff_ndwi.plot.imshow(cmap=\"Blues\", robust=True, aspect=3, size=4)\n</pre> diff_ndwi.plot.imshow(cmap=\"Blues\", robust=True, aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>diff_ndwi.plot.imshow(cmap=\"RdBu\", robust=True, center=0, aspect=3, size=4)\n</pre> In\u00a0[\u00a0]: Copied! <pre>rel_change_ndwi = diff_ndwi / np.maximum(post_s2_ndwi.sel(time=\"2020-12-19\"), pre_s2_ndwi.sel(time=\"2020-10-25\"))\nrel_change_ndwi\n</pre> rel_change_ndwi = diff_ndwi / np.maximum(post_s2_ndwi.sel(time=\"2020-12-19\"), pre_s2_ndwi.sel(time=\"2020-10-25\")) rel_change_ndwi <p>Let's plot the relative change map.</p> In\u00a0[\u00a0]: Copied! <pre>rel_change_ndwi.plot.imshow(cmap=\"RdBu\", robust=True, center=0, aspect=3, size=4)\n</pre> rel_change_ndwi.plot.imshow(cmap=\"RdBu\", robust=True, center=0, aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>flood_map = rel_change_ndwi &gt; 1\nflood_map.plot.imshow(cmap=\"Blues\", aspect=3, size=4)\n</pre> flood_map = rel_change_ndwi &gt; 1 flood_map.plot.imshow(cmap=\"Blues\", aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>unitar_yasa_gdf = gpd.read_file(os.path.join(data_path, \"st1_20201219_floodextent_labasa.geojson\"))\nunitar_yasa_gdf.explore()\n</pre> unitar_yasa_gdf = gpd.read_file(os.path.join(data_path, \"st1_20201219_floodextent_labasa.geojson\")) unitar_yasa_gdf.explore() <p>The <code>xarray.DataArray</code> object <code>flood_map</code> stores a raster representation of the flooded extents (i.e. areas where there was a large relative change in NDWI after Tropical Cyclone Yasa made landfall).</p> <p>However, it would be good to overlay the vector geometries of \"ground truth\" flood extents on our NDWI difference image see if our choice of threshold is sensible. To do this we'll need to make sure the coordinate reference systems (CRS) for our raster data and vector data match. Let's check their CRS.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"The CRS of the raster data is: {rel_change_ndwi.rio.crs}\")\nprint(f\"The CRS of the vector data is: {unitar_yasa_gdf.crs}\")\n</pre> print(f\"The CRS of the raster data is: {rel_change_ndwi.rio.crs}\") print(f\"The CRS of the vector data is: {unitar_yasa_gdf.crs}\") <p>The CRS of the vector data is EPSG:4326 which uses latitude and longitude to identify locations on the Earth's surface. The CRS of the raster data is EPSG:32760 which is UTM zone 60S - a projected CRS. This means the raster data has been projected onto a flat surface. Let's convert the vector data to match the raster data's CRS.</p> In\u00a0[\u00a0]: Copied! <pre>unitar_yasa_gdf_epsg_32760 = unitar_yasa_gdf.to_crs(\"EPSG:32760\")\n</pre> unitar_yasa_gdf_epsg_32760 = unitar_yasa_gdf.to_crs(\"EPSG:32760\") In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(figsize=(12, 4))\n\nflood_map.plot.imshow(\n    cmap=\"Blues\",\n    ax=ax,\n)\n    \nunitar_yasa_gdf_epsg_32760.plot(\n    color=\"None\",\n    edgecolor=\"cyan\",\n    linewidth=0.25,\n    ax=ax,\n    zorder=4,\n)\n    \nax.axis('off')\nplt.show()\n</pre> fig, ax = plt.subplots(figsize=(12, 4))  flood_map.plot.imshow(     cmap=\"Blues\",     ax=ax, )      unitar_yasa_gdf_epsg_32760.plot(     color=\"None\",     edgecolor=\"cyan\",     linewidth=0.25,     ax=ax,     zorder=4, )      ax.axis('off') plt.show() In\u00a0[\u00a0]: Copied! <pre>esri_lulc_path = os.path.join(data_path, \"esri_lulc_2020.tif\")\nesri_lulc = rxr.open_rasterio(esri_lulc_path)\n</pre> esri_lulc_path = os.path.join(data_path, \"esri_lulc_2020.tif\") esri_lulc = rxr.open_rasterio(esri_lulc_path) In\u00a0[\u00a0]: Copied! <pre>esri_lulc\n</pre> esri_lulc <p>Let's visualise the ESRI land cover map. Don't worry about the fiddly code to draw the legend.</p> In\u00a0[\u00a0]: Copied! <pre>esri_lulc_colours = [\n    \"#1A5BAB\",\n    \"#358221\",\n    \"#A7D282\",\n    \"#87D19E\",\n    \"#FFDB5C\",\n    \"#EECFA8\",\n    \"#ED022A\",\n    \"#EDE9E4\",\n    \"#F2FAFF\",\n    \"#C8C8C8\",\n  ]\n\nesri_lulc_classes = [\n    \"water\", \n    \"trees\", \n    \"grass\", \n    \"flooded vegetation\", \n    \"crops\", \n    \"scrub/shrub\", \n    \"build area\", \n    \"bare ground\", \n    \"snow/ice\", \n    \"clouds\",\n]\n\nfig, ax = plt.subplots(figsize=(12, 4))\ncmap = matplotlib.colors.ListedColormap(esri_lulc_colours)\nesri_lulc.sel(band=1).plot.imshow(cmap=cmap, ax=ax, add_colorbar=False)\n\nlegend_labels = [f'{i+1}: {esri_lulc_classes[i]}' for i in range(len(esri_lulc_classes))]\nax.legend(handles=[plt.Line2D([0], [0], marker=\"s\", color=\"w\", markerfacecolor=f\"{esri_lulc_colours[i]}\", markersize=10, label=label) for i, label in enumerate(legend_labels)])\nplt.show()\n</pre> esri_lulc_colours = [     \"#1A5BAB\",     \"#358221\",     \"#A7D282\",     \"#87D19E\",     \"#FFDB5C\",     \"#EECFA8\",     \"#ED022A\",     \"#EDE9E4\",     \"#F2FAFF\",     \"#C8C8C8\",   ]  esri_lulc_classes = [     \"water\",      \"trees\",      \"grass\",      \"flooded vegetation\",      \"crops\",      \"scrub/shrub\",      \"build area\",      \"bare ground\",      \"snow/ice\",      \"clouds\", ]  fig, ax = plt.subplots(figsize=(12, 4)) cmap = matplotlib.colors.ListedColormap(esri_lulc_colours) esri_lulc.sel(band=1).plot.imshow(cmap=cmap, ax=ax, add_colorbar=False)  legend_labels = [f'{i+1}: {esri_lulc_classes[i]}' for i in range(len(esri_lulc_classes))] ax.legend(handles=[plt.Line2D([0], [0], marker=\"s\", color=\"w\", markerfacecolor=f\"{esri_lulc_colours[i]}\", markersize=10, label=label) for i, label in enumerate(legend_labels)]) plt.show() <p>To identify flooded croplands, we'll need to create a cropland mask that we can apply to our flood map (a similar process to cloud masking). This will require us to create a binary cropland <code>True</code> or <code>False</code> layer that has the same shape and CRS as our flood map. First, let's check the shape of our flood map and our land cover map.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"The shape of the flood map is: {flood_map.shape}\")\nprint(f\"The shape of the land cover map is: {esri_lulc.shape}\")\n</pre> print(f\"The shape of the flood map is: {flood_map.shape}\") print(f\"The shape of the land cover map is: {esri_lulc.shape}\") In\u00a0[\u00a0]: Copied! <pre>print(f\"The CRS of the flood map is: {flood_map.rio.crs}\")\nprint(f\"The CRS of the land cover map is: {esri_lulc.rio.crs}\")\n</pre> print(f\"The CRS of the flood map is: {flood_map.rio.crs}\") print(f\"The CRS of the land cover map is: {esri_lulc.rio.crs}\") <p>We can see that the shape and CRS of the land cover map does not match the flood map. Therefore, we'll need to perform a range of raster geometry operations to change the shape and dimensions of the land cover data. First, we'll need to reproject the land cover map to the CRS EPSG:32760 which can change the shape of the array (here we're moving from a spherical to a flat projection). Then, we need to clip the land cover map to the extent of the flood map. And, finally, we need to resample the land cover map so it has the same shape as the flood map. Helpfully, the rioxarray package has a <code>reproject_match()</code> method that handles all of these steps for us.</p> In\u00a0[\u00a0]: Copied! <pre>esri_lulc_match = esri_lulc.rio.reproject_match(flood_map)\n</pre> esri_lulc_match = esri_lulc.rio.reproject_match(flood_map) <p>Let's check this worked OK.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"The shape of the flood map is: {flood_map.shape}\")\nprint(f\"The shape of the land cover map is: {esri_lulc_match.shape}\")\n</pre> print(f\"The shape of the flood map is: {flood_map.shape}\") print(f\"The shape of the land cover map is: {esri_lulc_match.shape}\") In\u00a0[\u00a0]: Copied! <pre>print(f\"The CRS of the flood map is: {flood_map.rio.crs}\")\nprint(f\"The CRS of the land cover map is: {esri_lulc_match.rio.crs}\")\n</pre> print(f\"The CRS of the flood map is: {flood_map.rio.crs}\") print(f\"The CRS of the land cover map is: {esri_lulc_match.rio.crs}\") <p>Now, let's create a cropland mask. Cropland is represented by the pixel value 5.</p> In\u00a0[\u00a0]: Copied! <pre>cropland = esri_lulc_match == 5\ncropland\n</pre> cropland = esri_lulc_match == 5 cropland In\u00a0[\u00a0]: Copied! <pre>cropland.sel(band=1).plot.imshow(cmap=\"Greens\", aspect=3, size=4)\n</pre> cropland.sel(band=1).plot.imshow(cmap=\"Greens\", aspect=3, size=4) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>flooded_croplands = flood_map.where(cropland)\nprint(f\"The area of flooded croplands is {(flooded_croplands.sum() * 100).values} square metres\")\n</pre>"},{"location":"notebooks/week-4_1/#data-wrangling-raster-data","title":"Data wrangling: raster data\u00b6","text":"<p>This lab will demonstrate a range of techniques for wrangling (transforming) raster data. This will include techniques to tidy, check, and visualise data; data subsetting; and various operations to transform image pixel values or summarise raster datasets.</p> <p>Here, you will work with two Sentinel-2 satellite images captured before and after Tropical Cyclone Yasa struck Fiji in December 2020. Tropical Cyclone Yasa made landfall in Fiji on 17-18 December with heavy rain and storm surges causing flooding. You will convert these images into datasets that reflect the presence of water and moisture on the land surface and conduct a change detection exercise comparing pre and post event images to estimate the area of cyclone induced flooding. Here, we'll be focusing on flood impacts on croplands surrounding Labasa on the island of Viti Levu.</p> <p></p>"},{"location":"notebooks/week-4_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-4_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-4_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-4_1/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-4_1/#what-is-data-wrangling","title":"What is data wrangling?\u00b6","text":"<p>Wickham and Grolemund (2017) and McKinney (2022) state that data wrangling consists of data import, data cleaning, and data transformation.</p>"},{"location":"notebooks/week-4_1/#data-import","title":"Data import\u00b6","text":"<p>Data import was covered in week 3 with examples of how to read tabular, vector, and raster data into Python programs.</p>"},{"location":"notebooks/week-4_1/#data-cleaning","title":"Data cleaning\u00b6","text":"<p>Data cleaning includes handling outliers and missing data. Here, we'll cloud mask Sentinel-2 remote sensing images, which is a data cleaning exercise.</p>"},{"location":"notebooks/week-4_1/#data-transformation","title":"Data transformation\u00b6","text":"<p>McKinney (2022) define data transformation as mathematical or statistical operations applied to data to generate new datasets. Data transformation can also include operations that reshape datasets or combine two or more datasets.</p> Detailed notes on data transformation for spatial and non-spatial data <p></p> As we're working with spatial and non-spatial data we can categorise data transformation operations as attribute operations, spatial operations, geometry operations, and raster-vector  operations (Lovelace et al. (2022)). <p></p> <p>Attribute operations are applied to non-spatial (attribute data). This could be a tabular dataset without any spatial information, the attribute table of a vector dataset, or the pixel values of a raster dataset. Common attribute operations include:</p> <ul> <li>Selecting columns from a table based on a condition.</li> <li>Selecting (subsetting) pixels from a raster based on a condition.</li> <li>Filtering rows from a table based on a condition.</li> <li>Creating a new column of values using a function applied to existing data.</li> <li>Computing summary statistics of columns in a table or of pixel values in a raster.</li> <li>Joining datasets based on matching values in columns (keys).</li> </ul> <p>Spatial operations transform data using the data's geographic information including shape and location. Vector spatial operations include:</p> <ul> <li>Spatial subsetting by selecting data points based on a geographic condition (e.g. selecting all fields in Western Australia).</li> <li>Spatial joins where datasets are combined based on their relationship in space.</li> <li>Spatial aggregation where summaries are produced for regions (e.g. the average crop yield for all fields in a region).</li> </ul> <p>Spatial operations on raster data are based on map algebra concepts and include:</p> <ul> <li>Local operations which are applied on a pixel by pixel basis (e.g. converting a raster of temperature values in \u00b0F to \u00b0C).</li> <li>Focal operations which summarise or transform a raster value using the values of neihbouring pixels (e.g. computing the average value within a 3 x 3 pixel moving window).</li> <li>Zonal operations which summarise or transform raster values using values inside an irregular shaped zone.</li> <li>Global operations which summarise the entire raster (e.g. computing the minimum value in the raster dataset).</li> </ul> <p>Geometry operations transform a dataset's geographic information. Common geometry operations for vector data include:</p> <ul> <li>Simplification of shapes.</li> <li>Computing the centroid of polygons.</li> <li>Clipping (subsetting) of geometries based on their intersection or relationship with another geometry.</li> </ul> <p>and geometry operations on raster data typically involve changing the spatial resolution and include:</p> <ul> <li>Aggregation or dissagregation.</li> <li>Resampling.</li> </ul> <p>Raster-vector operations involve both raster and vector datasets and include:</p> <ul> <li>Cropping or masking raster data using a vector geometry.</li> <li>Extracting raster values that intersect with a vector geometry.</li> <li>Rasterisation where a vector dataset is transformed to a raster layer.</li> <li>Vectorisation where a raster dataset is transformed to a vector layer.</li></ul> <p></p> <p>In this lab we're focusing on data transformation operations applied to raster data. Raster data breaks the Earth's surface up into a grid of cells (pixels). Each pixel is assigned a value that corresponds to the geographic feature or phenomenon of interest. In particular, we'll be working with remote sensing images where each pixel in a two dimensional raster stores a reflectance value (i.e. how much incoming light was reflected off the portion of the Earth's land surface that the pixel represents).</p> <p>To complete the task of creating a map of flooding following Tropical Cyclone Yasa, we will apply a range of subsetting, map algebra, and geometry operations to transform mutlispectral Sentinel-2 remote sensing images into a flood map.</p>"},{"location":"notebooks/week-4_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-4_1/#lab-data","title":"Lab data\u00b6","text":"<p>Let's quickly inspect the files that we have downloaded for this lab.</p>"},{"location":"notebooks/week-4_1/#xarray-refresher","title":"Xarray refresher\u00b6","text":"<p>Xarray <code>DataArray</code> data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values. <code>xarray.DataArray</code> objects have the following properties:</p> <ul> <li><code>values</code>: the multidimensional array of raster values</li> <li><code>dims</code>: a list of names for the dimensions of the array (e.g. instead of axis 0 describing the 0th (row) dimension of an array, that dimension can have a descriptive label such as longitude)</li> <li><code>coordinates</code>: a <code>list</code> of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)</li> <li><code>attrs</code>: a <code>dict</code> of metadata attributes describing the dataset</li> </ul> <p><code>xarray.DataArray</code> objects can be stored within a larger container called an <code>xarray.Dataset</code>. An <code>xarray.Dataset</code> can store many <code>xarray.DataArray</code> objects that share <code>dims</code> and <code>coordinates</code>. This is useful if you have arrays of different <code>Variables</code> that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single <code>xarray.Dataset</code>).</p> <p></p> <p>Schematic of an xarray.Dataset (source: xarray Getting Started)</p>"},{"location":"notebooks/week-4_1/#data-input","title":"Data input\u00b6","text":"<p>The <code>rioxarray</code> package provides tools for reading and writing raster geospatial data files into <code>xarray.DataArray</code> objects.</p> <p>Let's pass the path to a GeoTIFF file of the pre Tropical Yasa Sentinel-2 image into the <code>rioxarray</code> <code>open_rasterio()</code> function:</p>"},{"location":"notebooks/week-4_1/#subsetting-data","title":"Subsetting data\u00b6","text":"<p>Subsetting data (or selecting data) refers to operations that extract a subset of data from a larger dataset. Subsetting operations can be spatial or non-spatial.</p> <p>Spatial subsetting operations select data based on their location in space (e.g. extracting all paddocks that intersect with a farm boundary, or subsetting all remote sensing image pixels within a geographic region).</p> <p>Non-spatial subsetting operations select data based upon their attribute values or another non-spatial condition. For example, we could subset all values in a column of temperature measurements where the temperature was greater than 30 \u00b0C or all temperature measurements that match a date range. We can also subset data based on their position within a dataset (e.g. select the first 10 rows of a DataFrame or columns by column name). Subsetting data based on their position within the dataset is often referred to as indexing.</p> <p><code>xarray.DataArray</code> objects support label-based and positional subsetting:</p> <ul> <li>With positional subsetting, we can access <code>xarray.DataArray</code> values by selecting them using <code>[]</code> and index positions. For example, <code>demo_ds[0:4, 0:3, 0:3]</code> would select the first four bands (along the 0th dimension), first three rows (along the 1st dimension), and first three columns of array (along the 2nd dimension) of the array.</li> <li>With label-based subsetting we can use the <code>sel()</code> method to select elements from the array using dimension names. For example, <code>demo_ds.sel(time=\"2020-01-31, bands=[1, 2])</code> would select array values that correspond to the the 31st January 2020 and have band labels 1 and 2.</li> </ul> <p>It is recommended that you read the xarray guide on Subsetting data (or selecting data).</p> <p>To visualise the pre Tropical Cyclone Yasa Sentinel-2 image as an RGB image on our display, we need to select the red, green, and blue bands. This is a good use case for the <code>sel()</code> method.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you visualise the <code>pre_s2</code> image as a false colour composite where near infrared is rendered as red, red is rendered as green, and green is rendered as blue on the display?</p>"},{"location":"notebooks/week-4_1/#map-algebra","title":"Map algebra\u00b6","text":"<p>Following Lovelace et al. (2022), we refer to map algebra as operations that transform raster pixel values via statistical or mathematical operations which can involve combining pixel values from different raster layers or using neighbouring raster values.</p> <p>Lovelace et al. (2022) outline four different types of map algebra operations:</p>"},{"location":"notebooks/week-4_1/#local-operations","title":"Local operations\u00b6","text":"<p>Local map algebra operations operate on a pixel by pixel basis; the mathematical operation is applied independently to each pixel without reference to neighbouring pixel values. For example, addition, subtraction, multiplication, and logical operations can all be applied on a pixel by pixel basis.</p> <p>Commonly used local operations when working with remote sensing data are computing spectral indices or masking out cloudy pixels. Spectral indices are pixel by pixel mathematical combinations of spectral reflectance in different wavelengths that are used to monitor vegetation or land surface conditions. Read Zeng et al. (2022) for a review of vegetation indices.</p> <p>The normalised difference vegetation index (NDVI) is used for tracking vegetation condition and representing the greenness of vegetation in a remote sensing image.</p> <p>The NDVI is computed as:</p> <p>$NDVI=\\frac{NIR-red}{NIR+red}$</p> <p>Thus, the NDVI is computed via division, subtraction, and addition operations computed on a pixel by pixel basis using raster data corresponding to red and near infrared reflectance.</p>"},{"location":"notebooks/week-4_1/#focal-neighbourhood-operations","title":"Focal / neighbourhood operations\u00b6","text":"<p>Focal operations update a pixel's value using a combination of values from a regular shaped neighbourhood centred on the focal pixel (e.g. 3 x 3 or 5 x 5 pixel neighbourhood). An example of focal operations are dilation operations, which assign the focal pixel the maximum value found within the neighbourhood. This is often used to extend the coverage of cloud masks to conservatively remove thin clouds / poor atmospheric conditions in remote sensing images.</p>"},{"location":"notebooks/week-4_1/#zonal-operations","title":"Zonal operations\u00b6","text":"<p>Zonal operations summarise raster values in a target layer using a categorical zones raster layer to identify zones. For example, if we have a raster layer of NDVI values and a zones layer where each pixel value represents a crop type. A zonal operation computes the mean NDVI for for each crop type.</p>"},{"location":"notebooks/week-4_1/#global-operations","title":"Global operations\u00b6","text":"<p>Global operations compute summary statistics for an entire raster layer. For example, we could have a raster layer indicating the presence of forest or non-forest areas. Counting the number of forest pixels and multiplying the count by the pixel area creates global summary statistic of the area of forest cover.</p>"},{"location":"notebooks/week-4_1/#local-operations","title":"Local operations\u00b6","text":"<p>As mentioned above, local operations perform mathematical operations on arrays on a pixel by pixel basis. To identify and compute the area of cropland flooded by Tropical Cyclone Yasa, we will need to perform a range of local operations:</p> <ol> <li><p>Convert a cloud probability raster into a binary cloud mask (i.e. set all pixels to <code>True</code> where cloud probability is less than 50%).</p> </li> <li><p>Use the cloud mask to mask all cloudy pixels in Sentinel-2 multispectral satellite images (i.e. set all pixels to no data where the cloud mask is <code>False</code> - cloudy pixels).</p> </li> <li><p>Compute a spectral index, the normalised difference water index (NDWI), using bands of green and near infrared reflectance to highlight water and moisture presence.</p> </li> <li><p>Compute a difference image using the before and after Tropical Cyclone Yasa NDWI images to identify locations of change which are indicative of flood impacts.</p> </li> <li><p>Threshold the difference image to identify pixels where a large change in NDWI occurred to represent an estimate of flood extent.</p> </li> <li><p>Mask out non-cropland pixels using a land cover map to return an array of flooded cropland pixels.</p> </li> </ol> <p>An example of a local operation is dividing each pixel value by 10000 (or multiply by 0.0001). This is necessary because the data is spectral reflectance so should have a value between 0 and 1 (i.e. the ratio of incoming light to reflected light off the land surface in a spectral band). If you look at the information page for the Sentinel-2 remote sensing data we are using here, you can see it has a scale factor of 0.0001. This means the reflectance data has been scaled by multiplying it by 10000 so it can be stored as integer type. We need to convert it back to reflectance units.</p>"},{"location":"notebooks/week-4_1/#cloud-masking","title":"Cloud masking\u00b6","text":"<p>As you can see above, there are clouds obscuring parts of the land surface in the pre Tropical Cyclone Yasa Sentinel-2 image. We'll need to mask out those clouds. Let's start by loading a cloud probablity raster layer where each pixel value is a cloud probability score for the Sentinel-2 image taken before Tropical Cyclone Yasa.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you use the <code>plot.imshow()</code> method of an <code>xarray.DataArray</code> object to visualise the cloud probability raster?</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>How could you check that the cloud mask has been successfully applied?</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you repeat the process we have gone through above to cloud mask the post Tropical Cyclone Yasa Sentinel-2 image?</p> <p>First, you will need to create a path to the file <code>s2_tc_yasa_post_event.tif</code>, and then you will need to read the file into an <code>xarray.DataArray</code> object using rioxarray's <code>open_rasterio()</code> method. Use the variable name <code>post_s2</code> as a reference for the <code>xarray.DataArray</code> object.</p>"},{"location":"notebooks/week-4_1/#spectral-indices","title":"Spectral indices\u00b6","text":"<p>Spectral indices are mathematical combinations of spectral bands from remote sensing images to highlight features of interest on the land surface. To detect flooding associated with Tropical Cyclone Yasa we need to use a spectral index that's sensitive to the presence of water and moisture. We will compute the normalised difference water index (NDWI).</p> <p>The NDWI is computed as:</p> <p>$NDWI=\\frac{Green-NIR}{Green+NIR}$</p> <p>The NDWI ranges from -1 to 1, with positive values corresponding to the presence of water. The NDWI uses reflectance in the green and near infrared (NIR) portions of the electromagnetic spectrum. NIR light is absorbed by water and relatively more green light is reflected. It is this contrast between green and NIR reflectance that highlights water and moisture in images.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Computing the NDWI is a local map algebra operation, the NDWI is computed for each pixel in turn. Can you compute the NDWI using the arrays referenced by <code>pre_s2_cm</code> and <code>post_s2_cm</code>?</p> <p>Assign the results to the variables <code>pre_s2_ndwi</code> and <code>post_s2_ndwi</code>. You will need to use the <code>.sel()</code> method to select the bands that correspond to green and NIR reflectance. Green is band 2 and NIR is band 4.</p> <p>If you need help using <code>sel()</code>, look at how it is used earlier in this notebook or refer to the xarray documentation.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Why do we need to use the <code>sel()</code> method to select the green and NIR bands?</p> answer <p>The NDWI equation subtracts, adds, and divides two dimensional arrays (or rasters) (i.e. pixel wise subtracting the NIR array from the green array, pixel wise adding the green array and NIR array). The Sentinel-2 images that we're working with here have four bands (blue, green, red, and NIR). We need to convert the three dimensional four band arrays into two, two dimensional arrays to compute the NDWI. The <code>sel()</code> method lets us select a band to subset returning a two dimensional array.</p>"},{"location":"notebooks/week-4_1/#change-detection","title":"Change detection\u00b6","text":"<p>Change detection in the context of remote sensing image analysis is comparing two or more remote sensing images from different dates to detect change on the Earth's land surface. Change detection can be implemented as a local map algebra operation -  comparing change in pixels values across time where a large change in values indicates change in land surface conditions.</p> <p>We can detect flooded locations following Tropical Cyclone Yasa using a change detection analysis by comparing the pre and post event NDWI images. A large increase in NDWI values would indicate flooding.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>A simple change detection technique is to compute the difference in NDWI values for the pre and post event images. Can you do this and assign the difference image to the variable <code>diff_ndwi</code>?</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6Recap quiz","text":"<p>Can you consider a strength and limitation of using the <code>\"Blues\"</code> colour palette to viualise change in NDWI values?</p> answer <p>strength: change indicating increasing NDWI values are rendered in darker bluer shades which intuitively looks wetter.</p> <p>weakness: it is a sequential colour palette of blue shades from decreasing to increasing change in NDWI values. The sequential palette does not obviously convey both increasing and decreasing change values around zero. We could use a diverging palette with increasing change in NDWI represented by blue shades and decreasing change in NDWI represented by red shades.</p> <p>Here are the docs for <code>plot.imshow()</code> and <code>Matplotlib's colour palettes</code>. Can you use these docs to render the NDWI difference image with a diverging colour palette centred on zero?</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>What are some of the challenges / limitations to using the difference as a change detection metric?</p> answer <p>The magnitude can be influenced by how reflective a surface is or the illumination conditions when the image was captured. For example, a small absolute difference in spectral reflectance for a surface that does not reflect much might be a large relative change. Similarly, if one of the pre or post image capture conditions was brighter, then that could amplify or attenuate the difference signal. Therefore, often relative change is used as a change detection metric.</p> <p></p> <p>ESRI provide a formula for computing the relative change between two images:</p> <p>$relative change=\\frac{Post - Pre}{max(Post, Pre)}$</p> <p>The difference between the post event and pre event NDWI images is divided by the maximum NDWI pixel value considering both the pre and post images. Let's compute the relative change.</p>"},{"location":"notebooks/week-4_1/#flood-extent-maps","title":"Flood extent maps\u00b6","text":"<p>Here, we'll use a simple threshold to classify pixels as flooded or not-flooded. To start with, let's use a simple threshold from visual inspection of the data.</p> <p>Let's use a change threshold of an increase in NDWI of greater than 1 to represent flooded areas.</p>"},{"location":"notebooks/week-4_1/#geometry-operations","title":"Geometry operations\u00b6","text":""},{"location":"notebooks/week-4_1/#vector-geometry-operations","title":"Vector geometry operations\u00b6","text":"<p>Vector geometry operations transform the shape, size, and / or projection of vector datasets. They're applied to the geometries as opposed to the dataset attributes..</p> <p>We can load in a \"ground truth\" flood map prepared by UNITAR and compare their flood maps to our change image.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>By executing <code>unitar_yasa_gdf.crs</code>, are we accessing an attribute or a method?</p> answer <p>An attribute. Specifically we are accessing the <code>crs</code> attribute of a <code>GeoDataFrame</code> object. The <code>crs</code> attribute is self describing, it stores the coordinate reference system that the geometry information in the <code>GeoDataFrame</code> corresponds to.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>By executing <code>unitar_yasa_gdf.to_crs(\"EPSG:32760\")</code>, are we accessing an attribute or a method?</p> answer <p>A method. We can see it is a method by the parentheses following <code>to_crs()</code> and the fact \"to\" implies that we are doing something. Specifically, here, we are converting our geometry data to another CRS.</p> <p>By executing <code>to_crs()</code> we are performing a geometry data transformation operation. We are converting the geometry information (i.e. coordinates defining the polygons of flood extents) from one CRS to another.</p> <p>Now, let's plot our raster NDWI difference image and vector ground truth flood extents on the same map. You can see the correspondance between the dark blue shades of flooding detected using our remote sensing data and the light blue polygons which are the \"ground truth\" flood extents.</p>"},{"location":"notebooks/week-4_1/#raster-geometry-operations","title":"Raster geometry operations\u00b6","text":"<p>Raster geometry operations transform the shape, resolution, size, and / or projection of raster datasets. They're applied to the pixel geometries as opposed to the dataset attributes (i.e. pixel values).</p> <p>The final task we need to perform is to estimate the area of cropland that was flooded. To do this we need a map of cropland extents. We can read in a 10 m spatial resolution land cover map extracted from the ESRI 2020 Global Land Use Land Cover from Sentinel-2 dataset.</p> <p>In the ESRI land cover map, each pixel is assigned an integer value that corresponds to a land cover class. Cropland is class value 5. Let's read in the data.</p>"},{"location":"notebooks/week-4_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>As a final exercise, can you use the cropland mask to create a flood map of only flooded croplands? Use the cloud masking example as a template for how to do this.</p> <p>Can you compute the area of cropland that is flooded? You can do this by summing the number of flooded cropland pixels and multiplying the result by 100 (each pixel is 10 m x 10 m).</p> <p>You can find information aggregation of <code>xarray.DataArray</code> objects here. Is recommended you use the <code>sum()</code> method for this task.</p>"},{"location":"notebooks/week-4_2/","title":"Week 4 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-4_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-4_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-4_2\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-4_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-4_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-4_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-4_2\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-4_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n    !pip install rasterstats\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio     !pip install rasterstats In\u00a0[\u00a0]: Copied! <pre>import os\nimport time\n\nimport rioxarray as rxr\nimport xarray as xr\nimport plotly.express as px\nimport numpy as np\nimport geopandas as gpd\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\n\nfrom rasterstats import zonal_stats\n\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import time  import rioxarray as rxr import xarray as xr import plotly.express as px import numpy as np import geopandas as gpd import pandas as pd import matplotlib.pyplot as plt import matplotlib.colors  from rasterstats import zonal_stats  import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>data_path = os.path.join(os.getcwd(), \"data_lab-4_2\")\n</pre> data_path = os.path.join(os.getcwd(), \"data_lab-4_2\") In\u00a0[\u00a0]: Copied! <pre>os.listdir(data_path)\n</pre> os.listdir(data_path) <ul> <li>agrifieldnet_field_ids.parquet is a geospatial file storing the field boundaries as a polygon geometry and an integer field id.</li> <li>raster_labels.tif is a GeoTIFF file where each pixel value indicates what crop was grown in that pixel.</li> <li>s2_reflectance_agrifieldnet_competition_v1_source_0a664.tif is a multispectral Sentinel-2 image covering the extent of the fields.</li> </ul> <p>Let's start by exploring the Sentinel-2 image data. If we visualise it as an RGB image it should look like it covers and agricultural region.</p> In\u00a0[\u00a0]: Copied! <pre>s2_path = os.path.join(data_path, \"s2_reflectance_agrifieldnet_competition_v1_source_0a664.tif\")\ns2 = rxr.open_rasterio(s2_path)\n</pre> s2_path = os.path.join(data_path, \"s2_reflectance_agrifieldnet_competition_v1_source_0a664.tif\") s2 = rxr.open_rasterio(s2_path) In\u00a0[\u00a0]: Copied! <pre>s2\n</pre> s2 In\u00a0[\u00a0]: Copied! <pre>s2.sel(band=[4, 3, 2]).plot.imshow(robust=True)\n</pre> s2.sel(band=[4, 3, 2]).plot.imshow(robust=True) <p>Next, let's explore data in the <code>raster_labels.tif</code> file. This should be a GeoTIFF file storing raster data with the same projection system and x and y dimensions as the satellite image. Each pixel is assigned a numeric value that corresponds to a crop type. Based on the dataset's documentation, this is the mapping between numeric values and crop types in the labels dataset.</p> <ul> <li>1 - Wheat</li> <li>2 - Mustard</li> <li>3 - Lentil</li> <li>4 - No crop/Fallow</li> <li>5 - Green pea</li> <li>6 - Sugarcane</li> <li>8 - Garlic</li> <li>9 - Maize</li> <li>13 - Gram</li> <li>14 - Coriander</li> <li>15 - Potato</li> <li>16 - Bersem</li> <li>36 - Rice</li> </ul> In\u00a0[\u00a0]: Copied! <pre>labels_path = os.path.join(data_path, \"raster_labels.tif\")\nlabels = rxr.open_rasterio(labels_path)\nlabels.sel(band=1).plot.imshow(cmap=\"Set2\")\n</pre> labels_path = os.path.join(data_path, \"raster_labels.tif\") labels = rxr.open_rasterio(labels_path) labels.sel(band=1).plot.imshow(cmap=\"Set2\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>print(f\"The shape of the s2 array is {s2.shape}\")\nprint(f\"The shape of the crop type labels array is {labels.shape}\")\nprint(\"\")\nprint(f\"The CRS of the s2 array is {s2.rio.crs}\")\nprint(f\"The shape of the crop type labels array is {labels.rio.crs}\")\n</pre> <p>Finally, let's read in the field boundaries and display them on a web map.</p> In\u00a0[\u00a0]: Copied! <pre>gdf_field_ids = gpd.read_parquet(os.path.join(data_path, \"agrifieldnet_field_ids.parquet\"))\n</pre> gdf_field_ids = gpd.read_parquet(os.path.join(data_path, \"agrifieldnet_field_ids.parquet\")) In\u00a0[\u00a0]: Copied! <pre>gdf_field_ids.explore()\n</pre> gdf_field_ids.explore() In\u00a0[\u00a0]: Copied! <pre>zstats_s2_tmp = []\ns2_bands = list(range(1, 13))\n\nfor b in s2_bands:\n    zstats = zonal_stats(gdf_field_ids, s2_path, stats=[\"mean\"], band=b)\n    zstats_s2_tmp.append(pd.DataFrame(zstats))\n</pre> zstats_s2_tmp = [] s2_bands = list(range(1, 13))  for b in s2_bands:     zstats = zonal_stats(gdf_field_ids, s2_path, stats=[\"mean\"], band=b)     zstats_s2_tmp.append(pd.DataFrame(zstats)) <p><code>zstats_tmp</code> is a list of pandas <code>DataFrame</code>s. Each <code>DataFrame</code> stores the mean spectral reflectance values for the fields for separate spectral bands. Let's look at the zonal statistics results for the first band.</p> In\u00a0[\u00a0]: Copied! <pre>zstats_s2_tmp[0]\n</pre> zstats_s2_tmp[0] <p>The zonal statistics operation has returned <code>None</code> for every field.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>For the zonal statistics operation to work, you will need to align and reproject the vector data to match the raster dataset. We can do this by calling the <code>to_crs()</code> method on a <code>GeoDataFrame</code> and passing in the CRS of the raster dataset. This is a geometry transformation operation.</p> <pre># get the CRS for the s2 DataArray\ns2_crs = s2.rio.crs\n\nzstats_s2_tmp = []\ns2_bands = list(range(1, 13))\n\n## Look at how we call to_crs() on gdf_field_ids\nfor b in s2_bands:\n    zstats = zonal_stats(gdf_field_ids.to_crs(s2_crs), s2_path, stats=[\"mean\"], band=b)\n    zstats_s2_tmp.append(pd.DataFrame(zstats))\n\nzstats_s2_tmp[0]\n</pre> <p>We also need to convert the crop type labels into a tabular dataset where the crop type of each field is recorded. We can do this using a zonal statistics operation, but we do not want to use the mean as our summarising function.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>zstats = zonal_stats(gdf_field_ids.to_crs(s2.rio.crs), labels_path, band=1, stats=[\"majority\"])\nlabels_df = pd.DataFrame(zstats)\n</pre> <p>The variable <code>zstats_s2_tmp</code> should reference a list of <code>DataFrame</code> objects, where each <code>DataFrame</code> stores the zonal statistics results for a spectral band. However, we need to combine the separate <code>DataFrame</code> objects into one <code>DataFrame</code>. We can do this by concatenating (stacking) the <code>DataFrame</code>'s along an axis. Let's inspect the first <code>DataFrame</code> in <code>zstats_s2_tmp</code>.</p> In\u00a0[\u00a0]: Copied! <pre>zstats_s2_tmp[0]\n</pre> zstats_s2_tmp[0] <p>We can see that there is a single column of data which represents the mean spectral reflectance values for band 1 for each of the nine fields. We can combine this data with the <code>GeoDataFrame</code> storing the field id and boundary geometry using the pandas <code>concat()</code> function. <code>concat()</code> takes a list of <code>DataFrame</code>s as an argument and an axis to stack the data items along; here, we want to append columns so we stack along the second axis (axis 1 - remember Python indexes from 0).</p> In\u00a0[\u00a0]: Copied! <pre># first, let's concatenate all the zonal statistics results for the Sentinel-2 bands\ns2_df = pd.concat(zstats_s2_tmp, axis=1, ignore_index=True)\n</pre> # first, let's concatenate all the zonal statistics results for the Sentinel-2 bands s2_df = pd.concat(zstats_s2_tmp, axis=1, ignore_index=True) In\u00a0[\u00a0]: Copied! <pre># second, let's concatenate the zonal statistics results for the Sentinel-2 bands with the field id and boundaries\ngdf_s2 = pd.concat([gdf_field_ids, s2_df], axis=1, ignore_index=True)\ngdf_s2.columns = [\"field_id\", \"geometry\"] + [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"] \ngdf_s2.head()\n</pre> # second, let's concatenate the zonal statistics results for the Sentinel-2 bands with the field id and boundaries gdf_s2 = pd.concat([gdf_field_ids, s2_df], axis=1, ignore_index=True) gdf_s2.columns = [\"field_id\", \"geometry\"] + [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]  gdf_s2.head() <p>You will note that we updated the column names by assigning a list to the <code>columns</code> attribute of the <code>gdf_s2</code> <code>GeoDataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>gdf_labels = pd.concat([gdf_field_ids, labels_df], axis=1, ignore_index=True)\ngdf_labels.columns = [\"field_id\", \"geometry\"] + [\"crop_label\"]\n</pre> <p>As there is a <code>geometry</code> column in both <code>GeoDataFrame</code>s, let's drop it from <code>gdf_s2</code> to avoid it being duplicated in the joined dataset.</p> In\u00a0[\u00a0]: Copied! <pre>df_s2 = gdf_s2.drop(columns=[\"geometry\"])\n</pre> df_s2 = gdf_s2.drop(columns=[\"geometry\"]) <p>Review the pandas <code>merge()</code> documentation to see how we're setting up our join. The <code>how</code> argument specifies the type of join (<code>\"inner\"</code> means we only want to keep rows in both datasets where there is a match on the joining column) and the column to join on (<code>\"field_id\"</code> is the common column in both datasets that we're joining on).</p> In\u00a0[\u00a0]: Copied! <pre>gdf_joined = gdf_labels.merge(df_s2, how=\"inner\", on=\"field_id\")\n</pre> gdf_joined = gdf_labels.merge(df_s2, how=\"inner\", on=\"field_id\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>You could print out the columns of the joined dataset to check that we have columns corresponding to the crop type label and the spectral reflectance bands. <code>GeoDataFrame</code>s and <code>DataFrame</code>s have a <code>columns</code> attribute that stores the column names as a <code>list</code>.</p> <pre>print(gdf_joined.columns)\n</pre> <p></p> <p>You could also print out the first few rows of the dataset to check it looks sensible. The <code>DataFrame</code>'s <code>head()</code> method does this.</p> <pre>gdf_joined.columns()\n</pre> In\u00a0[\u00a0]: Copied! <pre>india_districts = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-4_2\", \"geoBoundaries-IND-ADM2_simplified.topojson\"))\nindia_districts.head()\n</pre> india_districts = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-4_2\", \"geoBoundaries-IND-ADM2_simplified.topojson\")) india_districts.head() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>india_districts = india_districts.loc[:, [\"shapeName\", \"geometry\"]]\n</pre> <p>Next, let's tidy up our subsetted dataset by giving it neat column names and setting the CRS.</p> In\u00a0[\u00a0]: Copied! <pre>india_districts.columns = [\"district\", \"geometry\"]\nindia_districts = india_districts.set_crs(\"EPSG:4326\")\nindia_districts.head()\n</pre> india_districts.columns = [\"district\", \"geometry\"] india_districts = india_districts.set_crs(\"EPSG:4326\") india_districts.head() In\u00a0[\u00a0]: Copied! <pre>india_districts.plot(column=\"district\")\n</pre> india_districts.plot(column=\"district\") <p>Now we can implement our spatial join. We use the <code>sjoin()</code> function from <code>GeoPandas</code> and specify the two datasets we are spatially joining. Again, we use an inner join where we keep only rows in the left and right <code>GeoDataFrame</code>s where there is a match in space. Here, we determine a match in space where the polygon in the left <code>GeoDataFrame</code> intersects with a polygon in the right <code>GeoDataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre>gdf_district = gpd.sjoin(\n    left_df=gdf_joined, \n    right_df=india_districts, \n    how=\"inner\", \n    predicate=\"intersects\"\n)\ngdf_district.head()\n</pre> gdf_district = gpd.sjoin(     left_df=gdf_joined,      right_df=india_districts,      how=\"inner\",      predicate=\"intersects\" ) gdf_district.head() <p>We now have a clean and tidy tabular dataset ready for analysis (e.g. building machine learning models to classify crop type based on spectral reflectance).</p>"},{"location":"notebooks/week-4_2/#data-wrangling-vector-and-tabular-data","title":"Data wrangling: vector and tabular data\u00b6","text":"<p>Often, datasets need to go through a series of data wrangling and transformation steps before they are ready for analysis or visualisation tasks. This lab will demonstrate several data wrangling and transformation operations focussing on vector and tabular data.</p> <p>We will work with a subset of the AgriFieldNet Competition Dataset (Radiant Earth Foundation and IDinsight, 2022) which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images. This dataset consists of a series of directories with each directory corresponding to a 256 x 256 pixel Sentinel-2 satellite image footprint and information about field boundaries and crop types within each image.</p> <p>This data is subset from a larger dataset covering agricultural fields in four Indian states: Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsight's Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation.</p> <p>Often, datasets for machine learning computer vision tasks (e.g. see the datasets on Radiant Earth's Source Cooperative) are provided with data samples for model development spread across many sub-directories. Prior to model training you need to extract the data from these directories and assemble it in a way that it can be passed into a model. The process of transforming data to a format ready for machine learning model development is called feature engineering. This lab will demonstrate how to convert the image format data into a vector-tabular dataset where each row corresponds to a field with columns for spectral reflectance measured by the Sentinel-2 sensor and a label for the crop type of the field. This dataset can then be used for machine learning tasks to predict what crop is growing in a field using satellite images, specifically relating patterns of spectral reflectance to a crop type. In this lab you will use the following data wrangling operations:</p> <ul> <li>zonal statistics using raster and vector data to compute mean Sentinel-2 spectral reflectance within each field boundary.</li> <li>combining datasets by stacking columns and rows to create larger tables and implementing spatial and non-spatial joins.</li> <li>subsetting operations to extract data from tabular <code>DataFrame</code>s.</li> <li>geometry operations to reproject and change the shape of geometries representing field locations.</li> </ul>"},{"location":"notebooks/week-4_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-4_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-4_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-4_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-4_2/#quick-recap-what-is-data-wrangling","title":"Quick recap: What is data wrangling?\u00b6","text":"<p>Wickham and Grolemund (2017) and McKinney (2022) state that data wrangling consists of data import, data cleaning, and data transformation.</p>"},{"location":"notebooks/week-4_2/#data-import","title":"Data import\u00b6","text":"<p>Data import was covered in week 3 with examples of how to read tabular, vector, and raster data into Python programs.</p>"},{"location":"notebooks/week-4_2/#data-cleaning","title":"Data cleaning\u00b6","text":"<p>Data cleaning includes handling outliers and missing data. Here, we'll cloud mask Sentinel-2 remote sensing images, which is a data cleaning exercise.</p>"},{"location":"notebooks/week-4_2/#data-transformation","title":"Data transformation\u00b6","text":"<p>McKinney (2022) define data transformation as mathematical or statistical operations applied to data to generate new datasets. Data transformation can also include operations that reshape datasets or combine two or more datasets.</p>"},{"location":"notebooks/week-4_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-4_2/#lab-data","title":"Lab data\u00b6","text":"<p>Let's quickly inspect the files that we have downloaded for this lab.</p>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Why have we used the <code>sel()</code> method to select the bands 4, 3, and 2 in that order?</p> answer <p>Bands 4, 3, and 2 correspond to reflectance in the red, green, and blue wavelengths. Subsetting these bands allows us to render the image as an RGB true colour composite on our display.</p>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you check that the <code>xarray.DataArray</code> object storing the crop type labels has the same coordinate reference system (CRS) and X and Y dimensions as the <code>xarray.DataArray</code> object storing the satellite image?</p>"},{"location":"notebooks/week-4_2/#zonal-statistics","title":"Zonal statistics\u00b6","text":"<p>We can use the <code>zonal_stats()</code> function from the rasterstats package to perform zonal statistics in Python. Zonal statistics summarise raster pixel values within zones. Here, we're going to compute the average spectral reflectance values recorded in our Sentinel-2 image dataset for each of the fields that we have a crop type label for.</p> <p>The <code>zonal_stats()</code> function takes in vector data of geometries (zones) that summary statistics are computed for as its first argument (or a file path to a vector file), the path to a raster dataset (i.e. a GeoTIFF file) as its second argument, a band argument that specifies which band of the raster data to compute zonal statistics for, and optionally a list of statistics to compute to the <code>stats</code> argument. You can find the full list of parameters for the <code>zonal_stats()</code> function in the package documentation.</p> <p>Can you look up what the <code>all_touched</code> parameter of the <code>zonal_stats()</code> function is used for in the rasterstats docs?</p> <p>Let's write a small routine that will loop over each band in our <code>xarray.DataArray</code> object <code>s2</code>, compute the zonal statistics for that band, and convert the result to a pandas <code>DataFrame</code> object.</p>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you explore the <code>xarray.DataArray</code> object <code>s2</code> and the <code>GeoDataFrame</code> object <code>gdf_field_ids</code> to see why our zonal statistics operation is returning <code>None</code>?</p> <p>Tip: for the zonal statistics operation to work, the raster and vector data need to be aligned in space. Consider checking the CRS of the datasets, and, if necessary changing the CRS of the <code>GeoDataFrame</code> <code>gdf_field_ids</code> to see if that helps.</p>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you compute the zonal statistics for each field using the crop type labels GeoTIFF file referenced by <code>labels_path</code> and using the majority function to summarise the raster data within zones?</p> <p>Refer to the rasterstats docs for information on selecting the summarising function for the zonal statistics operation.</p>"},{"location":"notebooks/week-4_2/#combining-datasets","title":"Combining datasets\u00b6","text":"<p>Often, we have to deal with datasets that are spread across files or objects in our programs and we need to combine them into one dataset for analysis. There are two common techniques for combining datasets: concatenating (stacking) or joining.</p> <p>Concatenating or stacking datasets is the process of appending data items (e.g. adding new columns or rows to an existing <code>DataFrame</code>).</p> <p>Joins refer to combining datasets based on the relationships between the two dataset. Key-based / tabular joins combine datasets by matching on common variables. Spatial joins combine datasets by matching datasets based on feature's relationships in space.</p>"},{"location":"notebooks/week-4_2/#concatenating-stacking-datasets","title":"Concatenating / stacking datasets\u00b6","text":""},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you combine the <code>labels_df</code> <code>DataFrame</code> with the <code>gdf_field_ids</code> <code>GeoDataFrame</code> using the pandas <code>concat()</code> function? Assign the result to the variable <code>gdf_labels</code>.</p>"},{"location":"notebooks/week-4_2/#key-based-joins","title":"Key-based joins\u00b6","text":"<p>We now have two separate <code>GeoDataFrame</code> objects in our program. We have a <code>GeoDataFrame</code> storing average spectral reflectance values for each field referenced by <code>gdf_s2</code> and a <code>GeoDataFrame</code> storing the crop type label for each field in <code>gdf_labels</code>. There is a matching column in both of these <code>GeoDataFrame</code>s - <code>field_id</code>.</p> <p>When two tables have a matching column(s) we can use join operations to merge them. Rows in both tables are matched using common values in the matching column(s) and the joined table has columns from both tables.</p> <p>Joining tables is a common operation in relational databases using SQL and the same operations can be implemented in Pandas using the <code>merge()</code> functions.</p> <p>Some important concepts for join operations:</p> <ul> <li>The columns with values used to match rows are called keys.</li> <li>one-to-one joins are where there is exactly one match between rows in the two tables being joined.</li> <li>many-to-one joins are where a row in one table can match one or more rows in another table.</li> <li>left joins keep all rows in the left table and only matching rows in the right table.</li> <li>inner joins keep only matching rows in the left and right tables.</li> </ul> <p>The Pandas <code>merge()</code> docs and McKinney (2022) provide useful explanations for how join operations work.</p> <p></p> <p>Let's consider these concepts in the context of joining our <code>GeoDataFrame</code> of each field's spectral reflectance values with a <code>GeoDataFrame</code> of each field's crop type label.</p> <p>The matching column in both tables is <code>field_id</code>. This the joining key.</p> <p>We are joining the two tables on <code>field_id</code> which should be unique to each field. Therefore, we are implementing a one-to-one join. We only want to keep fields where there is a crop type value and spectral reflectance values. Therefore, we'll use an inner join.</p> <p>Pandas <code>merge()</code> function can take the following arguments:</p> <ul> <li><code>left</code> - left table in the join.</li> <li><code>right</code> - right table in the join.</li> <li><code>how</code> - whether to use a left or inner join.</li> <li><code>left_on</code> - columns in left table to use as keys.</li> <li><code>right_on</code> - columns in the right table to use as keys.</li> </ul>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you check that our join has been completed correctly? Think about what the <code>DataFrame</code> should look like after a completed join and how you can inspect the <code>DataFrame</code> to check this.</p>"},{"location":"notebooks/week-4_2/#spatial-joins","title":"Spatial Joins\u00b6","text":"<p>Spatial join operations join the attributes of two vector layers based on their relationship in space. For example, if we have a <code>GeoDataFrame</code> storing field boundaries (polygon geometries) and field attributes and another <code>GeoDataFrame</code> storing district boundaries (polygon geometries) and a district name as an attribute, we can join the the two tables based on the largest intersection (overlap) between field boundaries and district boundaries. If the field boundaries <code>GeoDataFrame</code> was the left table in the spatial join, for each row (or geometry feature) the district name from the district with largest intersection would be joined to that table in a new column.</p> <p>GeoPandas provides an <code>sjoin()</code> function that can be used for spatial joins of two <code>GeoDataFrames</code>. The <code>sjoin()</code> function expects the following as arguments:</p> <ul> <li><code>left_df</code> - left <code>GeoDataFrame</code> in the spatial join.</li> <li><code>right_df</code> - right <code>GeoDataFrame</code> in the spatial join - columns from the <code>right_df</code> will be joined to <code>left_df</code>.</li> <li><code>how</code> - whether to use a left, inner, or right join.</li> <li><code>predicate</code> - a binary predicate that defines the spatial relationship between features in <code>right_df</code> and <code>left_df</code>.</li> </ul> <p>Binary predicates that can be used are:</p> <ul> <li>intersects</li> <li>contains</li> <li>crosses</li> <li>within</li> <li>touches</li> <li>overlaps</li> </ul> <p>Intersects is the default predicate for spatial joins in GeoPandas.</p> <p></p> <p>It would be useful to include some information about where in India the fields we're working with are located. To do this can perform a spatial join to combine our field boundaries with the boundaries of administrative units in India, districts in this case.</p> <p>We need to read in district geometries for India obtained from geoBoundaries.</p>"},{"location":"notebooks/week-4_2/#subsetting-pandas-dataframes","title":"Subsetting pandas <code>DataFrame</code>s\u00b6","text":"<p>To subset data from a pandas <code>DataFrame</code> we use the square brackets <code>[]</code> to specify the data values we'd like to extract. This is an example of non-spatial subsetting.</p> <p>Selection by labels</p> <p>Selection by labels refers to selecting values from a <code>DataFrame</code> by their label (i.e. column name).</p> <ul> <li>To select a column from a <code>DataFrame</code> we pass the column name into <code>[]</code> (e.g. <code>india_districts = india_districts[\"shapeName\"]</code> where <code>shapeName</code> is a <code>Series</code> object).</li> <li>To select many columns from a <code>DataFrame</code> we pass a list of column names into <code>[]</code> (e.g. (e.g. <code>india_districts = india_districts[[\"shapeName\", \"geometry\"]]</code>).</li> </ul> <p>Selection by position</p> <p>Selection by position refers to selecting values from a <code>DataFrame</code> by their index position (i.e. row or column number starting at 0).</p> <ul> <li>To select the $n^{th}$ row pass in <code>[n-1:n]</code>. Remember that Python indexes from zero so the $n-1$ index position is the $n^{th}$ row. The slice operator <code>:</code> is exclusive so <code>[n-1:n]</code> will only select the row at <code>n-1</code> (e.g. to select the $2^{nd}$ row use <code>df_row_2 = gdf[1:2]</code>).</li> <li>To select a slice of rows use the slice operator (e.g. to select the first 10 rows use <code>df_10_rows = gdf[0:10]</code>).</li> </ul> <p>Selection by condition</p> <p>Selection by condition selects rows that are <code>True</code> based on a condition (e.g. selecting all rows with a <code>shapeArea</code> greater than 1 - <code>india_districts_gt1 = india_districts[india_districts[\"shapeArea\"] &gt; 1]</code>).</p> <p><code>loc[]</code> and <code>iloc[]</code></p> <p>The more robust approach to subsetting data from <code>DataFrame</code>s is using the <code>loc</code> and <code>iloc</code> methods.</p> <ul> <li><code>loc</code> is used for selecting by labels (e.g. <code>india_districts = india_districts.loc[:, [\"shapeName\", \"geometry\"]]</code> - note the <code>[:, [\"shapeName\", \"geometry\"]]</code> syntax where <code>:</code> means select all rows).</li> <li><code>iloc</code> is used for selecting by position (e.g. to select the first 10 rows use <code>df_10_rows = gdf.iloc[0:10, :]</code>).</li> <li>To select the first 10 rows and columns we'd use <code>df_10_rows_10_cols = gdf.iloc[0:10, 0:10]</code>).</li> </ul>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"How many rows will the object referenced by <code>df_temp</code> have after calling <code>df_temp=df.iloc[0:5, :]</code>? 5 rows at index positions 0 to 4 from the <code>DataFrame</code> <code>df</code>.  How many columns from <code>df</code> will the object referenced by <code>df_temp</code> have after calling <code>df_temp=df.iloc[0:5, :]</code>? All the columns from <code>df</code>.  <p>Use <code>loc</code> and the <code>GeoDataFrame</code> referenced by <code>india_districts</code> to select the <code>shapeName</code> and <code>geometry</code> columns. Assign the result to the variable <code>india_districts</code>.</p>"},{"location":"notebooks/week-4_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>If you were to expand this workflow to be handle a large number of fields across many satellite image scenes, what Python tools might help you automate this process?</p> answer <p>For loops. You could include this data transformation process inside a for loop and iterate over a sequence of satellite images and converting them to tabular datasets.</p>"},{"location":"notebooks/week-4_practice/","title":"Week 4 practice","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-4_practice\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_practice.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-4_practice.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-4_practice\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-4_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-4_practice\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_practice.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-4_practice.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-4_practice\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-4_practice been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n    !pip install rasterstats\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio     !pip install rasterstats In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport os\nimport pprint \nimport plotly.express as px\nimport plotly.io as pio\n\nfrom rasterstats import zonal_stats\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import pandas as pd import geopandas as gpd import numpy as np import os import pprint  import plotly.express as px import plotly.io as pio  from rasterstats import zonal_stats  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>obs_event_path = os.path.join(os.getcwd(), \"data_lab-4_practice\", \"EMSR489_AOI07_GRA_PRODUCT_observedEventA_r1_v1.shp\")\nobs_event_gdf = gpd.read_file(obs_event_path)\nobs_event_gdf.explore()\n</pre> obs_event_path = os.path.join(os.getcwd(), \"data_lab-4_practice\", \"EMSR489_AOI07_GRA_PRODUCT_observedEventA_r1_v1.shp\") obs_event_gdf = gpd.read_file(obs_event_path) obs_event_gdf.explore() In\u00a0[\u00a0]: Copied! <pre>obs_event_gdf.head()\n</pre> obs_event_gdf.head() <p>We're going to be using these geometries in a zonal statistics operation. This is a raster-vector operation where we compute summary statistics for all raster pixels that intersect with a polygon geometry. Here, we'll use a Sentinel-2 NDVI image captured just after Tropical Cyclone Yasa impacted this region. However, this satellite image has the projection system <code>EPSG:32760</code>.</p> <p>Can you use the GeoPandas <code>GeoDataFrame</code> method <code>to_crs()</code> to convert <code>obs_event_gdf</code> to <code>EPSG:32760</code>? Assign the variable name <code>obs_event_gdf_32760</code> to the reprojected <code>GeoDataFrame</code>.</p> <p>The GeoPandas docs for reprojection are here if you want to look up how to do this.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>obs_event_gdf_32760 = obs_event_gdf.to_crs(\"EPSG:32760\")\n</pre> <p></p> Why do we need the polygon geometries and raster data to be in the same projection system for zonal statistics operations? <p>To ensure the polygon geometries are intersected with raster pixels that correspond to the same location on the Earth's land surface.</p> <p></p> <p>Our task is to compute summary statistics for each of the polygon geometries that delineate an area impacted by the tropical cyclone event. However, for comparison, we'd also like to generate some sample polygons of areas without a visible impact. We can do this through a sequence of geoprocessing operations that manipulate geometry objects.</p> <p>The first step is to combine our <code>GeoDataFrame</code> of many polygon objects into a single multipolygon object. We can do this using the <code>unary_union</code> property of a <code>GeoSeries</code> (i.e. the <code>geometry</code> column in a <code>GeoDataFrame</code>). The <code>unary_union</code> computes the union of many polygon objects. The is a geometry aggregation operation, combining many geometry objects into one.</p> <p>Below is a quick visualisation of a multipolygon object. We can use multipolygon objects in a number of ways in our programs. For example, we could use multipolygon objects to conveniently represent a country consisting of many islands as a single feature. However, here we'll use them as they help simplify some of the geometry operations we wish to perform in subsequent tasks.</p> In\u00a0[\u00a0]: Copied! <pre>import shapely\nshapely.geometry.MultiPolygon([\n    shapely.geometry.Polygon([(0.8,0.8), (0.9,1.0), (1.0,0.9), (0.9,0.9), (0.8,0.8)]), \n    shapely.geometry.Polygon([(0.0,0.1), (0.2,0.4), (0.4,0.3), (0.0,0.0), (0.0,0.1)])\n])\n</pre> import shapely shapely.geometry.MultiPolygon([     shapely.geometry.Polygon([(0.8,0.8), (0.9,1.0), (1.0,0.9), (0.9,0.9), (0.8,0.8)]),      shapely.geometry.Polygon([(0.0,0.1), (0.2,0.4), (0.4,0.3), (0.0,0.0), (0.0,0.1)]) ]) <p>If we print the multipolygon object, you can see the geometries are represented as a nested collection of polygons.</p> In\u00a0[\u00a0]: Copied! <pre>print(\n    shapely.geometry.MultiPolygon([\n        shapely.geometry.Polygon([(0.8,0.8), (0.9,1.0), (1.0,0.9), (0.9,0.9), (0.8,0.8)]), \n        shapely.geometry.Polygon([(0.0,0.1), (0.2,0.4), (0.4,0.3), (0.0,0.0), (0.0,0.1)])\n    ])\n)\n</pre> print(     shapely.geometry.MultiPolygon([         shapely.geometry.Polygon([(0.8,0.8), (0.9,1.0), (1.0,0.9), (0.9,0.9), (0.8,0.8)]),          shapely.geometry.Polygon([(0.0,0.1), (0.2,0.4), (0.4,0.3), (0.0,0.0), (0.0,0.1)])     ]) ) <p>Let's quickly inspect the structure of our <code>GeoDataFrame</code> with each polygon geometry on its own row.</p> In\u00a0[\u00a0]: Copied! <pre>obs_event_gdf_32760.head()\n</pre> obs_event_gdf_32760.head() <p>Now, let's compute the <code>unary_union</code> of the <code>geometry</code> column. It should return to us a <code>GeoSeries</code> with one element (a multipolygon object that has aggregated all the polygons in our <code>GeoDataFrame</code> into a single feature).</p> In\u00a0[\u00a0]: Copied! <pre>obs_event_gdf_union = gpd.GeoSeries(obs_event_gdf_32760[\"geometry\"].unary_union).set_crs(\"EPSG:32760\")\nobs_event_gdf_union\n</pre> obs_event_gdf_union = gpd.GeoSeries(obs_event_gdf_32760[\"geometry\"].unary_union).set_crs(\"EPSG:32760\") obs_event_gdf_union <p>Now we have combined our polygons into a single multipolygon object, we can use the buffer operation to expand the geometries by a specified distance to cover areas outside the locations impacted by Tropical Cyclone Yasa.</p> <p>Can you apply a 100 m buffer to the multipolygon object referenced by <code>obs_event_gdf_union</code> and reference it with the variable name <code>obs_event_gdf_buffer</code>?</p> <p>Hint: use the GeoPandas docs for the buffer operation.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>obs_event_gdf_buffer = obs_event_gdf_union.buffer(100)\n</pre> <p></p> <p>Next, we'll need to set the CRS for the buffered geometries back to <code>EPSG:32760</code>. In the map below, areas where disaster impact following the tropical cyclone was observed are coloured red and adjacent areas where not disaster impact was observed are coloured blue.</p> In\u00a0[\u00a0]: Copied! <pre>obs_event_gdf_buffer = obs_event_gdf_buffer.set_crs(\"EPSG:32760\")\nm = obs_event_gdf_buffer.explore()\nobs_event_gdf_32760.explore(m=m, color=\"red\")\n</pre> obs_event_gdf_buffer = obs_event_gdf_buffer.set_crs(\"EPSG:32760\") m = obs_event_gdf_buffer.explore() obs_event_gdf_32760.explore(m=m, color=\"red\") <p>We want to use this data as sample polygons where there was no disaster impact. Therefore, we need to remove the interior areas of the polygons which correspond to the locations where disaster impact on the land surface was recorded (these are the red polygons on the map above). We can do this using the GeoPandas <code>difference()</code> method; this returns to us geometries of all the locations in one geometry that are not in another geometry.</p> <p></p> <p>Can you use the <code>difference()</code> method of the <code>GeoDataFrame</code> <code>obs_event_gdf_buffer</code> to remove locations where a disaster event impact was recorded (represented in the <code>GeoDataFrame</code> <code>obs_event_gdf_union</code>)?</p> <p>Assign the result to the variable name <code>obs_event_no_impact</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>obs_event_no_impact = obs_event_gdf_buffer.difference(obs_event_gdf_union)\n</pre> <p></p> <p>Let's check the result looks sensible.</p> In\u00a0[\u00a0]: Copied! <pre>obs_event_no_impact.explore()\n</pre> obs_event_no_impact.explore() Why did we compute the buffer and difference operations on a multipolygon (created via the <code>unary_union</code>) instead of using a <code>GeoDataFrame</code> of many polygon geometries? <p>If we computed the buffer operation on many separate polygons, we'd return many larger buffered polygons which would be overlapping in places. This, in itself, is not necessarily an issue. But, we use the buffered polygons in a <code>difference()</code> operation to remove interior areas where disaster impacts occurred. The <code>difference()</code> operation works on a row-by-row basis (i.e. the geometry in row 1 of <code>GeoDataFrame</code> A is compared to the geometry of row 1 in <code>GeoDataFrame</code> B - the geometry in row 1 of <code>GeoDataFrame</code> A is NOT compared to the geometry of row 2 in <code>GeoDataFrame</code> B). This is problematic if the disaster impact area of represented by the geometry in row 2 in <code>GeoDataFrame</code> B overlaps with the geometry in row 1 of <code>GeoDataFrame</code> A. In this case we would not be able to fully remove areas where disaster impact was recorded. However, by comparing two multipolygon objects on a row-by-row basis (there is just one row in <code>GeoDataFrame</code> A and B) we can ensure all areas where disaster impact was recorded are removed.</p> <p>Now, let's convert the <code>GeoSeries</code> multipolygon object into a <code>GeoSeries</code> of many individual polygon objects using the <code>explode</code> method. This operation takes each of the polygons that comprise the composite multipolygon feature and returns them as a <code>GeoSeries</code> of separate polygon features.</p> Why have we used the <code>explode</code> method to split a multipolygon representation of areas not impacted by the disaster event into separate polygons? We will use these polygons to capture the NDVI signal of areas not impacted by Tropical Cyclone Yasa. If we compute the mean NDVI value for all pixels that intersect with the multipolygon object we'll get a single value for the entire region of interest. This single mean NDVI value might mask lots of spatial variation in the NDVI signal of not impacted areas; computing the NDVI value for many smaller polygons can provide information on variation in the NDVI values of not impacted areas across the scene. Also, if we have many geometries representing not impacted areas which we can compute mean NDVI values for, we can compare these NDVI values with the mean NDVI of nearby polygons where there was a disaster impact. This might give us a more accurate indication of the impact of the Tropical Cyclone on vegetation cover as we're more likely to be comparing similar land covers or exposure to cyclone event.    In\u00a0[\u00a0]: Copied! <pre>obs_event_no_impact\n</pre> obs_event_no_impact In\u00a0[\u00a0]: Copied! <pre>obs_event_no_impact = obs_event_no_impact.explode(ignore_index=True)\nobs_event_no_impact\n</pre> obs_event_no_impact = obs_event_no_impact.explode(ignore_index=True) obs_event_no_impact <p>Finally, let's do some <code>GeoDataFrame</code> subsetting and concatenation to combine polygons corresponding to disaster event impact with the no impact polygons.</p> <p>First, let's make the <code>obs_event_no_impact</code> <code>GeoSeries</code> a <code>GeoDataFrame</code> with an <code>obj_desc</code> column to match the observed disaster event impact <code>GeoDataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre>obs_event_no_impact_id = pd.DataFrame({\"obj_desc\": [\"no impact\" for x in range(0,35)]})\nobs_event_no_impact = gpd.GeoDataFrame(obs_event_no_impact_id, geometry=obs_event_no_impact, crs=\"EPSG:32760\")\nobs_event_no_impact.head()\n</pre> obs_event_no_impact_id = pd.DataFrame({\"obj_desc\": [\"no impact\" for x in range(0,35)]}) obs_event_no_impact = gpd.GeoDataFrame(obs_event_no_impact_id, geometry=obs_event_no_impact, crs=\"EPSG:32760\") obs_event_no_impact.head() <p>Next, let's just get the <code>obj_desc</code> and the <code>geometry</code> columns from <code>obs_event_gdf_32760</code> and concatenate the two <code>GeoDataFrames</code> (i.e. stack them on top of each other row wise).</p> Why are we subsetting out the the <code>obj_desc</code> and <code>geometry</code> columns? We are subsetting these two columns to match the columns in the <code>obs_event_no_impact</code> <code>GeoDataFrame</code>. This allows us to stack the <code>DataFrame</code>s on top of each other row wise with the columns matching up.   In\u00a0[\u00a0]: Copied! <pre>obs_event_gdf_32760 = obs_event_gdf_32760.loc[:, [\"obj_desc\", \"geometry\"]]\nsample_polygons = pd.concat([obs_event_gdf_32760, obs_event_no_impact], axis=0)\nsample_polygons\n</pre> obs_event_gdf_32760 = obs_event_gdf_32760.loc[:, [\"obj_desc\", \"geometry\"]] sample_polygons = pd.concat([obs_event_gdf_32760, obs_event_no_impact], axis=0) sample_polygons In\u00a0[\u00a0]: Copied! <pre>ndvi_zonal_stats = zonal_stats(sample_polygons, os.path.join(os.getcwd(), \"data_lab-4_practice\", \"ndvi_post_yasa_int16.tif\"), stats=[\"mean\"], all_touched=True)\npprint.pprint(f\"print the mean NDVI for the first 10 polygons: {ndvi_zonal_stats[0:10]}\")\n</pre> ndvi_zonal_stats = zonal_stats(sample_polygons, os.path.join(os.getcwd(), \"data_lab-4_practice\", \"ndvi_post_yasa_int16.tif\"), stats=[\"mean\"], all_touched=True) pprint.pprint(f\"print the mean NDVI for the first 10 polygons: {ndvi_zonal_stats[0:10]}\") <p>We can convert the list of dictionary objects into a <code>DataFrame</code> using the <code>DataFrame</code> constructor function.</p> In\u00a0[\u00a0]: Copied! <pre>zonal_stats_df = pd.DataFrame(ndvi_zonal_stats)\nzonal_stats_df.head()\n</pre> zonal_stats_df = pd.DataFrame(ndvi_zonal_stats) zonal_stats_df.head() <p>The NDVI data is in integer format and has been scaled by 1000 (it is cheaper to store integer data than floating point data). However, NDVI values should be between -1 and 1. Let's fix this by dividing the NDVI values by 1000.</p> In\u00a0[\u00a0]: Copied! <pre>zonal_stats_df[\"mean\"] = zonal_stats_df[\"mean\"] / 1000\nzonal_stats_df.head()\n</pre> zonal_stats_df[\"mean\"] = zonal_stats_df[\"mean\"] / 1000 zonal_stats_df.head() <p>Finally, we can add the column of <code>mean</code> NDVI values for each polygon back to our <code>GeoDataFrame</code>. This means we can display our polygons and map the fill colour of each polygon to its mean NDVI value.</p> In\u00a0[\u00a0]: Copied! <pre>sample_polygons[\"ndvi\"] = zonal_stats_df.loc[:, [\"mean\"]]\nsample_polygons.head()\n</pre> sample_polygons[\"ndvi\"] = zonal_stats_df.loc[:, [\"mean\"]] sample_polygons.head() In\u00a0[\u00a0]: Copied! <pre>sample_polygons.explore(column=\"ndvi\")\n</pre> sample_polygons.explore(column=\"ndvi\") Can you identify some limitations of this approach to generating a sample of areas not impacted by Tropical Cyclone Yasa? <ul> <li>We used the areas directly adjacent to observed impact as our no impact sample. If there is error in how the disaster impact was delineated, it is likely that a disaster impact signal might leak into zones we've labelled as not impacted. A better strategy might be to use sample locations a specified distance from an observed impact.</li> <li>There could be differences in NDVI across land covers. If there are different land covers in the impacted area and not impacted area, we could be assigning a difference in NDVI due to land cover differences to disaster impact.</li> <li>We could use difference NDVI images. Comparing the change in NDVI before and after disaster event in areas where a disaster impact was observed to areas where it was not.</li> </ul> <p></p> What data transformation and geoprocessing operations could you deploy if you just wanted to focus on the disaster impacts to croplands? <p>You could obtain a land cover map with a cropland class and mask all pixels that are not classified as cropland before computing zonal stats (i.e. set their values <code>NaN</code>).</p>"},{"location":"notebooks/week-4_practice/#week-4-practice-exercises","title":"Week 4 - Practice Exercises\u00b6","text":"<p>This notebook contains a range of practice exercises that demonstrate how data transformation and geoprocessing operations can be sequenced to complete a task. Here, we have a vector dataset of polygon geometries which correspond to areas in Fiji impacted by Tropical Cyclone Yasa. These geometries were digitised by the European Commission Copernicus Emergency Management Service (EMS) - Rapid Mapping Activations.</p> <p>We also have a post tropical cyclone event NDVI image computed from Sentinel-2 satellite data. We wish to i) compute the average NDVI value of pixels inside each polygon and ii) generate a sample of polygons where there was no observed disaster impact and compute the average NDVI values for these no-impact polygons.</p> <p>To compute the average NDVI values for all pixels that intersect a polygon geometry we will use a raster-vector operation called zonal statistics.</p> <p>However, before we get to the stage where we can compute zonal statistics we need to pre-process our polygon geometries. This involves reprojecting the geometries to match the coordinate reference system of the raster data and performing a range of geometry operations to generate a sample of no-impact polygons. Geometry operations involve manipulating the shape of geometric objects.</p> <p>Geometry operations that manipulate shapes include computing the centroids of polygons (transforming a polygon object to a point), buffering a geometry (increasing the size of a geometry), computing the intersection between two geometries (reducing the area, length, or size of a geometry based on its relationship with another geometry), or combining two geometries by computing their union. It is important to be able to look up different geometry operations in the GeoPandas documentation. Geometry operations are important parts of data transformation and geoprocessing workflows where raw or input spatial data is converted into a format for subsequent tasks such as statistical analysis, machine learning, or data visualisation.</p>"},{"location":"notebooks/week-4_practice/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-4_practice/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-4_practice/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-4_practice/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-4_practice/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-4_practice/#geometry-operations","title":"Geometry operations\u00b6","text":"<p>Let's load the vector polygon data that delineates the observed disaster impact following Tropical Cyclone Yasa and visualise it to see it's structure.</p>"},{"location":"notebooks/week-4_practice/#zonal-stats","title":"Zonal stats\u00b6","text":"<p>We now have 269 sample polygons of locations affected by Tropical Cyclone Yasa and those without visible impacts. We want to compute the average NDVI values across all pixels that intersect each polygon. This is a zonal statistics operation.</p> <p>We can use the <code>zonal_stats()</code> function from the rasterstats package to perform zonal statistics in Python.</p> <p>The <code>zonal_stats()</code> function takes in vector data of geometries that zonal statistics are computed for as its first argument (or a file path to a vector file), the path to a raster dataset (i.e. a GeoTIFF file) as its second argument, and optionally a list of statistics to compute to the <code>stats</code> argument. You can find the full list of parameters for the <code>zonal_stats()</code> function in the package documentation.</p> <p>Can you look up what the <code>all_touched</code> parameter of the <code>zonal_stats()</code> function is used to specify in the rasterstats docs?</p> <p>We have a GeoTIFF file of NDVI values computed from a Sentinel-2 image captured just after Tropical Cyclone Yasa at the file path <code>os.path.join(os.getcwd(), \"data_lab-4_practice\", \"ndvi_post_yasa_int16.tif\")</code>.</p> <p>Let's use it to compute the average NDVI value for each of the polygons in the <code>GeoDataFrame</code> <code>sample_polygons</code>. <code>zonal_stats</code> returns to us a dictionary object for each polygon with the summary statistic label as a key and the computed statistic as the value.</p>"},{"location":"notebooks/week-5_1/","title":"Week 5 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-5\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-5\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-5\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-5\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>%%HTML\n'&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mc9QG2R-rf4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt;'\n</pre> %%HTML '' In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import tree\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n\nrng = np.random.RandomState(0)\n</pre> import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import plotly.io as pio import os  from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay from sklearn.model_selection import GroupShuffleSplit from sklearn.inspection import permutation_importance from sklearn import tree  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\"  rng = np.random.RandomState(0) In\u00a0[\u00a0]: Copied! <pre>data_path = os.path.join(os.getcwd(), 'data_lab-5', 'agrifieldnet_processed_adm4.geojson')\ngdf = gpd.read_file(data_path)\n</pre> data_path = os.path.join(os.getcwd(), 'data_lab-5', 'agrifieldnet_processed_adm4.geojson') gdf = gpd.read_file(data_path) In\u00a0[\u00a0]: Copied! <pre>print(f\"The data is of type: {type(gdf)}\")\ngdf.head()\n</pre> print(f\"The data is of type: {type(gdf)}\") gdf.head() <p>Based on the dataset's documentation the below is the mapping between numeric values and crop types in the labels dataset.</p> <ul> <li>1 - Wheat</li> <li>2 - Mustard</li> <li>3 - Lentil</li> <li>4 - No crop/Fallow</li> <li>5 - Green pea</li> <li>6 - Sugarcane</li> <li>8 - Garlic</li> <li>9 - Maize</li> <li>13 - Gram</li> <li>14 - Coriander</li> <li>15 - Potato</li> <li>16 - Bersem</li> <li>36 - Rice</li> </ul> <p>Let's explore how many examples we have of different crop types. We can see that our dataset is dominated by wheat, mustard, and no crop / fallow labels.</p> In\u00a0[\u00a0]: Copied! <pre># make labels categorical for bar plot\nclass_mappings = {\n    \"1\": \"Wheat\",\n    \"2\": \"Mustard\",\n    \"3\": \"Lentil\",\n    \"4\": \"Fallow\",\n    \"5\": \"Green pea\",\n    \"6\": \"Sugarcane\",\n    \"8\": \"Garlic\",\n    \"9\": \"Maize\",\n    \"13\": \"Gram\",\n    \"14\": \"Coriander\",\n    \"15\": \"Potato\",\n    \"16\": \"Bersem\",\n    \"36\": \"Rice\"\n}\n\ngdf[\"labels_cat\"] = gdf[\"labels\"].astype(\"str\")\ngdf.replace({\"labels_cat\": class_mappings}, inplace=True)\n\ngdf.groupby(\"labels_cat\").count().loc[:, \"field_id\"]\n</pre> # make labels categorical for bar plot class_mappings = {     \"1\": \"Wheat\",     \"2\": \"Mustard\",     \"3\": \"Lentil\",     \"4\": \"Fallow\",     \"5\": \"Green pea\",     \"6\": \"Sugarcane\",     \"8\": \"Garlic\",     \"9\": \"Maize\",     \"13\": \"Gram\",     \"14\": \"Coriander\",     \"15\": \"Potato\",     \"16\": \"Bersem\",     \"36\": \"Rice\" }  gdf[\"labels_cat\"] = gdf[\"labels\"].astype(\"str\") gdf.replace({\"labels_cat\": class_mappings}, inplace=True)  gdf.groupby(\"labels_cat\").count().loc[:, \"field_id\"] <p>We can also explore the spatial distribution of the data. Hover over the points on the map with your cursor.</p> In\u00a0[\u00a0]: Copied! <pre>gdf.explore(\"labels_cat\", tiles=\"CartoDB dark_matter\", cmap=\"tab20\", categorical=True)\n</pre> gdf.explore(\"labels_cat\", tiles=\"CartoDB dark_matter\", cmap=\"tab20\", categorical=True) <p>There are some final preprocessing steps required before we are ready to train a model to classify a field's crop type.</p> <p>Scikit-learn models expect the input data and outcomes to be <code>array-like</code>. Generally, this is in the form of NumPy <code>ndarray</code> objects.</p> <p>We want the input data (features or predictors) to be in a separate object to the outcomes (labels). Therefore, we'll subset the <code>GeoDataFrame</code> object and store just the predictor variables in an <code>array-like</code> object <code>X</code> and the outcomes in an object <code>y</code>.</p> <p>Numeric Pandas <code>Series</code> or <code>DataFrame</code> objects are <code>array-like</code> and so we can directly subset columns from the <code>GeoDataFrame</code> to create input and output objects.</p> <p><code>X</code> generally has the shape <code>(n_samples, n_features)</code> where each sample is aligned along the rows dimension (or 0-axis in a rank 2 NumPy <code>ndarray</code>) and the features (or predictors) are aligned along the columns dimension (or 1-axis in a rank 2 NumPy <code>ndarray</code>).</p> In\u00a0[\u00a0]: Copied! <pre>X = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"index_right\", \"village\", \"geometry\"], axis=1)\ny = gdf.loc[:, \"labels\"]\n</pre> X = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"index_right\", \"village\", \"geometry\"], axis=1) y = gdf.loc[:, \"labels\"] In\u00a0[\u00a0]: Copied! <pre>X.head()\n</pre> X.head() <p>For classification tasks the values in <code>y</code> should be integer and for regression tasks the values in <code>y</code> should be floating point. As crop type is a categorical variable values in <code>y</code> should be of integer data type.</p> In\u00a0[\u00a0]: Copied! <pre>y.head()\n</pre> y.head() In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng, test_size=0.3)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng, test_size=0.3) In\u00a0[\u00a0]: Copied! <pre>print(f\"the size of the training features object is {X_train.shape}\") \nprint(f\"the size of the test features object is {X_test.shape}\")\nprint(f\"the size of the training outcomes object is {y_train.shape}\")\nprint(f\"the size of the test features object is {y_test.shape}\")\n</pre> print(f\"the size of the training features object is {X_train.shape}\")  print(f\"the size of the test features object is {X_test.shape}\") print(f\"the size of the training outcomes object is {y_train.shape}\") print(f\"the size of the test features object is {y_test.shape}\") In\u00a0[\u00a0]: Copied! <pre># create and train a random forests model\nrf = RandomForestClassifier(n_estimators=20, random_state=rng)\nrf.fit(X_train, y_train.astype(int))\n</pre> # create and train a random forests model rf = RandomForestClassifier(n_estimators=20, random_state=rng) rf.fit(X_train, y_train.astype(int)) In\u00a0[\u00a0]: Copied! <pre>y_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n</pre> y_pred = rf.predict(X_test)  print(classification_report(y_test, y_pred)) <p>From the classification report, we can ascertain the following:</p> <ul> <li>the model's overall accuracy is 0.68 - 68% of the examples in the test set were classified correctly.</li> <li>the model's performance is better for class labels 1 (wheat), 4 (fallow), and 9 (maize).</li> <li>the performance metric scores for the other classes is lower.</li> <li>the model's performance is best for classes with the most observations in the training dataset.</li> <li>we're getting a warning indicating to us that the precision and f1-score are being set to zero in labels with no predicted samples.</li> </ul> <p>We can also plot a confusion matrix to see if there are patterns of confusion between classes.</p> In\u00a0[\u00a0]: Copied! <pre>labels = [\"Wheat\", \n          \"Mustard\", \n          \"Lentil\", \n          \"Fallow\", \n          \"Green Pea\",\n          \"Sugarcane\",\n          \"Garlic\",\n          \"Maize\",\n          \"Gram\",\n          \"Coriander\",\n          \"Potato\",\n          \"Bersem\",\n          \"Rice\"]\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot(text_kw={\"fontsize\":10}, xticks_rotation=\"vertical\")\nplt.show()\n</pre> labels = [\"Wheat\",            \"Mustard\",            \"Lentil\",            \"Fallow\",            \"Green Pea\",           \"Sugarcane\",           \"Garlic\",           \"Maize\",           \"Gram\",           \"Coriander\",           \"Potato\",           \"Bersem\",           \"Rice\"] cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(text_kw={\"fontsize\":10}, xticks_rotation=\"vertical\") plt.show() <p>From the confusion matrix, we can see:</p> <ul> <li>there is confusion between the mustard and wheat classes.</li> <li>a large number of minitory classes as misclassified as wheat or mustard (majority classes).</li> <li>there were no successful classifications of coriander, garlic, or bersem.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre># import the tree module from scikit-learn\nfrom sklearn import tree\n\n# create a decision tree classifier object\nclf = tree.DecisionTreeClassifier(random_state=0)\n\n# train the model\nclf.fit(X_train, y_train)\n\n# test model\ny_pred_tree = clf.predict(X_test)\nprint(classification_report(y_test, y_pred_tree))\n</pre> In\u00a0[\u00a0]: Copied! <pre>all_preds = rf.predict(X)\n</pre> all_preds = rf.predict(X) <p>We can then append the predicted crop type labels to our initial <code>GeoDataFrame</code> as a new column and visualise these predictions on a map. Hover over points on the map with your cursor to see the actual (<code>labels_cat</code>) and predicted (<code>predicted</code>) crop types for a field.</p> In\u00a0[\u00a0]: Copied! <pre>gdf[\"predicted\"] = all_preds.astype(\"str\")\ngdf.replace({\"predicted\": class_mappings}, inplace=True)\nbasemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nattribution = \"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\ngdf.explore(column=\"predicted\", cmap=\"tab20\", categorical=True, tiles=basemap, attr=attribution, tooltip=[\"labels_cat\", \"predicted\"])\n</pre> gdf[\"predicted\"] = all_preds.astype(\"str\") gdf.replace({\"predicted\": class_mappings}, inplace=True) basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\" attribution = \"Tiles \u00a9 Esri \u2014 Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\" gdf.explore(column=\"predicted\", cmap=\"tab20\", categorical=True, tiles=basemap, attr=attribution, tooltip=[\"labels_cat\", \"predicted\"]) In\u00a0[\u00a0]: Copied! <pre>px.histogram(gdf, x=\"labels_cat\")\n</pre> px.histogram(gdf, x=\"labels_cat\") In\u00a0[\u00a0]: Copied! <pre>print(\"the number of samples by class in our overall dataset (pre-split) are:\")\ngdf.groupby('labels_cat').count().loc[:, \"field_id\"]\n</pre> print(\"the number of samples by class in our overall dataset (pre-split) are:\") gdf.groupby('labels_cat').count().loc[:, \"field_id\"] <p>Data leakage occurs when information in the test set leaks into the training dataset. This means the test set is not truly independent and does not provide an unbiased assessment of the model's performance on new data.</p> <p>Spatial correlation occurs when observations close to each other are more similar or disimilar than observations further away. This is encapsulated by Tobler's first law of Geography: \"Everything is related to everything else. But near things are more related than distant things.\"</p> <p>Geospatial data is often spatially correlated. This means that data points close to each other are not statistically independent. A random training and test split of spatially correlated data can result in the test dataset not being independent of the training dataset. This is because some of the data in the test set is correlated with data in the training set. Spatial correlation is causing data leakage and the evaluation of model performance using this test set will be biased.</p> In\u00a0[\u00a0]: Copied! <pre>X_sp = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"predicted\", \"index_right\", \"village\", \"geometry\"], axis=1)\ny_sp = gdf.loc[:, \"labels\"]\ngroups = gdf.loc[:, \"village\"]\n</pre> X_sp = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"predicted\", \"index_right\", \"village\", \"geometry\"], axis=1) y_sp = gdf.loc[:, \"labels\"] groups = gdf.loc[:, \"village\"] <p>scikit-learn has a <code>GroupShuffleSplit</code> object that has a <code>split()</code> method that can be used to generate splits of the dataset.</p> <p>First, we need to create an instance of the <code>GroupShuffleSplit</code> object specifying the number of different splits of the dataset that we want to create using the <code>n_splits</code> argument. We also use the <code>train_size</code> argument to define how much of the data should be allocated to the training and test sets.</p> <p>Here, we only want to create one split of our dataset at the <code>village</code> level so we set <code>n_splits=1</code>.</p> <p>Then, we call the <code>split()</code> method of <code>gss</code>, our <code>GroupShuffleSplit</code> object, passing in the features (<code>X_sp</code>), outcome labels (<code>y_sp</code>), and the groups (<code>groups</code>). This returns to us a <code>train_index</code> and <code>test_index</code> specifying the index locations of samples allocated to the training and test set. Passing in <code>groups</code> ensures that no samples from the same group (<code>village</code>) are in both the training and test sets.</p> <p>We then use the index locations in <code>train_index</code> and <code>test_index</code> to subset <code>X_sp</code> and <code>y_sp</code> for model training and testing.</p> In\u00a0[\u00a0]: Copied! <pre>gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(gss.split(X_sp, y_sp, groups)):\n    print(f\"processing split {i}\")\n    X_train_sp = X_sp.iloc[train_index, :]\n    X_test_sp = X_sp.iloc[test_index, :]\n    y_train_sp = y_sp.iloc[train_index]\n    y_test_sp = y_sp.iloc[test_index]\n    print(f\"the size of the training features object is {X_train_sp.shape}\") \n    print(f\"the size of the test features object is {X_test_sp.shape}\")\n    print(f\"the size of the training outcomes object is {y_train_sp.shape}\")\n    print(f\"the size of the test outcomes object is {y_test_sp.shape}\")\n</pre> gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=0)  for i, (train_index, test_index) in enumerate(gss.split(X_sp, y_sp, groups)):     print(f\"processing split {i}\")     X_train_sp = X_sp.iloc[train_index, :]     X_test_sp = X_sp.iloc[test_index, :]     y_train_sp = y_sp.iloc[train_index]     y_test_sp = y_sp.iloc[test_index]     print(f\"the size of the training features object is {X_train_sp.shape}\")      print(f\"the size of the test features object is {X_test_sp.shape}\")     print(f\"the size of the training outcomes object is {y_train_sp.shape}\")     print(f\"the size of the test outcomes object is {y_test_sp.shape}\") <p>Now we're ready to train and test our model.</p> <p>Let's train the model and test its performance.</p> In\u00a0[\u00a0]: Copied! <pre># create and train a random forests model\nrf_sp = RandomForestClassifier(n_estimators=20, random_state=rng)\nrf_sp.fit(X_train_sp, y_train_sp.astype(int))\n</pre> # create and train a random forests model rf_sp = RandomForestClassifier(n_estimators=20, random_state=rng) rf_sp.fit(X_train_sp, y_train_sp.astype(int)) In\u00a0[\u00a0]: Copied! <pre>y_pred_sp = rf.predict(X_test_sp)\n\nprint(classification_report(y_test_sp, y_pred_sp))\n</pre> y_pred_sp = rf.predict(X_test_sp)  print(classification_report(y_test_sp, y_pred_sp))"},{"location":"notebooks/week-5_1/#machine-learning-1","title":"Machine learning 1\u00b6","text":"<p>This lab will provide an introduction to key machine learning concepts and also demonstrate how you can use Scikit-learn to implement machine learning workflows in Python.</p> <p>The focus of this lab will be on supervised machine learning. This lab will develop a machine learning workflow that classifies the crop type of fields in India using spectral reflectance values recorded by the Sentinel-2 satellite. It is based on the AgriFieldNet Competition Dataset (Radiant Earth Foundation and IDinsight, 2022) which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images.</p> <p>The dataset includes spectral reflectance values, crop type labels, field id, and geometry for the field's location from cropping landscapes in the Indian States of Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsights Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation.</p>"},{"location":"notebooks/week-5_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-5_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-5_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-5_1/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-5_1/#machine-learning","title":"Machine learning\u00b6","text":"<p>Machine learning is the process of learning from data to make predictions. Supervised machine learning models are trained to predict an outcome based on input data (predictors or features). The model is trained to minimise the error in predictions using a training set where both the outcome labels and input data are known. If the outcome is categorical (e.g. land cover type, cloud / no-cloud) then it is a classification machine learning task and if the outcome is numeric (e.g. crop yield, temperature) then it is a regression machine learning task.</p> <p>There are also unsupervised machine learning tasks where there are no known outcomes prior to model training. Unsupervised machine learning models typically cluster datasets with similar data points assigned to the same cluster or group.</p> <p>Please watch this introduction to machine learning video from Climate Change AI:</p>"},{"location":"notebooks/week-5_1/#scikit-learn","title":"Scikit-learn\u00b6","text":"<p>Scikit-learn is an open-source machine learning package for Python. It provides a range of tools for preprocessing datasets for machine learning, training machine learning models, evaluating model performance, and using a trained model to make predictions. A range of supervised and unsupervised machine learning algorithms can be used with Scikit-learn.</p>"},{"location":"notebooks/week-5_1/#task","title":"Task\u00b6","text":"<p>The focus of this lab will be on supervised machine learning. This lab will develop a machine learning workflow that classifies the crop type of fields in India using spectral reflectance values recorded by the Sentinel-2 satellite. It is based on the AgriFieldNet Competition Dataset (Radiant Earth Foundation and IDinsight, 2022) which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images.</p> <p>The dataset includes spectral reflectance values, crop type labels, field id, and geometry for the field's location from cropping landscapes in the Indian States of Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsights Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation.</p>"},{"location":"notebooks/week-5_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-5_1/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/week-5_1/#data-pre-processing","title":"Data pre-processing\u00b6","text":"<p>Often, after sourcing data, the first task in a machine learning workflow is data preprocessing - transforming the raw data into a format ready for model training or making predictions. These tasks are often referred to as feature engineering - the process of engineering or creating features or predictor variables.</p> <p>Let's inspect the data. We can see it is a <code>GeoDataFrame</code> with columns corresponding to the <code>field_id</code>, <code>labels</code> (crop type identifier), spectral reflectance in several wavebands (<code>B*</code>) and <code>ndvi</code>, the <code>village</code> where the field is located in India, and <code>geometry</code> <code>POINT</code> object for the field centroid.</p>"},{"location":"notebooks/week-5_1/#train-test-splits","title":"Train-test splits\u00b6","text":"<p>For supervised machine learning tasks we need to create training and test datasets.</p> <p>The model is trained using the training set which consists of matched features and outcomes.</p> <p>The model is then evaluated using the test set. A prediction is made using features in the test set and the prediction is compared with known outcomes for those features. This provides an indication of the model's performance. It is important that the test set is independent from the training set - an important part of machine learning model development is preventing information from the test set leaking into the training set.</p> <p>Scikit-learn provides a useful <code>train_test_split()</code> function which expects <code>X</code> and <code>y</code> <code>array-like</code> objects as inputs and will return 4 <code>array-like</code> objects (<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>).</p> <p>We can provide further arguments to <code>train_test_split()</code>:</p> <ul> <li><code>test_size</code> determines the proportion of the input data that is allocated to the test set</li> <li><code>random_state</code> is a seed that ensures the same random split of the data occurs each time the code is executed. This is important for reproduciblity of results.</li> </ul>"},{"location":"notebooks/week-5_1/#model-training","title":"Model training\u00b6","text":"<p>Scikit-learn provides a range of machine learning algorithms that can be trained for different tasks (e.g. classification, regression, text, images, clustering etc.).</p> <p>In Scikit-learn terminology each of these algorithms is called an <code>estimator</code> - the docs have a useful interactive guide to help you select the right <code>estimator</code> for your machine learning task.</p> <p>Each <code>estimator</code> object has a <code>fit()</code> method. The fit method expects the training data as arguments (<code>X_train</code> and <code>y_train</code>) and when called learns rules that minimise the error in predicting the outcome labels in <code>y_train</code>. This is the learning part of the machine learning workflow.</p> <p>Here, we will demonstrate how to train a tree-based machine learning model: a random forests classifier.</p> <p>First, we create an <code>estimator</code> object for the model. Then, we use the <code>estimator</code>'s <code>fit()</code> method to train the model.</p>"},{"location":"notebooks/week-5_1/#random-forests-classifiers","title":"Random forests classifiers\u00b6","text":"<p>Random forests models are an ensemble and tree-based model. They're a tree-based model as they consist of an ensemble of decision tree classifiers.</p> <p>Please read through this Google Machine Learning Guide on decision trees and random forests.</p> Detailed notes on tree-based models <p>Decision tree classifiers are trained to learn rules that classify outcome labels based on input features by recursively splitting the feature space. The algorithm starts by finding the value of a feature that splits the dataset into two groups which minimise the \"impurity\" of outcome labels in each group. Then, that process is repeated by splitting each of the two groups, again to minimise the \"impurity\" of outcome labels. This process repeats until a stopping criterion is reached. The Gini index is the default metric to measure class impurity in each internal node of the tree.</p> <p>The class label associated with each of the terminal nodes of the tree is based on the most commonly occurring class.</p> <p>Individual decision tree classifiers are relatively quick to train, can learn non-linear and interactive relationships between input features and outcome labels, and are easy to visualise and interpret.</p> <p>However, there are limits to decision tree classifiers. They are often not the most accurate classifiers. They also have high variance; if you train a decision tree classifier on two different samples it will likely learn different relationships and generate different predictions. Large decision trees can also overfit the training data; they can learn to fully represent the structure of the training set but will not generalise well to new and unseen data.</p> <p>Random forests models mitigate the limitations of a single decision tree classifier by:</p> <p>bagging: training a number (ensemble) of decision trees based on bootstrap samples of the training datasets. The average prediction from many decision tree models reduces the variance in predictions.</p> <p>sampling features at each split: when training each of the decision trees in the ensemble, a random selection of features are searched for each split within the tree. This prevents a small number of features from dominating the model, enables the model to learn using all the input features, and reduces overfitting. If there are p features, then often the m \u221ap are considered at each split.</p> <p>majority vote: for classification tasks, the final predicted value from a random forest model is the most common prediction of the outcome label across all trees in the ensemble.</p> <p></p> <p>Let's create a random forest model <code>estimator</code> object using the <code>RandomForestClassifier()</code> function. We'll set the <code>n_estimators</code> parameter to 20 here; this means the random forest will consist of an ensemble of 20 decision tree classifiers. The <code>random_state</code> parameter ensures we learn the same model each time we train it on the same data; this is important for reproducible results.</p>"},{"location":"notebooks/week-5_1/#model-evaluation","title":"Model evaluation\u00b6","text":"<p>After training a model, we need to evaluate it to assess its performance. This evaluation allows us to compare different models and get an indication of how well the model will perform when it is used on new data.</p> <p>After training, the model object, <code>rf</code> in this case stores rules that map input data to output (predicted) labels. In the case of a random forests model, these rules are stored and expressed as a collection of decision trees.</p> <p>It is important that the test data used to evaluate a model is independent of the training data; this is to ensure an unbiased estimate of model performance.</p> <p>There are a range of model evaluation metrics for classification tasks:</p> <ul> <li>accuracy: the proportion of correctly classified examples.</li> <li>recall: the ratio of true positives to true positives and false negatives - the ability of the classifier to capture all positive cases. $recall = \\frac{tp}{tp+fn}$.</li> <li>precision: the ratio of true positives to true positives and false positives - the classifiers ability not to label something as positive when it is not. $precision = \\frac{tp}{tp+fp}$.</li> <li>f1-score: the f1-score combines the recall and precision scores and takes on the range 0 to 1. $F1 = 2\\cdot\\frac{precision\\cdot{recall}}{precision+recall}$</li> </ul> <p>Scitkit-learn provides a <code>classification_report()</code> which can be used to generate performance metrics for each class and the model as a whole.</p> <p>The <code>classification_report()</code> expects known outcome labels and predicted outcome labels. To generate the predicted labels, we can use the <code>predict()</code> method of the <code>estimator</code> object and pass in input test data.</p>"},{"location":"notebooks/week-5_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Earlier, we discussed that random forests models should be more accurate than a single decision tree classifier. Can you train a decision tree classifier to test if this the case?</p> <p>The documentation for the decision tree classifier is here.</p> <p>Use <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, and <code>y_test</code> for this task</p>"},{"location":"notebooks/week-5_1/#visualising-predictions","title":"Visualising predictions\u00b6","text":"<p>We have a <code>DataFrame</code> <code>X</code> which stores all the features before we split the data into training and test splits. We can use <code>X</code> to generate a predicted crop type label for each of our data points.</p>"},{"location":"notebooks/week-5_1/#class-imbalance","title":"Class imbalance\u00b6","text":"<p>Our training and test datasets are clearly imbalanced across the outcome class labels. The majoring of samples are of wheat (1), mustard (2), or fallow (4) classes.</p> <p>We can see the imbalance in our dataset using a bar plot.</p>"},{"location":"notebooks/week-5_1/#recap-quiz","title":"Recap quiz\u00b6","text":"How could imbalanced data affect model performance? <ul> <li>The model will not see enough examples of minority classes to learn rules to discriminate them from the input data</li> <li>The model will learn it can achieve good overall accuracy by just predicting majority classes</li> </ul> What could we do to fix the class imbalance problem? <ul> <li>Undersample the majority classes</li> <li>Oversample the minority classes</li> <li>Get more data</li> <li>Pool the minority classes to reduce the total number of classes</li> </ul>"},{"location":"notebooks/week-5_1/#data-leakage","title":"Data leakage\u00b6","text":""},{"location":"notebooks/week-5_1/#recap-quiz","title":"Recap quiz\u00b6","text":"How could you generate training and test splits which are not spatially correlated? First, you could explore the spatial correlation in your dataset using techniques such as Moran's I and Local Moran's I statistics.   <p>If you believe that your samples are not likely to be correlated across administrative boundaries such as villages, counties, states etc. you could randomly split your data at the administrative boundary-level as opposed to the sample-level. That is, instead of taking a random hold-out sample of data points for the test set you would take a random sample of administrative units as the test set and all data points inside those units would be your test set.</p> <p>An alternative strategy if there are no useful administrative boundaries could be to spatially cluster your samples using their coordinates so proximal data points are allocated to the same cluster and randomly hold-out some clusters as the test set.</p> <p></p> <p>Let's use the <code>village</code> column in our dataset <code>gdf</code> as a group to guide generation of training and test sets. We'll ensure that no samples from the same village are in both the training and test sets.</p> What is our assumption when using villages as the grouping variable? We are assuming that data points in neighbouring villages are not spatially correlated, and, therefore, there is no data leakage from the the test set to the training set. Is this a safe assumption? Do you think villages next to each other will have different agricultural contexts?"},{"location":"notebooks/week-5_2/","title":"Week 5 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-5\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-5\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-5\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-5\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import tree\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n\nrng = np.random.RandomState(0)\n</pre> import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import plotly.io as pio import os  from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay from sklearn.model_selection import GroupShuffleSplit from sklearn.model_selection import cross_val_score from sklearn.model_selection import cross_validate from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.inspection import permutation_importance from sklearn import tree  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\"  rng = np.random.RandomState(0) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-5\", \"lobell_2019_maize.csv\"))\nprint(f\"the shape of the DataFrame is {df.shape}\")\ndf.head()\n</pre> df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-5\", \"lobell_2019_maize.csv\")) print(f\"the shape of the DataFrame is {df.shape}\") df.head() In\u00a0[\u00a0]: Copied! <pre>fig = px.histogram(\n    data_frame=df, \n    x=\"cc_yield\",  \n    marginal=\"box\"\n)\nfig.show()\n</pre> fig = px.histogram(     data_frame=df,      x=\"cc_yield\",       marginal=\"box\" ) fig.show() <p>The <code>DataFrame</code> stores Sentinel-2 derived vegetation indices in the following columns:</p> <ul> <li><code>gcvi_doy_151</code> - average field GCVI on day of year 151.</li> <li><code>gcvi_doy_171</code> - average field GCVI on day of year 171.</li> <li><code>ndvi_doy_151</code> - average field NDVI on day of year 151.</li> <li><code>ndvi_doy_171</code> - average field NDVI on day of year 171.</li> <li><code>mtci_doy_151</code> - average field MTCI on day of year 151.</li> <li><code>mtci_doy_171</code> - average field MTCI on day of year 171.</li> </ul> <p>NDVI is the normalised difference vegetation index. GCVI is the green chlorophyll index. MTCI is the meris terrestrial chlorophyll index. These will be the main features (predictors) in our model.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>fig = px.scatter(\n    df,\n    x = \"gcvi_doy_151\", ## CHANGE THIS FOR DIFFERENT VEGETATION INDICES\n    y = \"cc_yield\",\n    trendline = \"ols\",\n    opacity=0.25,\n    labels={\"cc_yield\": \"Maize crop yield (Mg/ha)\",\n           \"gcvi_doy_151\": \"GCVI (DOY 151)\"} ## CHANGE THIS FOR DIFFERENT VEGETATION INDICES\n)\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre># get X and y data\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n</pre> # get X and y data  # drop nas as Linear Regression object cannot be trained on datasets with missing data df_linear_reg = df.loc[: , [     \"cc_yield\",     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\"]].dropna()  # get X X = df_linear_reg.loc[:, [     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\"]]  # get Y y = df_linear_reg.loc[:, \"cc_yield\"] In\u00a0[\u00a0]: Copied! <pre># create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_val_score(reg, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n</pre> # create a LinearRegression estimator object reg = LinearRegression()  # evaluate using 5-fold cross validation cv_scores = cross_val_score(reg, X, y, cv=5, scoring=\"neg_mean_absolute_error\") <p><code>cv_scores</code> should reference an array of values recording the mean absolute error for the predictions of maize crop yield for each fold. Scikit-learn returns negative mean absolute error values (becauase their convention is that a higher metric values are better than lower metric values which holds for metrics for categorical outcomes such as accuracy). Therefore, we'll want to convert negative mean absolute error values to positive.</p> In\u00a0[\u00a0]: Copied! <pre># print cross validation test scores\nfor i, mae in enumerate(cv_scores):\n    print(f\"the mae for the {i}th fold is {round(abs(mae), 4)}\")\n</pre> # print cross validation test scores for i, mae in enumerate(cv_scores):     print(f\"the mae for the {i}th fold is {round(abs(mae), 4)}\") <p>If we want to use more than one metric to evaluate the model, we can pass in a list of metrics to the <code>scoring</code> argument. Let's also estimate the mean squared error value as well as the mean absolute error. The mean squared error penalises the model more for predictions with larger error.</p> <p>To use multiple metrics we need to use the <code>cross_validate()</code> function instead.</p> In\u00a0[\u00a0]: Copied! <pre># evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"])\ncv_scores\n</pre> # evaluate using 5-fold cross validation cv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"]) cv_scores In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>print(f\"mean mae: {abs(cv_scores['test_neg_mean_absolute_error'].mean())}\")\nprint(f\"mean mse: {cv_scores['test_neg_mean_squared_error'].mean()}\")\n</pre> <p></p> <p>Can you train and evaluate a random forests model using 5-fold cross-validation to see if it improves the predictive performance?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>rf = RandomForestRegressor(n_estimators=20, random_state=rng)\nrf_cv_scores = cross_validate(rf, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\nrf_cv_scores\n</pre> In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() <p>Now, let's one hot encode the <code>slope_sr</code> variable and see how it is represented as numeric data. (scroll to the far right of the displayed <code>DataFrame</code>).</p> In\u00a0[\u00a0]: Copied! <pre>df_cat = pd.get_dummies(df, columns=[\"slope_sr\"])\n</pre> df_cat = pd.get_dummies(df, columns=[\"slope_sr\"]) In\u00a0[\u00a0]: Copied! <pre>df_cat.head()\n</pre> df_cat.head() In\u00a0[\u00a0]: Copied! <pre>df_cat.columns\n</pre> df_cat.columns <p>Now, let's retrain our linear regression model using slope as a feature.</p> In\u00a0[\u00a0]: Copied! <pre># get X and y data\n\n## NOTE WE ARE USING df_cat here!!\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_cat.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"])\ncv_scores\n</pre> # get X and y data  ## NOTE WE ARE USING df_cat here!! # drop nas as Linear Regression object cannot be trained on datasets with missing data df_linear_reg = df_cat.loc[: , [     \"cc_yield\",     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",     \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]].dropna()  # get X X = df_linear_reg.loc[:, [     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",     \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]]  # get Y y = df_linear_reg.loc[:, \"cc_yield\"]  # create a LinearRegression estimator object reg = LinearRegression()  # evaluate using 5-fold cross validation cv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"]) cv_scores In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>df_2 = pd.get_dummies(df, columns=[\"slope_sr\", \"soiltype_sr\", \"soilqual_sr\"])\ndf_2.head()\n</pre> <p>Can you use <code>df_2</code> with <code>soiltype_sr</code>, <code>slope_sr</code>, and <code>soilqual_sr</code> as training data in a random forests model?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre># get X and y data\n\n# NOTE WE USE df_2 here!!\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_2.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\", \n    \"soiltype_sr_CLAY\", \n    \"soiltype_sr_LOAM\",\n    \"soiltype_sr_OTHER (SPECIFY)\", \n    \"soiltype_sr_SANDY\", \n    \"soilqual_sr_FAIR\",\n    \"soilqual_sr_GOOD\", \n    \"soilqual_sr_POOR\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\", \n    \"soiltype_sr_CLAY\", \n    \"soiltype_sr_LOAM\",\n    \"soiltype_sr_OTHER (SPECIFY)\", \n    \"soiltype_sr_SANDY\", \n    \"soilqual_sr_FAIR\",\n    \"soilqual_sr_GOOD\", \n    \"soilqual_sr_POOR\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\nrf = RandomForestRegressor(n_estimators=20, random_state=rng)\nrf_cv_scores = cross_validate(rf, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\nrf_cv_scores\n</pre> In\u00a0[\u00a0]: Copied! <pre># NOTE we use df_cat here!!\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_cat.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# fit the model\nreg.fit(X, y)\n</pre> # NOTE we use df_cat here!!  # drop nas as Linear Regression object cannot be trained on datasets with missing data df_linear_reg = df_cat.loc[: , [     \"cc_yield\",     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",      \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]].dropna()  # get X X = df_linear_reg.loc[:, [     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",      \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]]  # get Y y = df_linear_reg.loc[:, \"cc_yield\"]  # create a LinearRegression estimator object reg = LinearRegression()  # fit the model reg.fit(X, y) <p>Now we compute permuation importance using 30 shuffles of each feature. The results referenced by <code>p_imp</code> is a dictionary object with arrays showing the model error when each feature was randomly shuffled and for each repeat of the random shuffling. It also has a property <code>importances_mean</code> which is the mean increase in error across all iterations when a feature was randomly shuffled.</p> <p>We use the <code>permutation_importance()</code> function and pass in the <code>LinearRegression()</code> estimator, the features and labels data (<code>X</code> and <code>y</code>), the metric to evaluate model performance to the <code>scoring</code> argument, and specify the number of repeats with the <code>n_repeats</code> argument.</p> In\u00a0[\u00a0]: Copied! <pre>p_imp = permutation_importance(reg, X, y, scoring=\"neg_mean_absolute_error\", n_repeats=30, random_state=rng)\n</pre> p_imp = permutation_importance(reg, X, y, scoring=\"neg_mean_absolute_error\", n_repeats=30, random_state=rng) In\u00a0[\u00a0]: Copied! <pre>p_imp[\"importances_mean\"]\n</pre> p_imp[\"importances_mean\"] <p>Let's use this data to make a permutation importance plot that visualises the increase in error when a feature is randomly shuffled.</p> <p>First, let's get a list of column headings for each feature and convert the negative mean absolute error values to positive.</p> In\u00a0[\u00a0]: Copied! <pre>columns = X.columns\np_imp = abs(p_imp[\"importances_mean\"])\n</pre> columns = X.columns p_imp = abs(p_imp[\"importances_mean\"]) <p>Now, let's combine the feature labels and importance values into a <code>DataFrame</code>, sort the <code>DataFrame</code> by the importance scores, and generate a bar plot.</p> In\u00a0[\u00a0]: Copied! <pre>p_imp_df = pd.DataFrame({\"feature\": columns, \"importance\": p_imp})\np_imp_df = p_imp_df.sort_values(by=[\"importance\"], ascending=True)\np_imp_df\n</pre> p_imp_df = pd.DataFrame({\"feature\": columns, \"importance\": p_imp}) p_imp_df = p_imp_df.sort_values(by=[\"importance\"], ascending=True) p_imp_df In\u00a0[\u00a0]: Copied! <pre>fig = px.bar(p_imp_df, y=\"feature\", x=\"importance\", height=600)\nfig.show()\n</pre> fig = px.bar(p_imp_df, y=\"feature\", x=\"importance\", height=600) fig.show() answer 1 <p>Here, we filter out any data points representing mixed cropping fields using the condition <code>df[purestand] == 1</code> where a value of 1 in the <code>purestand</code> column indicates pure maize cropping.</p> <p>This approach means our vegetation indices should just be capturing information about maize crop condition.</p> <pre># get X and y data\n\n# drop mixed cropping fields\ndf_pure = df.loc[df[\"purestand\"] == 1, :]\n\n# one hot encode categorical predictors\ndf_pure = pd.get_dummies(df_pure, columns=[\"slope_sr\", \"soiltype_sr\", \"soilqual_sr\"])\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_pure.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\ncv_scores\n</pre> answer 2 <p>Here, we control for the presence of mixed cropping by using intercropping and crop rotation variables as features in the model.</p> <p>This approach might be suited to generating a maize crop prediction model that's applicable to the Ugandan context where mixed cropping is prevalent and pure maize fields are uncommon.</p> <pre># get X and y data\n\n# one hot encode categorical predictors\ndf_2 = pd.get_dummies(df, columns=[\"slope_sr\", \"soiltype_sr\", \"soilqual_sr\"])\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_2.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\",\n    \"intercrop_legume\",\n    \"intercrop_cassava\",\n    \"crop_rotation\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\",\n    \"intercrop_legume\",\n    \"intercrop_cassava\",\n    \"crop_rotation\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\ncv_scores\n</pre>"},{"location":"notebooks/week-5_2/#machine-learning-2","title":"Machine Learning 2\u00b6","text":"<p>Machine learning is the process of learning from data to make predictions. Supervised machine learning models are trained to predict an outcome based on input data (predictors or features). The model is trained to minimise the error in predictions using a training set where both the outcome labels and input data are known. A key part of the machine learning model development workflow is evaluating model performance. This lab will introduce a technique for evaluating model performance in the context of limited training and test data: cross-validation.</p> <p>Previously, we demonstrated a workflow to develop a machine learning model for a classification task: predicting a field's crop type. In this lab we will develop a machine learning model for a regression task (predicting a continuous number) and evaluate the model using k-fold cross-validation.</p> <p>The task for this lab is to develop and evaluate a machine learning model that can predict smallholder farm maize crop yields in Uganda using remotely sensed vegetation indices as input features.</p> <p>In this lab you'll learn how to use Scikit-learns tools for evaluating models using cross-validation. We'll also introduce approaches for pre-processing categorical data to be used as features in machine learning models and techniques for interpreting the model and exploring how the model is making predictions.</p>"},{"location":"notebooks/week-5_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-5_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-5_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-5_2/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-5_2/#cross-validation","title":"Cross-validation\u00b6","text":"<p>A key part of the machine learning model development workflow is evaluating model performance. This should provide an assessment of how well a model will perform in making predictions on new or unseen data. Machine learning models are data hungry, the more examples the model sees during training the better it will be able to learn mappings that relate input features to outcome labels. However, we also want to test our model on a dataset that is representative of conditions the model might encounter \"in-the-wild\"; this results in setting aside a chunk of our ground truth dataset that cannot be used for model training. Thus, our model is not trained using all available ground truth data.</p> <p>One strategy that is deployed to maximise data available for model training and to provide an assessment of model performance is cross-validation.</p> <p>Before we explore our ground truth dataset for model development, let's quickly introduce cross-validation. Previously, we evaluated the model's performance by removing a random sample of the data prior to model training to use as a test set.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Why is it important for the test dataset to be randomly sampled from the ground truth data? We want the test dataset to be representative and unbiased to provide as realistic assessment of the model's performance on new data as possible.   What is a potential limitation of using a single hold-out randomly sampled test set for evaluating model performance? With a randomly sampled test set, each time the machine learning model development workflow is repeated new training and test sets would be generated and the model will have different performance scores. Using a single test set means, that by chance, the model could have an overly optimistic or pessimistic assessment of its performance.  <p>Further, by withholding a test set we reduce the amount of data available to train the model. A smaller training dataset can reduce the model's performance. Thus, as we're removing data to form the test set we'd expect the model's error to be larger than if we'd trained the model on the entire dataset.</p> <p></p> <p>In k-fold cross-validation there is not a single test set. Instead, the ground truth dataset is randomly split into $k$ folds. For example, if $k=5$ the ground truth dataset would be randomly split into 5 groups. Then, in turn, each fold is held out as a test set and the model is trained using data from the remaining four folds. Each fold takes a turn at being the test set. The model performance can be summarised using the average of the performance metrics generated using each fold. This means the model's performance is less susceptible to being influenced by a chance split of the ground truth data into training and test splits. It also means we can use the whole dataset to train the model and evaluate its performance.</p>"},{"location":"notebooks/week-5_2/#maps-2016-data","title":"MAPS 2016 Data\u00b6","text":"<p>The dataset we're using is the data from Lobell et al. (2019). Their analysis compared different approaches to estimating smallholder maize crop yields in Uganda: farmer reported yields, subplot crop cut samples, full plot crop cut samples, and satellite-based crop yield esimates.</p> <p>Boosting agricultural productivity in smallholder landscapes is important for improving a range of livelihood outcomes including food security and poverty alleviation. Accurate data on smallholder farmer crop production is a key ingredient to guiding development initiatives, government policies / programs, agricultural management and input use, and monitoring progress towards several Sustainable Development Goals.</p> <p>Traditionally, agricultural productivity in smallholder landscapes has been measured using farmer reported crop yields via surveys after harvests. These estimates are subject to considerable error impacting the quality of the data.</p> <p>More accurate measures of crop yield include physically harvesting a sub plot or full plot - crop cutting. However, crop cutting is more costly, time consuming, and requires liaising with farmers to generate large datasets of yield measurements.</p> <p>Lobell et al. (2019) explore the potential for using satellite data to measure crop yields in smallholder fields to address i) the issue of error in farmer reported yields, and ii) the cost of crop cutting.</p> <p>We're going to use the replication data from their paper and see if we can develop a machine learning model to accurately predict smallholder maize crop yield using satellite data as inputs. As this dataset only has a few hundred data points, we'll use all the data to train the model and test its performance using cross-validation</p>"},{"location":"notebooks/week-5_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-5_2/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/week-5_2/#explore-data","title":"Explore data\u00b6","text":"<p>There data that we have read into <code>df</code> includes a range of variables related to crop yield outcomes, farm management and farm type, and satellite-derived vegetation indices from the Sentinel-2 sensor.</p> <p>We'll be using the crop cut maize yield measures from sample sub plots in the fields as our outcome variable here - this is referenced by the column <code>cc_yield</code>. The units are Mg/ha.</p> <p>Let's look at the distribution of yield values.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you generate scatter plots to explore the correlation between vegetation index values and maize crop yield?</p>"},{"location":"notebooks/week-5_2/#cross-validation","title":"Cross-validation\u00b6","text":"<p>We'll start by training a linear regression model that predicts maize crop yield as a function of vegetation indices and evaluate its performance using cross validation.</p> <p>Maize crop yield is a continuous variable and we need a metric to evaluate model performance. We'll use the <code>mean_absolute_error</code> as the metric which is the mean absolute difference between predicted and observed crop yields. In this case, the mean absolute error will be computed using observations in the held out fold in cross-validation.</p> <p>We'll need to create a linear regression estimator object that we can train (using its <code>fit()</code> method). To evaluate the model using cross validation you call the <code>cross_val_score()</code> function on the dataset and the estimator. You pass in the metric you wish to use to evaluate the model to the <code>scoring</code> argument.</p> <p>We'll also need to use the <code>dropna()</code> method of pandas <code>DataFrame</code>s to remove missing data before training the model. Scikit-learn models cannot be trained on datasets with <code>NaN</code> values.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you estimate the mean mean absolute error and mean mean squared error across the five test folds using the <code>cv_scores</code> dictionary object?</p>"},{"location":"notebooks/week-5_2/#categorical-features","title":"Categorical features\u00b6","text":"<p>In our <code>DataFrame</code> there are some categorical variables that could help improve our predictions of crop yield. These include the slope, soil type, and soil quality variables. These variables are <code>str</code> type. We can only pass numeric data into our models; therefore, we'll need to recode the text data to a numeric representation.</p> <p>One approach for recoding categorical data is one hot encoding. Each unique value in a one hot encoded categorical variable is assigned a new column in the <code>DataFrame</code>. For rows when this value is present a value of one is assigned and zero otherwise.</p> <p>Let's one hot encode the slope variable <code>slope_sr</code> to illustrate this concept.</p> <p>The pandas <code>get_dummies()</code> function can be used to one hot encode a column in a <code>DataFrame</code>. The <code>get_dummies()</code> function has a <code>columns</code> argument that takes a list of column names that will be one hot encoded.</p> <p>First, let's visualise our <code>DataFrame</code> <code>df</code> and inspect the values in the <code>slope_sr</code> column. You should see the values \"FLAT\", \"MODERATE SLOPE\", \"SLIGHT SLOPE\", \"STEEP SLOPE\" as <code>str</code> data.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you also recode the soil type <code>soiltype_sr</code> and <code>soilqual_sr</code> variables from categorical to numeric using one hot encoding? Reference the result with the variable <code>df_2</code>.</p>"},{"location":"notebooks/week-5_2/#controlling-randomness","title":"Controlling randomness\u00b6","text":"<p>Some elements of the machine learning workflow are inherently random. For example, allocating data points to folds in k-fold cross-validation and bootstrap sampling of data to train decision trees in random forests.</p> <p>While this randomness is important (e.g. to ensure unbiased estimates of model performance when using cross-validation) it presents a challenge for reproducible results. The randomness of estimators (e.g. an instance of <code>RandomForestsRegressor()</code> or a cross-validation splitter is controlled by a <code>random_state</code> parameter.</p> <p>Some general tips on setting the <code>random_state</code> parameter:</p> <ul> <li>Never set <code>random_state</code> to <code>None</code> for reproducible results.</li> <li>Create a <code>RandomState</code> variable at the start of your program and pass it to all functions that accept a <code>random_state</code> argument. Look at the start of this notebook and see if you can sport where we create a <code>RandomState</code> variable just after we import the modules.</li> <li>If you're generating cross validation splits, use an integer value instead of a <code>RandomState</code> instance.</li> </ul> <p>This is quite an advanced topic, but important to ensure your results are reproducible. Generally, following the guidelines above is the best way to go. However, you can read more about this topic here.</p>"},{"location":"notebooks/week-5_2/#feature-importance","title":"Feature importance\u00b6","text":"<p>Machine learning models are often considered \"black boxes\". That is, it is not clear how the model is using input features to make predictions and what relationships it has learnt to relate features to outcomes.</p> <p>One strategy to make machine learning models more interpretable is to compute feature importance (or permutation importance). The feature importance is a measure of how much the error in a model's prediction increases when a feature is omitted from the model. Features with larger importance scores are therefore more important for making accurate predictions.</p> <p>The permutation feature importance score is computed as the decrease in a model's performance when a feature is randomly shuffled (permuted). This should ensure there is no relationship between the feature and the outcome.</p> <p>You can read more about feature importance in the Interpretable Machine Learning book and in the Scikit-learn docs.</p> <p>First, let's set up and fit a linear regression model that predicts maize crop yield using vegetation indices and field characteristics.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Do the feature importance results make sense? Can you explain them? The most important features for predictive importance are the vegetation indices. There is an established literature that vegetation indices are correlated with, and predictive of, crop yields.   <p>However, we should be cautious in interpreting the differences between vegetation indices as it is likely that the vegetation indices are correlated (even if they're designed to capture different information about vegetation growth and condition). When one of the vegetation indices is permuted (shuffled), it is likely the model will still have access to information about this feature through other features in the model which it is correlated with. You can read more about this here.</p> <p></p> Here, we computed the feature importance scores using the training data. Can you think of a limit to computing feature importance with the training set compared to using the test set? Computing feature importance using a held-out test set would indicate which features are important for the model's capacity to generalise well to unseen data. Features that are important for the training set migh be causing the model to overfit. You can read more about this here."},{"location":"notebooks/week-5_2/#final-activity","title":"Final activity\u00b6","text":"<p>You will notice in <code>df</code> that there are some columns related to mixed cropping in some of the maize fields (e.g.<code>intercrop_legume</code>, <code>intercrop_cassava</code>, <code>crop_rotation</code>, <code>purestand</code>). One issue that could be affecting model performance is that we're using average vegetation indices across the whole field but not all of the field is maize cropping. This means that our vegetation index data is not purely capturing a maize crop signal but also the condition of other crops. We might be able to improve the model's performance if we restrict our analysis to pure maize fields or control for the effect of mixed cropping.</p> <p>Can you create a training set that will use one or more of the <code>intercrop_legume</code>, <code>intercrop_cassava</code>, <code>crop_rotation</code>, and <code>purestand</code> variables to train a model to predict maize yield that accounts for the mixed cropping practices inherent in smallholder systems in Uganda. Evaluate your model using cross-validation and justify the rationale for your approach.</p>"},{"location":"notebooks/week-6_1/","title":"Week 6 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-6\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-6.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-6.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-6\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-6 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-6\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-6.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-6.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-6\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-6 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install mapclassify\n    !pip install skimage\n</pre> if 'google.colab' in str(get_ipython()):     !pip install mapclassify     !pip install skimage In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nimport geopandas as gpd\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\nimport requests\nfrom skimage import io\nfrom io import StringIO\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import json import geopandas as gpd import pandas as pd import plotly.express as px import plotly.io as pio import numpy as np import requests from skimage import io from io import StringIO  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>r = requests.get(\"https://cropmonitortools.org/agmet/australia/ww_s1_2023/condition/adm1/western_australia.jpg\")\nr.status_code\n</pre> r = requests.get(\"https://cropmonitortools.org/agmet/australia/ww_s1_2023/condition/adm1/western_australia.jpg\") r.status_code <p>The call to <code>requests.get(\"https://cropmonitortools.org/agmet/australia/ww_s1_2023/condition/adm1/western_australia.jpg\")</code> creates a HTTP request message and sends it to the specified URL.</p> <p>The <code>response</code> object <code>r</code> has a <code>status_code</code> property that tells us if our request was successful or not.</p> In\u00a0[\u00a0]: Copied! <pre>r.status_code\n</pre> r.status_code <p>We can also access the HTTP response message's body where the requested data is stored via the <code>content</code> property. Let's print the first 100 bytes of data requested from the server.</p> In\u00a0[\u00a0]: Copied! <pre>r.content[0:100]\n</pre> r.content[0:100] <p>As the data we have requested is image data, it has been sent to us as binary data and not text. We can see this by the <code>b</code> before the string representation of the data printed on our display.</p> <p>The data for the image file we requested is currently stored as bytes in memory and referenced by the <code>content</code> property of the response object <code>r</code>. Let's save this data to disk.</p> In\u00a0[\u00a0]: Copied! <pre>with open(os.path.join(os.getcwd(), \"data_lab-6\", \"winter_wheat_wa_2023.png\"), \"wb\") as dst:\n    dst.write(r.content)\n</pre> with open(os.path.join(os.getcwd(), \"data_lab-6\", \"winter_wheat_wa_2023.png\"), \"wb\") as dst:     dst.write(r.content) <p>Let's check it saved OK.</p> In\u00a0[\u00a0]: Copied! <pre>img = io.imread(os.path.join(os.getcwd(), \"data_lab-6\", \"winter_wheat_wa_2023.png\"))\npx.imshow(img, height=600)\n</pre> img = io.imread(os.path.join(os.getcwd(), \"data_lab-6\", \"winter_wheat_wa_2023.png\")) px.imshow(img, height=600) <p>If we inspect the url for the potential yield image we downloaded, we can see that it corresponds to selection widgets on GEOGLAMS's crop monitor Agro-meteorological Earth Observation Indicators tool such as year, geographic location, and crop type (<code>ww</code> represents winter wheat).</p> <p>We can create a small routine to loop over years to automate the process of downloading agro-meteorological indicator images for Western Australia. This is a simple example of web scraping - the process of extracting data from web pages. We're not restricted to downloading images, it's also possible to get text data from web pages too. This can be useful if there is information we require for our analysis on web pages that we cannot download in easier to use formats (e.g. CSV files).</p> In\u00a0[\u00a0]: Copied! <pre>years = range(2018, 2023, 1)\n\nfor y in years:\n    r = requests.get(f\"https://cropmonitortools.org/agmet/australia/ww_s1_{y}/condition/adm1/western_australia.jpg\")\n            \n    with open(os.path.join(os.getcwd(), \"data_lab-6\", f\"winter_wheat_wa_{y}.png\"), \"wb\") as dst:\n        dst.write(r.content)\n\n# check to see if the files have downloaded\nprint(os.listdir(os.path.join(os.getcwd(), \"data_lab-6\")))\n</pre> years = range(2018, 2023, 1)  for y in years:     r = requests.get(f\"https://cropmonitortools.org/agmet/australia/ww_s1_{y}/condition/adm1/western_australia.jpg\")                  with open(os.path.join(os.getcwd(), \"data_lab-6\", f\"winter_wheat_wa_{y}.png\"), \"wb\") as dst:         dst.write(r.content)  # check to see if the files have downloaded print(os.listdir(os.path.join(os.getcwd(), \"data_lab-6\"))) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>years = range(2018, 2023, 1)\n\nfor y in years:\n    r = requests.get(f\"https://cropmonitortools.org/agmet/australia/ww_s1_{y}/condition/adm1/queensland.jpg\")\n            \n    with open(os.path.join(os.getcwd(), \"data_lab-6\", f\"winter_wheat_wa_{y}.png\"), \"wb\") as dst:\n        dst.write(r.content)\n</pre> In\u00a0[\u00a0]: Copied! <pre>path_to_file = os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.geojson\")\nfiles_to_send = {\"file_1\": open(path_to_file, \"r\")}\nr = requests.post(\"https://httpbin.org/post\", files=files_to_send)\n</pre> path_to_file = os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.geojson\") files_to_send = {\"file_1\": open(path_to_file, \"r\")} r = requests.post(\"https://httpbin.org/post\", files=files_to_send) In\u00a0[\u00a0]: Copied! <pre># check request was successful\nr.status_code\n</pre> # check request was successful r.status_code In\u00a0[\u00a0]: Copied! <pre># inspect the data that was POSTed - it should print an (ugly) list of coordinates\n# here we only print the first 2000 characters\nr.text[0:2000]\n</pre> # inspect the data that was POSTed - it should print an (ugly) list of coordinates # here we only print the first 2000 characters r.text[0:2000] In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>path_to_file = os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.shp\")\nfiles_to_send = {\"file_1\": open(path_to_file, \"rb\")}\nr = requests.post(\"https://httpbin.org/post\", files=files_to_send)\nr.status_code\n</pre> <p>We can also pass Python data structures into the body of a POST message. For example, the body of a POST message can include data with a key:value pair structure. We can store this data in a Python dictionary and then pass the dictionary into the <code>post()</code> method's <code>data</code> parameter.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict = {\n    \"lon\": -118,\n    \"lat\": -30\n}\nr = requests.post(\"https://httpbin.org/post\", data=demo_dict)\nr.status_code\n</pre> demo_dict = {     \"lon\": -118,     \"lat\": -30 } r = requests.post(\"https://httpbin.org/post\", data=demo_dict) r.status_code In\u00a0[\u00a0]: Copied! <pre># check the data that was POSTed to httpbin\nr.text\n</pre> # check the data that was POSTed to httpbin r.text <p>When we print the response message using <code>r.text</code> we are printing the text encoded version of the response. So, you can see other parts of the HTTP response message including the headers (e.g. we can see the <code>\"Content-Length\"</code> header which tells us the size of the data in the body of the response).</p> <p>There are some other parameters we can set in <code>requests</code> object methods that are useful. We can use the <code>timeout</code> parameter to specify how many seconds we want to wait for the server to respond. You can see a list of these parameters here.</p> In\u00a0[\u00a0]: Copied! <pre>r = requests.get(\"https://httpbin.org/basic-auth/username/password\", auth=(\"username\", \"password\"))\nr.status_code\n</pre> r = requests.get(\"https://httpbin.org/basic-auth/username/password\", auth=(\"username\", \"password\")) r.status_code <p>It is best practice to avoid storing usernames and passwords (i.e. credentials in Python code). You should set your username and password as environment variables on your system and then read them into your program as required.</p> In\u00a0[\u00a0]: Copied! <pre># set environment variables\n# you should do this outside your program's source code (e.g. on the Python console) or in a separate config file that you don't share\nos.environ[\"user\"] = \"username\"\nos.environ[\"pwd\"] = \"password\"\n\n# use environment variables in authenticated request\nr = requests.get(\"https://httpbin.org/basic-auth/username/password\", auth=(os.environ[\"user\"], os.environ[\"pwd\"]))\nr.status_code\n</pre> # set environment variables # you should do this outside your program's source code (e.g. on the Python console) or in a separate config file that you don't share os.environ[\"user\"] = \"username\" os.environ[\"pwd\"] = \"password\"  # use environment variables in authenticated request r = requests.get(\"https://httpbin.org/basic-auth/username/password\", auth=(os.environ[\"user\"], os.environ[\"pwd\"])) r.status_code In\u00a0[\u00a0]: Copied! <pre>%%HTML\n&lt;iframe src=\"https://api.agric.wa.gov.au/v2/weather/openapi/\" width=\"900px\" height=\"800px\"&gt;&lt;/iframe&gt;\n</pre> %%HTML  <p>The DPIRD Weather API 2.0 requires authentication and using an API key to access its services. Therefore, to practice making web API calls via HTTP requests we will work with open-meteo's and SILO's free weather API.</p> <p>open-meteo's historical weather API is documented here. When making an web API call we often need to send values that define how the server should process our request. We saw above how we can use POST requests to send data to a server. Another method of sending data to web APIs is via query parameters.</p> <p>Query parameters are appended to the URL where the request is sent to as key:value pairs.</p> <p>open-meteo's web API is located at: <code>https://archive-api.open-meteo.com/v1/archive</code> - it is also embedded below.</p> <p>Note, open-meteo's free API is capped at 10,000 requests per-day. If you request lots of data or long time-series, you might hit usage limits.</p> In\u00a0[\u00a0]: Copied! <pre>%%HTML\n&lt;iframe src=\"https://open-meteo.com/en/docs/historical-weather-api\" width=\"900px\" height=\"900px\"&gt;&lt;/iframe&gt;\n</pre> %%HTML  <p>To make a GET request to open-meteo's API for: daily maximum temperature (at 2 m), the month of February 2023, and coordinates in Perth we'd use the following URL:</p> <pre><code>https://archive-api.open-meteo.com/v1/archive?latitude=-31.9523&amp;longitude=115.8613&amp;start_date=2023-02-01&amp;end_date=2023-02-28&amp;daily=temperature_2m_max&amp;timezone=auto\n</code></pre> <p>Can you spot after the URL for the API the <code>?</code> and then a series of key:value pairs describing our query with each key:value pair separated by <code>&amp;</code>. This the format for adding query parameters to a URL.</p> <p>The <code>requests</code> object's <code>get()</code> method in Python has a <code>params</code> argument that takes a dictionary of parameters and values and appends them to the URL as query parameters.</p> <p>We can request temperature data for February 2023 over Perth as follows:</p> In\u00a0[\u00a0]: Copied! <pre>payload = {\n    \"latitude\": -31.9523,\n    \"longitude\": 115.8613,\n    \"start_date\": \"2023-02-01\",\n    \"end_date\": \"2023-02-27\",\n    \"daily\": \"temperature_2m_max\",\n    \"timezone\": \"auto\"\n}\nr = requests.get(\"https://archive-api.open-meteo.com/v1/archive\", params=payload)\nr.status_code\n</pre> payload = {     \"latitude\": -31.9523,     \"longitude\": 115.8613,     \"start_date\": \"2023-02-01\",     \"end_date\": \"2023-02-27\",     \"daily\": \"temperature_2m_max\",     \"timezone\": \"auto\" } r = requests.get(\"https://archive-api.open-meteo.com/v1/archive\", params=payload) r.status_code <p>We can print the response object from open-meteo's API and verify that it has returned to us a month's worth of daily maximum temperature values.</p> In\u00a0[\u00a0]: Copied! <pre>r.text\n</pre> r.text <p>The format of the response from open-meteo's API should be familiar, it is JSON formatted data. When responses are returned to us as JSON we can use the <code>json()</code> method of the response object to access the data as a dictionary object. This makes it easier for us to work with data returned from the web API in our programs. For example, we could extract the date and temperature data and make them columns in a pandas <code>DataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre>json_response = r.json()\nprint(f\"the type of json_response is {type(json_response)}\")\n</pre> json_response = r.json() print(f\"the type of json_response is {type(json_response)}\") In\u00a0[\u00a0]: Copied! <pre>json_response\n</pre> json_response In\u00a0[\u00a0]: Copied! <pre>date = json_response[\"daily\"][\"time\"]\nmax_temp = json_response[\"daily\"][\"temperature_2m_max\"]\ndf = pd.DataFrame({\"date\": date, \"max_temp\": max_temp})\ndf.head()\n</pre> date = json_response[\"daily\"][\"time\"] max_temp = json_response[\"daily\"][\"temperature_2m_max\"] df = pd.DataFrame({\"date\": date, \"max_temp\": max_temp}) df.head() In\u00a0[\u00a0]: Copied! <pre>px.line(\n    df,\n    x=\"date\",\n    y=\"max_temp\",\n    labels={\n        \"max_temp\": \"daily maximum T \u00b0C\"\n    }\n)\n</pre> px.line(     df,     x=\"date\",     y=\"max_temp\",     labels={         \"max_temp\": \"daily maximum T \u00b0C\"     } )      In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>payload = {\n    \"latitude\": -31.9523,\n    \"longitude\": 115.8613,\n    \"start_date\": \"2022-01-01\", # note changing date\n    \"end_date\": \"2022-01-31\", # note changing date\n    \"hourly\": \"temperature_2m\", # note changing key and value\n    \"timezone\": \"auto\"\n}\nr = requests.get(\"https://archive-api.open-meteo.com/v1/archive\", params=payload)\nprint(f\"response status code {r.status_code}\")\n\n# extract json response and covert to DataFrame\njson_response = r.json()\nhours = json_response[\"hourly\"][\"time\"]  # note change in keys in response (hourly not daily)\ntemp = json_response[\"hourly\"][\"temperature_2m\"]\ndf_hourly = pd.DataFrame({\"date_time\": hours, \"temp\": temp})\n\n# make hourly temperature chart for Jan 2023\npx.line(\n    df_hourly,\n    x=\"date_time\",\n    y=\"temp\",\n    labels={\n        \"temp\": \"T \u00b0C\",\n        \"date_time\": \"date and time\"\n    }\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>field_poly = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.geojson\"))\nfield_centroid = field_poly.geometry.centroid\nlongitude = field_centroid.x[0]\nlatitude = field_centroid.y[0]\n</pre> field_poly = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.geojson\")) field_centroid = field_poly.geometry.centroid longitude = field_centroid.x[0] latitude = field_centroid.y[0] In\u00a0[\u00a0]: Copied! <pre>m = field_poly.explore()\nfield_centroid.explore(m=m, color=\"red\")\n</pre> m = field_poly.explore() field_centroid.explore(m=m, color=\"red\") <p>Now we have our field's centroid, we're in a position to make API calls to the open-meteo API as HTTP requests.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>payload = {\n    \"latitude\": latitude,\n    \"longitude\": longitude,\n    \"hourly\": \"evapotranspiration,vapor_pressure_deficit\", # note we can add two comma separated values here\n    \"timezone\": \"auto\"\n}\n \nr = requests.get(\"https://api.open-meteo.com/v1/forecast\", params=payload)\n\n# get the JSON response from open-meteo and process\njson_response = r.json()\nhours = json_response[\"hourly\"][\"time\"]  # note change in keys in response (hourly not daily)\nevap = json_response[\"hourly\"][\"evapotranspiration\"]\nvpd = json_response[\"hourly\"][\"vapor_pressure_deficit\"]\ndf_forecast = pd.DataFrame({\"date_time\": hours, \"evap\": evap, \"vpd\": vpd})\n\n# example line chart for evapotranspiration forecast\npx.line(\n    df_forecast,\n    x=\"date_time\",\n    y=\"evap\",\n    labels={\n        \"evap\": \"evapotranspiration (mm)\",\n        \"date_time\": \"date and time\"\n    }\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre>payload = {\n    \"lon\": longitude,\n    \"lat\": latitude,\n    \"start\": \"20230101\",\n    \"finish\": \"20240101\",\n    \"comment\": \"rx\", # climate variables guide: https://www.longpaddock.qld.gov.au/silo/about/climate-variables/\n    \"format\": \"csv\",\n    \"username\": \"geog3300@uwa.class\",\n}\n\ndrill_dataset_api = \"https://www.longpaddock.qld.gov.au/cgi-bin/silo/DataDrillDataset.php?\"\n\nr = requests.get(drill_dataset_api, params=payload)\nr.status_code\n</pre> payload = {     \"lon\": longitude,     \"lat\": latitude,     \"start\": \"20230101\",     \"finish\": \"20240101\",     \"comment\": \"rx\", # climate variables guide: https://www.longpaddock.qld.gov.au/silo/about/climate-variables/     \"format\": \"csv\",     \"username\": \"geog3300@uwa.class\", }  drill_dataset_api = \"https://www.longpaddock.qld.gov.au/cgi-bin/silo/DataDrillDataset.php?\"  r = requests.get(drill_dataset_api, params=payload) r.status_code <p>Here, we've requested the data to be returned to us in CSV format (refer back to week 3 for a refresher on CSV formatted data). Data values in CSV formatted data are separated by commas. The <code>text</code> attribute of the response object stores the CSV data as a string object.</p> In\u00a0[\u00a0]: Copied! <pre>r.text[0:1000]\n</pre> r.text[0:1000] <p>We can read the CSV formatted data into a pandas <code>DataFrame</code> object. However, first we need to pass the string object of CSV formatted data into the <code>StringIO()</code> constructor function. <code>StringIO</code> objects represent string data in-memory in a file-like manner. This means we can pass the <code>StringIO</code> object into pandas <code>read_csv()</code> function and it will read the CSV data as if it was reading it from a CSV file on disk.</p> In\u00a0[\u00a0]: Copied! <pre>silo_df = pd.read_csv(StringIO(r.text))\nsilo_df.head()\n</pre> silo_df = pd.read_csv(StringIO(r.text)) silo_df.head()"},{"location":"notebooks/week-6_1/#web-apis","title":"Web APIs\u00b6","text":""},{"location":"notebooks/week-6_1/#client-server-architectures","title":"Client-server architectures\u00b6","text":"<p>The client-server architecture is a model for describing computer-to-computer communication. Much of the internet, and web-enabled applications, utilise the client-server model for data exchange. For example, a client application such as your web browser can request data from a server (e.g. the data required to render the web page on your display). If the client's request is valid, the server will respond by sending the requested data to the client. This model of clients sending a request and servers responding with data is at the heart of the client-server protocol.</p> <p>This is a schematic from MDN Web Docs that illustrates how a client, here a web browser, requests data from various servers to build a web page on the client's display. When the servers have processed the client's requests, the arrows depicting data flow on the diagram below will reverse and the servers will respond by sending the requested data to the client.</p> <p></p> <p>Attributions and copyright licensing by Mozilla Contributors is licensed under CC-BY-SA 2.5.</p> <p>Perhaps the most common example of client-server computing is web browsers requesting data from servers to render web pages. However, many applications can act as clients. For example, two servers in the cloud could communicate in a client-server manner with an application on one server requesting data from another. Or, various sensors (e.g. weather stations) could act as clients and post data to a server as new measurements are taken. They key point is that client-server computing describes a way that two applications can communicate over a network and share data.</p> <p>If you open up Google Chrome and then go View \u2192 Developer \u2192 Developer Tools and head to https://www.openstreetmap.org/ you can see your browser making requests to OpenStreetMap and rendering the responses (PNG files among other data) on a web map (you can also try this for other websites and see if you can spot data sent by the server to your browser which is then rendered on the display).</p> <p>Make sure you have the Network tab selected and are viewing All messages. You can click on a message to see its structure and the data transferred.</p> <p></p> <p></p>"},{"location":"notebooks/week-6_1/#task","title":"Task\u00b6","text":"<p>The tasks for this lab are i) to generate requests and process responses that scrape data from websites, and ii) to develop a small weather data application for a field in Western Australia based on a client-server model. The client will make requests for weather data to a server which will respond with weather data matching parameters set out in the request. The client will the visualise this weather data.</p>"},{"location":"notebooks/week-6_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-6_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-6_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-6_1/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-6_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-6_1/#http","title":"HTTP\u00b6","text":"<p>Hypertext transfer protocol (HTTP) is the protocol that describes how messages are exchanged between clients and servers.</p> <p>To send messages via HTTP a connection between the client and the server needs to be created. HTTP messages are sent over a transmission control protocol (TCP) connection. TCP connections are used to send HTTP messages as they ensure that all data sent is delivered in the right order (i.e. data transfer between clients and servers is error free).</p> <p>Once the connection is created, the client sends a HTTP message to the server over the connection. The server responds with a HTTP message sent back to the client. Then, the connection is closed or reused for subsequent requests.</p> <p>HTTP messages have a well-defined structure. Request messages start by specifying the HTTP method, then the path to the server or resource they are making a request to, followed by the HTTP version of the message. The path can be a complete URL; for example, to request the data to render the Microsoft Planetary Computer Data Catalog in your browser you need to send a request to <code>https://planetarycomputer.microsoft.com/catalog</code>.</p> <p>After the line denoting the HTTP method, path, and version, HTTP headers are specified. This is extra information that is provided to the server to help handle the request (e.g. you could put credentials for authentication here or descriptions of data included in the body). HTTP headers comprise a name and a value separated by a colon <code>:</code>. The following headers are used to describe data in the body of a request message:</p> <ul> <li><code>Content-Length: &lt;size of data in body of HTTP message - decimal number of bytes&gt;</code></li> <li><code>Content-Type: &lt;type of data in body of HTTP message&gt;</code></li> <li><code>Content-Encoding: &lt;compression algorithm used to compress data in body&gt;</code></li> </ul> <p>These are examples of headers used to pass credentials to the server for authenticated access to server resources (e.g. if you are sending a username and password to the server).</p> <ul> <li><code>Authorization: &lt;authentication scheme&gt; &lt;credentials&gt;</code></li> </ul> <p>The HTTP Basic authentication scheme sends username and password combinations as base64 encoded data (note, this is not encrypted and is just text data encoded in a different format. So, this is not secure unless sent over an encrypted connection - you can see if your connection is encrypted as the url will start with https (and not http)). If our username is <code>testuser</code> and our password is <code>testpassword</code>, we'd include our credentials in a HTTP message header as:</p> <p><code>Authorization: Basic dGVzdHVzZXI6dGVzdHBhc3N3b3Jk</code></p> <p><code>dGVzdHVzZXI6dGVzdHBhc3N3b3Jk</code> is the base64 encoding of <code>testuser:testpassword</code>.</p> <p>Finally, data can be included in the request body. If you are POSTing data to a server, such as an image in PNG format or text as JSON data, this is where that data would go.</p> <p></p> <p>Attributions and copyright licensing by Mozilla Contributors is licensed under CC-BY-SA 2.5.</p> <p>Once the server has received and processed a client's request, it sends a response message. Similar to the request message format, the HTTP response includes the version of the HTTP protocol, a status code indicating if the request was successful or failed, HTTP headers which provide information for the client to process the response, and a body containing data the client requested.</p> <p>It is important to be aware of HTTP response status codes to identify if your request to a server was successful or if there was an error that you'll need to address. A HTTP response status code greater than or equal to 400 indicates an error occurred and the request could not be processed. You can read up on the range of status codes here.</p> <p></p> <p>Attributions and copyright licensing by Mozilla Contributors is licensed under CC-BY-SA 2.5.</p>"},{"location":"notebooks/week-6_1/#http-methods","title":"HTTP methods\u00b6","text":"<p>HTTP request messages start with a HTTP method. These indicate how the server should handle the client's request. Some of the commonly used HTTP methods are listed below:</p> <ul> <li><code>GET</code>: request a specified resource or data.</li> <li><code>POST</code>: post data to the server.</li> <li><code>DELETE</code>: delete the specified resource or data on the server.</li> <li><code>PUT</code>: replace the specified resource or data on the server.</li> </ul> <p>Generally, we don't need to write HTTP messages manually but software applications do this for us. Commonly, this is done by your web browswer or apps on your smartphone. When we're working in Python we can use the requests package to make and handle HTTP requests and responses.</p> <p>The requests package has a <code>requests</code> object with methods for each of the HTTP methods. You can see the docs for the <code>requests</code> object here. The general pattern for using a <code>requests</code> object method is:</p> <p><code>r = requests.&lt;http method&gt;(&lt;url&gt;)</code></p> <p>where <code>r</code> is a response object from the server. For example, we can make a request to get image data that is rendered on GEOGLAMS's crop monitor Agro-meteorological Earth Observation Indicators tool. We pass in the url to where the image is located on a server to the <code>get</code> method. The server returns to us a response object referenced by <code>r</code>.</p> <p>To get the url for an image, you can navigate to their website, right click on the image, and copy image address.</p> <p>Let's start by getting the image for the agro-meteorological indicators for winter wheat in Western Australia in 2023.</p>"},{"location":"notebooks/week-6_1/#get-requests","title":"GET requests\u00b6","text":""},{"location":"notebooks/week-6_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you adapt the above routine to download agro-meteorological indicator images from GEOGLAM's crop monitor for a different state in Australia?</p>"},{"location":"notebooks/week-6_1/#post-requests","title":"POST requests\u00b6","text":"<p>We can use httpbin to practice sending HTTP requests to a server. It has endpoints that allow us to send demo requests using HTTP methods, authentication types, and request different data resources from servers.</p> <p>So far we have been using HTTP GET requests to retrieve data from the server hosting the GEOGLAMS's crop monitor Agro-meteorological Earth Observation Indicators tool. However, there are many instances when we want to send data to the server as part of our request. A common case of sending data to a server is when we fill out and submit a form on a website. However, there are many cases where we need to send data to a server; for example, if the server responds with a satellite image we might need to send a polygon geometry in the request to tell the server which area of the Earth's surface we want a satellite image for.</p> <p>The data in a POST request is included in the request message's body. Let's demo sending some POST requests to httpbin. The url for httpbin's POST endpoint is <code>https://httpbin.org/post</code>.</p> <p>The <code>requests</code> object has a <code>post()</code> method that we can use to create POST requests.</p> <p>To send files in the body of the POST message, we can use the <code>files</code> parameter of the <code>post()</code> method. The <code>files</code> parameter takes a dictionary of files as it's argument. Inside the <code>data_lab-6</code> directory is a <code>BF66_bdy.geojson</code> file - this is the GeoJSON representation for a field boundary in Western Australia. Let's send this file to httpbin in a POST request.</p>"},{"location":"notebooks/week-6_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>GeoJSON data is a text representation of a geometry. A shapefile's .shp file is binary.</p> <p>Can you adapt the above code snippet to send a POST message with <code>BF66_bdy.shp</code> to httpbin's POST endpoint?</p> <p>If you get stuck head back to week 3's content on opening files in binary mode.</p>"},{"location":"notebooks/week-6_1/#authentication","title":"Authentication\u00b6","text":"<p>We can also use <code>requests</code> object method's <code>auth</code> argument to pass in username and passwords for HTTP Basic authentication. httpbin has an endpoint to test making authenticated requests at: <code>\"https://httpbin.org/basic-auth/username/password\"</code>. You can read up on authentication here.</p>"},{"location":"notebooks/week-6_1/#web-apis","title":"Web APIs\u00b6","text":""},{"location":"notebooks/week-6_1/#application-programming-interfaces","title":"Application programming interfaces\u00b6","text":"<p>An API is an application programming interface. An API provides is an interface between two applications that defines how they can communicate (i.e. one application can make requests to another application via its API and receive a response). API's should be well-described, this allows client applications to make requests (or API calls) to use another application's services via its API.</p> <p>As an example, Plotly Express publishes its API reference which describes how you can access services provided by the Plotly package from within your Python programs (i.e. the Plotly Express API). The services provided by Plotly Express and accessed via its API are used to generate visualisations.</p> <p>A web API is a web service provided by a web server and accessible via its API. For example, when we load a website our browser makes an API call to a server's web API to request the data required to render the web page. We can see an example of web APIs on DPIRD's Agriculture and Food website; for example, the Weather API 2.0 defines a set of endpoints that can queried to retrieve information about weather stations and weather data.</p> <p>The DPIRD Weather API 2.0 is published below (you can scroll down to see the various endpoints). The is a description of the interface to DPIRD servers which provide services for disseminating weather data based on client application requests. You can see that the endpoints match HTTP methods - we can make HTTP requests from within our programs to the DPIRD Weather API 2.0 to obtain weather data. You can click on each of the endpoints to see what kind of information needs to be passed into the web API by the client when making requests.</p> <p>The DPIRD Weather API 2.0 is published using the swagger and the OpenAPI Specification - a common format for describing web APIs.</p>"},{"location":"notebooks/week-6_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you use the open-meteo API to get hourly temperature values for the month of January 2022?</p>"},{"location":"notebooks/week-6_1/#building-a-weather-forecast-app","title":"Building a weather forecast app\u00b6","text":"<p>Let's use open-meteo's APIs to build a simple weather data applications for a field in Western Australia. The application will request a 7-day weather forecast for the field.</p> <p>First, let's load the field's extent into our program and extract it's centroid. We'll need the centroid's latitude and longitude values to pass to the open-meteo API as query parameters.</p>"},{"location":"notebooks/week-6_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you use the open-meteo weather forecast API to get a 7-day hourly forecast for evapotranspiration and vapour pressure deficit for a field in Western Australia?</p> <p>Code to read in a GeoJSON file identifying the field's location is provided below.</p> <p>The open-meteo weather forecast API can be found here.</p>"},{"location":"notebooks/week-6_1/#requesting-a-long-term-time-series-of-meteorological-data","title":"Requesting a long-term time-series of meteorological data\u00b6","text":""},{"location":"notebooks/week-6_1/#silo","title":"SILO\u00b6","text":"<p>SILO is a database of daily, pre-processed Australian climate data from 1889 to the present day. The product is hosted by the Queensland Department of Environment and Science (DES) and is based on observational data from the Bureau of Meteorology and other providers. It is made available under the Creative Commons Attribution 4.0 International (CC BY 4.0) licence.</p> <p>SILO's point data API lets you request historical data for a point in Australia and select a time period and a variety of meteorological variables. You can use the patch point tool to create a request URL to pull data.</p> <p>Below is a short example requesting a year's worth of temperature and precipitation data that you can use as a template to build on.</p>"},{"location":"notebooks/week-6_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you download 10 years worth of daily meteological data for the field including the variables class A pan evaporation and relative humidity from SILO?</p> <p>Can you create two line charts visualising class A pan evaporation and relative humidity values over time</p> <p>You are on your own with this one .......</p>"},{"location":"notebooks/week-6_2/","title":"Week 6 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-6\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-6.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-6.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-6\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-6 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-6\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-6.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-6.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-6\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-6 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n    !pip install planetary-computer\n    !pip install pystac-client\n    !pip install odc-stac\n    !pip install adlfs\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio     !pip install planetary-computer     !pip install pystac-client     !pip install odc-stac     !pip install adlfs In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport odc.stac\nimport pystac_client\nimport planetary_computer as pc\nimport plotly.express as px\nimport plotly.io as pio\nfrom skimage import io\n\nfrom pystac.extensions.eo import EOExtension as eo\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import json import geopandas as gpd import pandas as pd import numpy as np import xarray as xr import odc.stac import pystac_client import planetary_computer as pc import plotly.express as px import plotly.io as pio from skimage import io  from pystac.extensions.eo import EOExtension as eo  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># open a connection to the Microsoft Planetary Computer's root STAC catalog\npc_catalog = pystac_client.Client.open(\n    url=\"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    # modifier=planetary_computer.sign_inplace\n)\n</pre> # open a connection to the Microsoft Planetary Computer's root STAC catalog pc_catalog = pystac_client.Client.open(     url=\"https://planetarycomputer.microsoft.com/api/stac/v1\",     # modifier=planetary_computer.sign_inplace ) <p>A <code>pystac_client</code> object has a <code>search()</code> method that can be used to specify a query to search a STAC Collection for STAC Items that meet certain conditions. The <code>search()</code> method has the following parameters that can be used to define scope of the query:</p> <ul> <li><code>max_items</code> - maximum number of items to return from the search.</li> <li><code>bbox</code> - a list or tuple of of bounding box coordinates. STAC Items that intersect the bounding box will be returned.</li> <li><code>intersects</code> - a str or dict representation of a GeoJSON geometry or Shapely <code>geometry</code>. STAC Items that intersect the geometry will be returned.</li> <li><code>datetime</code> - a single datetime or datetime range used to filter STAC Items.</li> <li><code>query</code> - list of JSON or query parameters using the STAC API query extension.</li> </ul> <p>You can see the full details for the <code>search()</code> method here.</p> In\u00a0[\u00a0]: Copied! <pre># load field boundary from shapefile\ndata_path = os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.shp\")\naoi = gpd.read_file(data_path)\n\n# add the field boundary to a map object\nm = aoi.explore()\naoi_env = aoi[\"geometry\"].envelope\n# draw envelope in red\naoi_env.explore(m=m, color=\"red\", style_kwds={\"fillOpacity\": 0})\n</pre> # load field boundary from shapefile data_path = os.path.join(os.getcwd(), \"data_lab-6\", \"BF66_bdy.shp\") aoi = gpd.read_file(data_path)  # add the field boundary to a map object m = aoi.explore() aoi_env = aoi[\"geometry\"].envelope # draw envelope in red aoi_env.explore(m=m, color=\"red\", style_kwds={\"fillOpacity\": 0}) <p>A <code>GeoSeries</code> is a sequence of Shapely <code>geometry</code> objects. Thus, we can just extract the first and only element of the <code>aoi_env</code> <code>GeoSeries</code> to obtain a Shapely <code>geometry</code>.</p> In\u00a0[\u00a0]: Copied! <pre># get Shapely geometry object\naoi_shapely = aoi_env[0]\nprint(aoi_shapely)\n</pre> # get Shapely geometry object aoi_shapely = aoi_env[0] print(aoi_shapely) <p>The process to obtain a GeoJSON str or dict representation of the envelope is more involved. First, we use the <code>GeoPandas</code> <code>to_json()</code> method to convert the <code>GeoSeries</code> to a GeoJSON FeatureCollection in str format.</p> <p>Then, we use the <code>json.loads()</code> to function to parse the JSON string data to a Python dict.</p> <p>Finally, we can subset the <code>geometry</code> property out of the dict.</p> In\u00a0[\u00a0]: Copied! <pre>aoi_json = json.loads(aoi_env.to_json())\nprint(\"AOI Envelope as GeoJSON FeatureCollection\")\nprint(\"\")\nprint(aoi_json)\naoi_geometry = dict(aoi_json[\"features\"][0])[\"geometry\"]\nprint(\"\")\nprint(\"AOI Envelope as GeoJSON Geometry\")\nprint(\"\")\nprint(aoi_geometry)\n</pre> aoi_json = json.loads(aoi_env.to_json()) print(\"AOI Envelope as GeoJSON FeatureCollection\") print(\"\") print(aoi_json) aoi_geometry = dict(aoi_json[\"features\"][0])[\"geometry\"] print(\"\") print(\"AOI Envelope as GeoJSON Geometry\") print(\"\") print(aoi_geometry) <p>Finally, it is simple to obtain a list of coordinates for the bounding box by using the <code>total_bounds</code> property of the <code>GeoSeries</code> and converting it to a list object.</p> <p>See the GeoPandas <code>total_bounds</code> docs.</p> In\u00a0[\u00a0]: Copied! <pre>bbox = aoi_env.total_bounds.tolist()\nbbox\n</pre> bbox = aoi_env.total_bounds.tolist() bbox In\u00a0[\u00a0]: Copied! <pre>time_of_interest = \"2019-10-01/2019-11-01\"\n</pre> time_of_interest = \"2019-10-01/2019-11-01\" In\u00a0[\u00a0]: Copied! <pre># Search the Planetary Computers S2 Catalog\ns2_search = pc_catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=bbox,\n    datetime=time_of_interest,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n)\n\n# Check how many items were returned\ns2_items = s2_search.item_collection()\nprint(f\"Returned {len(s2_items)} Items\")\n</pre> # Search the Planetary Computers S2 Catalog s2_search = pc_catalog.search(     collections=[\"sentinel-2-l2a\"],     bbox=bbox,     datetime=time_of_interest,     query={\"eo:cloud_cover\": {\"lt\": 10}}, )  # Check how many items were returned s2_items = s2_search.item_collection() print(f\"Returned {len(s2_items)} Items\") In\u00a0[\u00a0]: Copied! <pre>s2_items\n</pre> s2_items In\u00a0[\u00a0]: Copied! <pre># empty list\ncloud_cover = []\nfor i in s2_items:\n    cloud_cover.append(eo.ext(i).cloud_cover)\n</pre> # empty list cloud_cover = [] for i in s2_items:     cloud_cover.append(eo.ext(i).cloud_cover) <p>Next, we'll find the minimum cloud cover value and that STAC Item's position in our <code>ItemCollection</code> <code>s2_items</code>.</p> In\u00a0[\u00a0]: Copied! <pre>min_cloud_cover = min(cloud_cover)\nmin_cloud_cover_idx = cloud_cover.index(min_cloud_cover)\nprint(f\"The STAC Item with lowest cloud cover had {min_cloud_cover}% cloud cover\")\nprint(f\"The index postion of the STAC Item with lowest cloud cover in our ItemCollection is {min_cloud_cover_idx}\")\n</pre> min_cloud_cover = min(cloud_cover) min_cloud_cover_idx = cloud_cover.index(min_cloud_cover) print(f\"The STAC Item with lowest cloud cover had {min_cloud_cover}% cloud cover\") print(f\"The index postion of the STAC Item with lowest cloud cover in our ItemCollection is {min_cloud_cover_idx}\") <p>Let's subset the the STAC Item with the lowest cloud cover from our <code>ItemCollection</code>. This should give us a single STAC Item which we can inspect.</p> In\u00a0[\u00a0]: Copied! <pre>least_cloudy_s2 = s2_items[min_cloud_cover_idx]\nleast_cloudy_s2\n</pre> least_cloudy_s2 = s2_items[min_cloud_cover_idx] least_cloudy_s2 <p>Now we've identified the STAC Item with the lowest cloud cover, we need to download it. This is where we head to the Assets property of the STAC Item where we see a series of <code>href</code> properties with hyperlinks to where that data is physically stored (here, this is in Azure Blob Storage as cloud-optimised GeoTIFF files).</p> <p>We can print out the list of Assets associated with the STAC Item.</p> In\u00a0[\u00a0]: Copied! <pre># print assets properties of STAC Item\nleast_cloudy_s2.assets.keys()\n</pre> # print assets properties of STAC Item least_cloudy_s2.assets.keys() In\u00a0[\u00a0]: Copied! <pre># lets look at the property for B02 - blue band reflectance\nleast_cloudy_s2.assets[\"B02\"]\n</pre> # lets look at the property for B02 - blue band reflectance least_cloudy_s2.assets[\"B02\"] <p>The <code>href</code> points to a cloud-optmised GeoTIFF (COG) file stored in Azure Blob Storage (i.e. in the cloud). A COG file is similar to a regular GeoTIFF file, but it can receive HTTP requests to retrieve portions of data that correspond to a geographic extent and at a particular zoom level.</p> <p>Planet (a commercial CubeSat company that make use of STAC and GeoTIFFs in their products) have a blog post that introduce COGs.</p> In\u00a0[\u00a0]: Copied! <pre>s2_xr = odc.stac.load(\n    [least_cloudy_s2], patch_url=pc.sign, bbox=bbox\n)\ns2_xr\n</pre> s2_xr = odc.stac.load(     [least_cloudy_s2], patch_url=pc.sign, bbox=bbox ) s2_xr <p>The Sentinel-2 data that we have downloaded is stored as a collection of <code>xarray.DataArray</code> objects within a larger container called <code>xarray.Dataset</code>. An <code>xarray.Dataset</code> can store many <code>xarray.DataArray</code> objects that share <code>dims</code> and <code>coordinates</code>. Here, we have different arrays of different <code>Variables</code> that correspond to the same locations and time-periods but different spectral bands.</p> <p>If we want to stack the arrays corresponding to spectral bands into a 3D-array stored within a <code>xarray.DataArray</code> object, we can use the <code>to_array()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>s2_xr =s2_xr.to_array()\ns2_xr\n</pre> s2_xr =s2_xr.to_array() s2_xr In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_xr.sel(variable=\"B03\").plot(robust=True)\n</pre> <p></p> <p>Or, if we're being precise we can also select the array to visualise by variable and time labels. This is also necessary if we're visualising the data using <code>imshow()</code> which requires either a 2D array or 3D array with three bands (to visualise as an RGB image).</p> <pre>s2_xr.sel(variable=\"B03\", time=\"2019-10-21T02:04:51.024000000\").plot.imshow(robust=True)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE  answer <pre>s2_r_nir_xr = odc.stac.load(\n    [least_cloudy_s2], bands=[\"B04\", \"B08\"], patch_url=pc.sign, bbox=bbox\n).to_array()\ns2_r_nir_xr\n</pre> In\u00a0[\u00a0]: Copied! <pre># Search the Planetary Computers Landsat Catalog\nlandsat_search = pc_catalog.search(\n    collections=[\"landsat-c2-l2\"],\n    bbox=bbox,\n    datetime=time_of_interest,\n    query={\n        \"eo:cloud_cover\": {\"lt\": 10},\n        \"platform\": {\"in\": [\"landsat-8\", \"landsat-9\"]}\n    },\n)\n\n# Check how many items were returned\nlandsat_items = landsat_search.item_collection()\nprint(f\"Returned {len(landsat_items)} Items\")\n</pre> # Search the Planetary Computers Landsat Catalog landsat_search = pc_catalog.search(     collections=[\"landsat-c2-l2\"],     bbox=bbox,     datetime=time_of_interest,     query={         \"eo:cloud_cover\": {\"lt\": 10},         \"platform\": {\"in\": [\"landsat-8\", \"landsat-9\"]}     }, )  # Check how many items were returned landsat_items = landsat_search.item_collection() print(f\"Returned {len(landsat_items)} Items\") <p>Similar to the example working with Sentinel-2 data above, we can query each <code>item</code>'s cloud cover property to find the least cloudy image.</p> In\u00a0[\u00a0]: Copied! <pre># empty list\ncloud_cover = []\nfor i in landsat_items:\n    cloud_cover.append(eo.ext(i).cloud_cover)\n\nmin_cloud_cover = min(cloud_cover)\nmin_cloud_cover_idx = cloud_cover.index(min_cloud_cover)\nprint(f\"The STAC Item with lowest cloud cover had {min_cloud_cover}% cloud cover\")\nprint(f\"The index postion of the STAC Item with lowest cloud cover in our ItemCollection is {min_cloud_cover_idx}\")\n</pre> # empty list cloud_cover = [] for i in landsat_items:     cloud_cover.append(eo.ext(i).cloud_cover)  min_cloud_cover = min(cloud_cover) min_cloud_cover_idx = cloud_cover.index(min_cloud_cover) print(f\"The STAC Item with lowest cloud cover had {min_cloud_cover}% cloud cover\") print(f\"The index postion of the STAC Item with lowest cloud cover in our ItemCollection is {min_cloud_cover_idx}\") <p>Next we can select the least cloud <code>item</code> and disply it's attributes.</p> In\u00a0[\u00a0]: Copied! <pre>least_cloudy_item = landsat_items[min_cloud_cover_idx]\nleast_cloudy_item\n</pre> least_cloudy_item = landsat_items[min_cloud_cover_idx] least_cloudy_item <p>We can print out the band names and their description too. This is useful if we want to subset particular bands from the image for further analysis or visualisation.</p> In\u00a0[\u00a0]: Copied! <pre>max_key_length = len(max(least_cloudy_item.assets, key=len))\nfor key, asset in least_cloudy_item.assets.items():\n    print(f\"{key.rjust(max_key_length)}: {asset.title}\")\n</pre> max_key_length = len(max(least_cloudy_item.assets, key=len)) for key, asset in least_cloudy_item.assets.items():     print(f\"{key.rjust(max_key_length)}: {asset.title}\") <p>Finally, let's read the red, green, blue, and near infrared bands for the least cloudy Landsat image covering the field we're working with.</p> In\u00a0[\u00a0]: Copied! <pre>landsat_xr = odc.stac.load(\n    [least_cloudy_item], bands=[\"blue\", \"green\", \"red\", \"nir08\"], patch_url=pc.sign, bbox=bbox\n).isel(time=0).to_array()\nlandsat_xr\n</pre> landsat_xr = odc.stac.load(     [least_cloudy_item], bands=[\"blue\", \"green\", \"red\", \"nir08\"], patch_url=pc.sign, bbox=bbox ).isel(time=0).to_array() landsat_xr In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>landsat_xr.sel(variable=[\"red\", \"green\", \"blue\"]).plot.imshow(robust=True)\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>red = landsat_xr.sel(variable=\"red\").astype(\"float\")\nnir = landsat_xr.sel(variable=\"nir08\").astype(\"float\")\nndvi = (nir - red) / (nir + red)\nndvi.plot.imshow(robust=True)\n</pre> In\u00a0[\u00a0]: Copied! <pre>season_of_interest = \"2019-05-01/2019-10-31\"\n</pre> season_of_interest = \"2019-05-01/2019-10-31\" <p>We can pass this expanded time period in the the <code>search()</code> method of the <code>pc_catalog()</code> object to search the Planetary Computers <code>landsat-c2-l2</code> collection for all Landsat 8 scenes that intersect out bounding box between May and October 2019.</p> In\u00a0[\u00a0]: Copied! <pre># Search the Planetary Computers Landsat Catalog\nlandsat_search = pc_catalog.search(\n    collections=[\"landsat-c2-l2\"],\n    bbox=bbox,\n    datetime=season_of_interest,\n    query={\n        \"eo:cloud_cover\": {\"lt\": 10},\n        \"platform\": {\"in\": [\"landsat-8\"]}\n    },\n)\n\n# Check how many items were returned\nlandsat_items = landsat_search.item_collection()\nprint(len(landsat_items))\n</pre> # Search the Planetary Computers Landsat Catalog landsat_search = pc_catalog.search(     collections=[\"landsat-c2-l2\"],     bbox=bbox,     datetime=season_of_interest,     query={         \"eo:cloud_cover\": {\"lt\": 10},         \"platform\": {\"in\": [\"landsat-8\"]}     }, )  # Check how many items were returned landsat_items = landsat_search.item_collection() print(len(landsat_items)) <p>The first argument to <code>odc.stac.load()</code> is an iterable object (e.g. a list) of STAC <code>items</code>. We can pass in the <code>landsat_items</code> <code>ItemCollection</code> object which stores a series of STAC <code>items</code>.</p> In\u00a0[\u00a0]: Copied! <pre>landsat_xr = odc.stac.load(\n    landsat_items, bands=[\"blue\", \"green\", \"red\", \"nir08\"], patch_url=pc.sign, bbox=bbox\n)\n</pre> landsat_xr = odc.stac.load(     landsat_items, bands=[\"blue\", \"green\", \"red\", \"nir08\"], patch_url=pc.sign, bbox=bbox ) <p>This has returned to us an <code>xarray.Dataset</code> with four variables (one for each of the spectral bands we requested) and each variable is a 3D array with x, y, and time dimensions.</p> In\u00a0[\u00a0]: Copied! <pre>landsat_xr\n</pre> landsat_xr <p>Let's convert the <code>xarray.Dataset</code> object to an <code>xarray.DataArray</code> object which stores a 4D array with x, y, variable, and time dimensions.</p> In\u00a0[\u00a0]: Copied! <pre>landsat_arr = landsat_xr.to_array()\nlandsat_arr\n</pre> landsat_arr = landsat_xr.to_array() landsat_arr <p>If your refer back to week 2, we used facet plots where we plot data on many subplots that share axes. Plotting a time series of satellite images is a good use for a faceted plot. We can create a different subplot for each time point and keep the x and y axes representing geographic location the same. An <code>xarray.DataArray</code>'s <code>plot.imshow()</code> method has a <code>col</code> argument that we can pass a dimension into for creating faceted plots; here, we'll pass in the <code>\"time\"</code> dimension.</p> In\u00a0[\u00a0]: Copied! <pre>landsat_arr.sel(variable=[\"red\", \"green\", \"blue\"]).plot.imshow(col=\"time\", col_wrap=2)\n</pre> landsat_arr.sel(variable=[\"red\", \"green\", \"blue\"]).plot.imshow(col=\"time\", col_wrap=2) In\u00a0[\u00a0]: Copied! <pre>search = pc_catalog.search(\n    collections=[\"era5-pds\"], datetime=\"2019-05\", query={\"era5:kind\": {\"eq\": \"an\"}}\n)\nitems = search.item_collection()\n\nprint(len(items))\nitem = items[0]\nitem\n</pre> search = pc_catalog.search(     collections=[\"era5-pds\"], datetime=\"2019-05\", query={\"era5:kind\": {\"eq\": \"an\"}} ) items = search.item_collection()  print(len(items)) item = items[0] item <p>To download data from the Planetary Computer we need to sign it, as discussed above. Let's also subset out the air temperature asset.</p> In\u00a0[\u00a0]: Copied! <pre>signed_item = pc.sign(item)\nair_temp = signed_item.assets[\"air_temperature_at_2_metres\"]\n</pre> signed_item = pc.sign(item) air_temp = signed_item.assets[\"air_temperature_at_2_metres\"] <p>We can directly read the air temperature data into an <code>xarray.DataArray</code> object using the <code>xr.open_dataset()</code> function as the data is stored in zarr format (refer back to week 3 for a refresher on zarr data).</p> In\u00a0[\u00a0]: Copied! <pre>ds = xr.open_dataset(air_temp.href, **air_temp.extra_fields[\"xarray:open_kwargs\"])\n</pre> ds = xr.open_dataset(air_temp.href, **air_temp.extra_fields[\"xarray:open_kwargs\"]) In\u00a0[\u00a0]: Copied! <pre>ds\n</pre> ds <p>Let's get the coordinates for the centre of the field, and extract the air temperature data for that location and plot it.</p> In\u00a0[\u00a0]: Copied! <pre># get the coordinates for the field's centroid\nx, y = aoi_env.centroid[0].coords.xy\nprint(x, y)\n</pre> # get the coordinates for the field's centroid x, y = aoi_env.centroid[0].coords.xy print(x, y) In\u00a0[\u00a0]: Copied! <pre>ds[\"air_temperature_at_2_metres\"].sel(lon=x[0], lat=y[0], method=\"nearest\").plot()\n</pre> ds[\"air_temperature_at_2_metres\"].sel(lon=x[0], lat=y[0], method=\"nearest\").plot()"},{"location":"notebooks/week-6_2/#spatiotemporal-asset-catalogs-stac","title":"SpatioTemporal Asset Catalogs (STAC)\u00b6","text":"<p>This lab will demonstrate how to search for and download geospatial data in the cloud.</p> <p>It will introduce SpatioTemporal Asset Catalogs (STAC), a specification that makes it easy to query and search through large collections of geospatial data assets stored in the cloud.</p> <p>You will also learn to use the pystac_client package which provides tools for working with STAC in Python.</p> <p>We're going to use the pystac_client package to query a range of STAC Catalogs hosted in the cloud. We'll complete the following tasks:</p> <ul> <li>Find the least cloudy Sentinel-2 image for a field in Western Australia using the Microsoft Planetary Computer.</li> <li>Find the least cloud Landsat image for a field in Western Australia using the Microsoft Planetary Computer.</li> <li>Find and download a time-series of cloud free satellite images during a growing season.</li> <li>Find and download meteorological data from cloud-based data catalogs.</li> </ul>"},{"location":"notebooks/week-6_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-6_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-6_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-6_2/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-6_2/#spatiotemporal-asset-catalogs-stac","title":"SpatioTemporal Asset Catalogs (STAC)\u00b6","text":"<p>First, let's briefly outline what the STAC specification is before completing some data querying and downloading tasks to make the concepts concrete.</p> <p>spatiotemporal asset: this is a file comprising geospatial data for a location and point in time. For example, this could be Landsat or Sentinel-2 satellite images stored in the cloud such as in Microsoft Azure or Amazon Web Services. This is a file that we can download and use the data in our analysis and applications. However, if you look at Microsoft's Planetary Computer Data Catalog, Amazon Web Services Open Data, or the Digital Earth Australia Open Data Cube you will see there are lots of spatiotemporal assets available (for free). The challenge is searching through these collections of assets to find the data you need and downloading it. The STAC specification provides a solution for this.</p> <p>The STAC specification comprises:</p> <ul> <li>STAC Item - a GeoJSON feature that represents a spatiotemporal asset with links to the spatiotemporal asset and additional metadata fields (e.g. bounding box, thumbnail, datetime, cloud cover).</li> <li>STAC Catalog - a JSON file of links to STAC Items to support querying and retrieving STAC Items. STAC Catalogs can comprise sub-catalogs that group together related data within a larger structure. For example, Microsoft's Planetary Computer might create a STAC Catalog for all of its spatiotemporal assets and organise these assets in sub-catalogs (e.g. a catalog for Landsat 7, Landsat 8, Sentinel-2, SRTM DEM etc.).</li> <li>STAC Collection - an extension of a STAC Catalog with additional metadata properties (e.g. extents, licences, providers) to describe STAC Items within the collection.</li> <li>STAC API - an API that allows clients to query a STAC collection, search for STAC Items, and retrieve their links for downloading. The search endpoint is designed to receive queries of STAC Catalogs that filter on location, date, and time as well as other fields. It returns a GeoJSON FeatureCollection object with of STAC Items that meet the search criteria.</li> </ul>"},{"location":"notebooks/week-6_2/#tips","title":"Tips\u00b6","text":"<p>These are some tips for working with STAC here.</p> <ul> <li>use rectangular bounding boxes or area-of-interest geometries to quickly identify STAC Items that intersect with their extent.</li> <li>for exploratory work use small areas-of-interest to minimise the size of searches of STAC Collections and the amount of data transmitted over the network.</li> </ul>"},{"location":"notebooks/week-6_2/#useful-links","title":"Useful links\u00b6","text":"<ul> <li>STAC website: the STAC homepage with details about STAC, tutorials, and links to STAC catalogs.</li> <li>STAC Browser: a web browser to search for STAC catalogs.</li> <li>STAC Index: an index of STAC catalogs and tutorials.</li> <li>Microsoft Planetary Computer Catalog: Microsoft Planetary Computer's STAC catalogs.</li> </ul>"},{"location":"notebooks/week-6_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-6_2/#sentinel-2-and-microsoft-planetary-computer","title":"Sentinel-2 and Microsoft Planetary Computer\u00b6","text":"<p>To provide an introducion to the STAC specification and using it to search for spatiotemporal assets, we'll use it to query Microsoft's Planetary Computer to find a cloud free Sentinel-2 satellite image for a field in Western Australia.</p> <p>We'll be using the pystac_client package which is a STAC Python Client providing classes for working with STAC Catalogs and APIs.</p> <p>First, we need to create a <code>pystac_client.Client</code> object which contains the methods and attributes to interact with a given STAC Catalog. Using the <code>pystac_client.Client.open()</code> method we can open a STAC Catalog or API and read the root catalog.</p> <p>The <code>pystac_client.Client.open()</code> method requires a <code>url</code> which points to the STAC catalog or api. The <code>url</code> for the Microsoft Planetary Computer STAC API is <code>\"https://planetarycomputer.microsoft.com/api/stac/v1\"</code>.</p>"},{"location":"notebooks/week-6_2/#area-of-interest","title":"Area of interest\u00b6","text":"<p>Before we can <code>search()</code> the Planetary Computer STAC Catalog we need to create the geographic extent for our query.</p> <p>We're going to start by reading in a geometry for the field boundary stored in a shapefile. We need to convert the shapefile to one of:</p> <ul> <li>bounding box coordinates</li> <li>a GeoJSON geometry</li> <li>a Shapely <code>geometry</code></li> </ul> <p>We'll demonstrate how to do each of these conversions for your reference.</p> <p>First let's read the data from file. Then, we'll compute the <code>envelope</code> of the field's geometry. The envelope is the smallest rectangular geometry to cover the field's geometry. It is often beneficial to pass in simpler geometries than more complex shapes for identifying STAC Items that intersect with an area-of-interest.</p>"},{"location":"notebooks/week-6_2/#datetime","title":"Datetime\u00b6","text":"<p>Let's specify a datetime range to search. Here, we'll look for all Sentinel-2 STAC Items that intersect our area-of-interest for the month of October 2019.</p>"},{"location":"notebooks/week-6_2/#extensions","title":"Extensions\u00b6","text":"<p>The STAC specification permits extensions which allow for more detailed descriptions of STAC Items in a collection. A commonly used extension is the <code>Electro-Optical Extension Specification</code> for describing snapshots of the Earth for a point-in-time and designed for data that's captured for one or more wavelengths of the electromagnetic spectrum (i.e. remote sensing data).</p> <p>It includes the following item properties:</p> <ul> <li><code>eo:bands</code>: an array of available bands (i.e. different spectral wavebands for a remote sensing image).</li> <li><code>eo:cloud_cover</code>: an estimate of cloud cover for the STAC Item.</li> <li><code>eo:snow_cover</code>: an estimate of snow and ice cover for the STAC Item.</li> </ul> <p>The <code>eo:cloud_cover</code> property could be useful to help with searching a STAC Collection for cloud free scenes.</p> <p>We can set up a query of <code>eo</code> properties as: <code>{\"eo:cloud_cover\": {\"lt\": 10}}</code>. This will find all STAC Items with a property of <code>eo:cloud_cover</code> less than 10%.</p>"},{"location":"notebooks/week-6_2/#search","title":"Search\u00b6","text":"<p>We're now ready to search the Planetary Computer STAC Catalog's <code>sentinel-2-l2a</code> for all images with low cloud cover in October 2019 that intersect our area-of-interest.</p> <p>The <code>s2_search</code> object is an <code>ItemSearch</code> instance which represents the search of a STAC API. We can retrieve the STAC Items returned by the search as an <code>ItemCollection</code> using the <code>item_collection()</code> method.</p> <p>We can print the <code>ItemCollection</code> and interactively explore its contents. This helpfully illustrates the structure of the STAC specification. Our search of the <code>sentinel-2-l2a</code> collection returned 2 STAC Items. Each STAC Item corresponds to a Sentinel-2 image.</p> <p>We can explore each of the STAC Items and see that it has several metadata properties (e.g. Bounding Box, Datetime, platform, proj:epsg, eo:cloud_cover), it also has an Assets slot which stores links to the underlying data referenced by the STAC Item. In this case it is a cloud-optimised GeoTIFF files stored in Microsoft Azure.</p>"},{"location":"notebooks/week-6_2/#download-data","title":"Download data\u00b6","text":"<p>Now we've completed a search of the STAC API and identified that there are two Sentinel-2 images that meet our search criteria, we're in a position to download these images and use their data.</p> <p>As these are optical images of the Earth's surface, we'd like to use the least cloudy image.  We can write a small routine to find the STAC Item with the lowest <code>eo:cloud_cover</code> value and download that item.</p> <p>We imported the <code>EOExtension</code> module as <code>eo</code> at the start of the notebook. We can call the <code>eo.ext()</code> method on a STAC Item to extend it with properties from the <code>eo</code> extension. This allows us to get the <code>eo</code> item properties such as <code>cloud_cover</code>.</p> <p>Let's loop over all the STAC Items in our search, retrieve their <code>eo:cloud_cover</code> value, and append that value to a list.</p>"},{"location":"notebooks/week-6_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Why do these features of a cloud-optimised GeoTIFF make them more suited to working with big geospatial datasets than regular GeoTIFF files? As geospatial datasets increase in size (e.g. satellites capturing data with ever finer spatial resolutions and with a higher cadence) the amount of data we'd need to store and read into memory increases. This might exceed our computer's capacity or result in long runtimes for our program. COGs allow us to just read the data that corresponds to our area-of-interest and not the entire file. This means we can make use of the larger storage capacity of cloud providers and just retrieve the data we need.   <p>To download the data for the least cloudy Sentinel-2 data image we can use the <code>load()</code> function from the odc-stac package.</p> <p>The <code>load()</code> function has the following parameters:</p> <ul> <li><code>items</code> - an iterable of STAC items to download.</li> <li><code>bands</code> - a list of bands to download. Defaults to all if an argument is not passed into the function.</li> <li><code>bbox</code> - a bounding box of latitude and longitude values to download data for in the format: <code>[min(lon), min(lat), max(lon), max(lat)]</code>.</li> <li><code>geopolygon</code> - a geometry to download data for which can be a GeoJSON dict, geopandas GeoDataFrame, or shapely object.</li> <li><code>path_url</code> - transforms the URL describing the assets location. This is useful for working with the Planetary Computer to sign the link.</li> </ul>"},{"location":"notebooks/week-6_2/#signing-links","title":"Signing links\u00b6","text":"<p>To download data from the Planetary Computer the link needs to be \"signed\". This allows Microsoft to manage traffic and use of the Planetary Computer's resources in the cloud.</p> <p>The <code>planetary_computer</code> package was imported as <code>pc</code> and has a <code>sign()</code> function we can use to sign links.</p> <p>For a full list of parameters that you can use to control how <code>load()</code> downloads data from STAC inspect the docs.</p> <p>The data is downloaded into an <code>xarray.Dataset</code> object.</p>"},{"location":"notebooks/week-6_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you visualise the array visible green reflectance values from <code>xarray.DataArray</code> <code>s2_xr</code>?</p> <p>Green reflectance is referenced by the Variable label <code>B03</code>.</p>"},{"location":"notebooks/week-6_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Often we don't need all the spectral bands in a satellite image and it's good to be efficient with the amount of data we download. You can call the <code>keys()</code> method on a STAC <code>item</code> to get a list of bands (e.g. <code>least_cloudy_s2.assets.keys()</code>). Can you use this information and the <code>bands</code> parameter of <code>odc.stac.load()</code> to download only the red and near infrared Sentinel-2 bands?</p> <p>You can find a table listing the Sentinel-2 bands here.</p>"},{"location":"notebooks/week-6_2/#landsat-and-microsoft-planetary-computer","title":"Landsat and Microsoft Planetary Computer\u00b6","text":"<p>Landsat is a series of satellite missions run by the US Geological Survey (USGS) and NASA. It has been operational since 1972 and the current Landsat mission is Landsat 9. Landsat satellites provide measures of spectral reflectance in the visible, near infrared, shortwave infrared, and thermal portions of the electromagnetic spectrum. The visible and near infrared bands have a spatial resolution of 30 m.</p> <p>The Landsat level 2 STAC Collection in the Planetary Computer is labelled <code>\"landsat-c2-l2\"</code>. We can search this STAC Collection for Landsat scenes the intersect the bounding box for our field and for a specified time frame:</p>"},{"location":"notebooks/week-6_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you visualise the least cloudy Landsat image as true colour (RGB) composite image?</p> <p>You can refer to the Planetary Computer examples to help with this task.</p>"},{"location":"notebooks/week-6_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you compute the NDVI for the least cloud Landsat image and visualise it?</p> <p>You can refer to the Planetary Computer examples to help with this task.</p>"},{"location":"notebooks/week-6_2/#visualising-a-time-series-of-satellite-images","title":"Visualising a time-series of satellite images\u00b6","text":"<p>We can use the <code>odc.stac.load()</code> function to download many satellite images of the same location captured on different dates. Let's visualise all the relatively cloud free Landsat 8 images captured during the 2019 growing season for the field in Western Australia we're working with. First, we need to expand the time period we're searching for images in.</p>"},{"location":"notebooks/week-6_2/#downloading-meteorological-data-from-stac-collections","title":"Downloading meteorological data from STAC Collections\u00b6","text":"<p>Alongside remote sensing images, meteorological data is often organised within STAC Collections. There are lots of examples of how to access meteorological data from the Planetary Computer's Data Catalog. You are encouraged to try out a few of them.</p> <p>Here is a short example of how we can retrieve air temperature data covering the field in Western Australia from the ERA5 climate reanalysis product.</p> <p>First, let's search the <code>era5-pds</code> catalog for all data in May 2019 and extract the first item in the collection returned to us. We can see that the assets key points to the location of data representing many climate variables (<code>href</code> - a URL to where the data file is stored in the cloud).</p>"},{"location":"notebooks/week-6_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>What does setting <code>method=\"nearest\"</code> enable in the <code>sel()</code> method?</p> answer <p>It let's us select the <code>xarray.DataArray</code> values closed to the nearest lat and lon values we also use for selection.</p>"},{"location":"notebooks/week-7_1/","title":"Week 7 1","text":"In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install geopandas\n    !pip install pyarrow\n    !pip install mapclassify\n    !pip install rasterio\n    !pip install planetary-computer\n    !pip install pystac-client\n    !pip install ipyleaflet\n</pre> if 'google.colab' in str(get_ipython()):     !pip install geopandas     !pip install pyarrow     !pip install mapclassify     !pip install rasterio     !pip install planetary-computer     !pip install pystac-client     !pip install ipyleaflet In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nimport geopandas as gpd\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport numpy as np\nimport requests\nfrom skimage import io\nimport pystac_client\nimport planetary_computer as pc\n\nfrom pystac.extensions.eo import EOExtension as eo\n\nfrom ipyleaflet import Map, TileLayer, WMSLayer, basemaps, LegendControl, SplitMapControl\nfrom traitlets import Unicode\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import json import geopandas as gpd import pandas as pd import plotly.express as px import plotly.io as pio import numpy as np import requests from skimage import io import pystac_client import planetary_computer as pc  from pystac.extensions.eo import EOExtension as eo  from ipyleaflet import Map, TileLayer, WMSLayer, basemaps, LegendControl, SplitMapControl from traitlets import Unicode  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>!pip install OWSLib\n</pre> !pip install OWSLib In\u00a0[\u00a0]: Copied! <pre>from owslib.wms import WebMapService\n</pre> from owslib.wms import WebMapService In\u00a0[\u00a0]: Copied! <pre>payload = {\n    \"transparent\": \"true\",\n    \"format\": \"image/png\",\n    \"exceptions\": \"XML\",\n    \"styles\": \"level4\",\n    \"version\": \"1.3.0\",\n    \"time\": \"2020-01-01\",\n    \"service\": \"WMS\",\n    \"request\": \"GetMap\",\n    \"layers\": \"ga_ls_landcover\",\n    \"bbox\": \"12885448.480201874,-3786384.6331344917,12895232.419822376,-3776600.6935139894\",\n    \"width\": 256,\n    \"height\": 256,\n    \"crs\": \"EPSG:3857\"\n}\n\nr = requests.get(\"https://ows.dea.ga.gov.au\", params=payload)\nr.status_code\n</pre> payload = {     \"transparent\": \"true\",     \"format\": \"image/png\",     \"exceptions\": \"XML\",     \"styles\": \"level4\",     \"version\": \"1.3.0\",     \"time\": \"2020-01-01\",     \"service\": \"WMS\",     \"request\": \"GetMap\",     \"layers\": \"ga_ls_landcover\",     \"bbox\": \"12885448.480201874,-3786384.6331344917,12895232.419822376,-3776600.6935139894\",     \"width\": 256,     \"height\": 256,     \"crs\": \"EPSG:3857\" }  r = requests.get(\"https://ows.dea.ga.gov.au\", params=payload) r.status_code In\u00a0[\u00a0]: Copied! <pre># write the returned image data to a PNG file\nos.makedirs(os.path.join(os.getcwd(), \"week-7\"), exist_ok=True)\nwith open(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms.png\"), \"wb\") as dst:\n    dst.write(r.content)\n\n# visualise returned image\nimg = io.imread(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms.png\"))\npx.imshow(img, height=600)\n</pre> # write the returned image data to a PNG file os.makedirs(os.path.join(os.getcwd(), \"week-7\"), exist_ok=True) with open(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms.png\"), \"wb\") as dst:     dst.write(r.content)  # visualise returned image img = io.imread(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms.png\")) px.imshow(img, height=600) <p>To verify that we setup the <code>payload</code> object defining the query parameters to Digital Earth Australia's WMS server correctly, we can skip using the <code>params</code> argument in the <code>get()</code> method and pass in the full URL string.</p> In\u00a0[\u00a0]: Copied! <pre>r_fullstring = requests.get(\"https://ows.dea.ga.gov.au/?transparent=true&amp;format=image/png&amp;exceptions=XML&amp;styles=level4&amp;version=1.3.0&amp;time=2020-01-01&amp;service=WMS&amp;request=GetMap&amp;layers=ga_ls_landcover&amp;bbox=12885448.480201874,-3786384.6331344917,12895232.419822376,-3776600.6935139894&amp;width=256&amp;height=256&amp;crs=EPSG:3857\")\nprint(r_fullstring.status_code)\n\nwith open(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms-fullstring.png\"), \"wb\") as dst:\n    dst.write(r.content)\n\n# visualise returned image\nimg = io.imread(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms-fullstring.png\"))\npx.imshow(img, height=600)\n</pre> r_fullstring = requests.get(\"https://ows.dea.ga.gov.au/?transparent=true&amp;format=image/png&amp;exceptions=XML&amp;styles=level4&amp;version=1.3.0&amp;time=2020-01-01&amp;service=WMS&amp;request=GetMap&amp;layers=ga_ls_landcover&amp;bbox=12885448.480201874,-3786384.6331344917,12895232.419822376,-3776600.6935139894&amp;width=256&amp;height=256&amp;crs=EPSG:3857\") print(r_fullstring.status_code)  with open(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms-fullstring.png\"), \"wb\") as dst:     dst.write(r.content)  # visualise returned image img = io.imread(os.path.join(os.getcwd(), \"week-7\", \"demo-dea-wms-fullstring.png\")) px.imshow(img, height=600) <p>The request we just made to the Digital Earth Australia WMS server to retrieve an image illustrates the process that a client web map application would go through to retrieve the data to fill the web map component of your display. As you move around the web map with your mouse (or touch and drag on a touchscreen) the client web map application generates these requests, sends them to the WMS server, and renders the returned image data.</p> <p>The below video demonstrates using the Digital Earth Australia Map (https://maps.dea.ga.gov.au/) to render the DEA Land Cover Calendar Year (Landsat) data using WMS. You can see the client (here the Digital Earth Australia Map application in your web browser) making several of HTTP requests to the Digital Earth Australia WMS server to generate PNG images to create the map visualisation on the display.</p> In\u00a0[\u00a0]: Copied! <pre>%%HTML\n&lt;div style=\"padding:50% 0 0 0;position:relative;\"&gt;&lt;iframe src=\"https://player.vimeo.com/video/816397558?h=84992531b5&amp;amp;badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"dea-wms-landsat.mov\"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=\"https://player.vimeo.com/api/player.js\"&gt;&lt;/script&gt;\n</pre> %%HTML  In\u00a0[\u00a0]: Copied! <pre>m = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 117), zoom=9)\n\nm\n</pre> m = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 117), zoom=9)  m <p>To visualise the DEA Land Cover Calendar Year (Landsat) data on an ipyleaflet web map we first need to create a <code>WMSLayer()</code> object (<code>WMSLayer()</code> docs here).</p> <p>To create a <code>WMSLayer()</code> object we need to pass in the <code>url</code> for the WMS server, specify the <code>layers</code> we want to render on the web map, and specify a format for the web map images and provide an attribution. Providing an attribution is often important as many WMS service licenses require an attribution for use.</p> <p>We can use a <code>Map</code> object's <code>add_layer()</code> method to add the WMS layer to the map.</p> In\u00a0[\u00a0]: Copied! <pre>dea_landsat_lc_wms = WMSLayer(\n    url=\"https://ows.dea.ga.gov.au\",\n    layers=\"ga_ls_landcover\",\n    format=\"image/png\",\n    transparent=True,\n    attribution=\"DEA Land Cover Calendar Year (Landsat)\"\n)\n\nm.add_layer(dea_landsat_lc_wms)\n\nm\n</pre> dea_landsat_lc_wms = WMSLayer(     url=\"https://ows.dea.ga.gov.au\",     layers=\"ga_ls_landcover\",     format=\"image/png\",     transparent=True,     attribution=\"DEA Land Cover Calendar Year (Landsat)\" )  m.add_layer(dea_landsat_lc_wms)  m <p>We can change the <code>style</code> used to render the web map image data. The DEA Land Cover Calendar Year (Landsat) has a basic style which we can request from the server - note how we set the <code>styles</code> argument to <code>\"level3\"</code> below. This is an example of how our client, here the ipyleaflet <code>Map</code> object, has dynamically requested the WMS server to generate images for the web map display using a different visual style.</p> <p>We can also create a <code>LegendControl</code> object by passing a dictionary object of legend element names and colours to the <code>LegendControl()</code> constructor function. We can then add the <code>LegendControl</code> object to the <code>Map</code> object. Controls are widgets that we can use to control how we interact with and interpret the map (e.g. zoom widgets, legends, split map, fullscreen etc).</p> In\u00a0[\u00a0]: Copied! <pre># create WMS layer with level3 style for DEA Land Cover Calendar Year (Landsat)\ndea_landsat_basic_lc_wms = WMSLayer(\n    url=\"https://ows.dea.ga.gov.au\",\n    layers=\"ga_ls_landcover\",\n    format=\"image/png\",\n    transparent=True,\n    styles=\"level3\",\n    attribution=\"DEA Land Cover Calendar Year (Landsat)\"\n)\n\n# create dict to define legend elements and colour\nlevel_3_legend = {\n    \"Water\": \"#4d9fdc\",\n    \"Natural Bare Surface\": \"#f3ab69\",\n    \"Artificial Surface\": \"#da5c69\",\n    \"Natural Aquatic Vegetation\": \"#34bf79\",\n    \"Natural Terrestrial Vegetation\": \"#1f7912\",\n    \"Cultivated Terrestrial Vegetation\": \"#acbc2e\"\n}\n\n# create LegendControl object\nlegend = LegendControl(level_3_legend)\n\n# create a new map instance\nm = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 117), zoom=9)\n\n# add WMS layer to map\nm.add_layer(dea_landsat_basic_lc_wms)\n\n# add legend to the map\nm.add_control(legend)\n\n# visualise the map\nm\n</pre> # create WMS layer with level3 style for DEA Land Cover Calendar Year (Landsat) dea_landsat_basic_lc_wms = WMSLayer(     url=\"https://ows.dea.ga.gov.au\",     layers=\"ga_ls_landcover\",     format=\"image/png\",     transparent=True,     styles=\"level3\",     attribution=\"DEA Land Cover Calendar Year (Landsat)\" )  # create dict to define legend elements and colour level_3_legend = {     \"Water\": \"#4d9fdc\",     \"Natural Bare Surface\": \"#f3ab69\",     \"Artificial Surface\": \"#da5c69\",     \"Natural Aquatic Vegetation\": \"#34bf79\",     \"Natural Terrestrial Vegetation\": \"#1f7912\",     \"Cultivated Terrestrial Vegetation\": \"#acbc2e\" }  # create LegendControl object legend = LegendControl(level_3_legend)  # create a new map instance m = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 117), zoom=9)  # add WMS layer to map m.add_layer(dea_landsat_basic_lc_wms)  # add legend to the map m.add_control(legend)  # visualise the map m In\u00a0[\u00a0]: Copied! <pre>payload = {\n    \"service\": \"WMS\",\n    \"version\": \"1.3.0\",\n    \"request\": \"GetCapabilities\"\n}\n\nr_dea_get_cap = requests.get(\"https://ows.dea.ga.gov.au\", params=payload)\nr_dea_get_cap.status_code\n</pre> payload = {     \"service\": \"WMS\",     \"version\": \"1.3.0\",     \"request\": \"GetCapabilities\" }  r_dea_get_cap = requests.get(\"https://ows.dea.ga.gov.au\", params=payload) r_dea_get_cap.status_code <p>This response object contains the Digital Earth Australia's WMS server's metadata in XML format. We can access it via the <code>text</code> property of the response object. However, it's quite long so we'll print the first 2000 characters so you can see the format.</p> In\u00a0[\u00a0]: Copied! <pre>r_dea_get_cap.text[0:2000]\n</pre> r_dea_get_cap.text[0:2000] <p>We can use the Python package OWSLib for easier querying and filtering of WMS server metadata. Passing in the URL for a WMS server to the OWSLib <code>WebMapService()</code> function creates a <code>WebMapService_1_3_0</code> object that we can use to query a WMS service.</p> In\u00a0[\u00a0]: Copied! <pre>dea_wms = WebMapService(\"https://ows.dea.ga.gov.au\", version=\"1.3.0\")\ndea_wms.identification.title\n</pre> dea_wms = WebMapService(\"https://ows.dea.ga.gov.au\", version=\"1.3.0\") dea_wms.identification.title <p>We can use the <code>WebMapService_1_3_0</code> object's <code>contents</code> property to print a list of <code>layers</code> that the WMS service can generate web map images for.</p> In\u00a0[\u00a0]: Copied! <pre>list(dea_wms.contents)\n</pre> list(dea_wms.contents) <p>Each of these layers has a more descriptive title property.</p> In\u00a0[\u00a0]: Copied! <pre>dea_wms[\"ga_ls_wo_fq_cyear_3\"].title\n</pre> dea_wms[\"ga_ls_wo_fq_cyear_3\"].title <p>We can also see what <code>styles</code> the WMS server can use to render web map images.</p> In\u00a0[\u00a0]: Copied! <pre>dea_wms[\"ga_ls_wo_fq_cyear_3\"].styles\n</pre> dea_wms[\"ga_ls_wo_fq_cyear_3\"].styles <p>We can see what coordinate reference systems the WMS server can use when generating web map images.</p> In\u00a0[\u00a0]: Copied! <pre>dea_wms[\"ga_ls_wo_fq_cyear_3\"].crsOptions\n</pre> dea_wms[\"ga_ls_wo_fq_cyear_3\"].crsOptions In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>dea_wos_wms = WMSLayer(\n    url=\"https://ows.dea.ga.gov.au\",\n    layers=\"ga_ls_wo_fq_cyear_3\",\n    format=\"image/png\",\n    transparent=True,\n    styles=\"annual_wofs_frequency_3\",\n    attribution=\"DEA Water Observations Calendar Year (Landsat)\"\n)\n\nm = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 117), zoom=10)\n\nm.add_layer(dea_wos_wms)\n\nm\n</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>class TimeWMSLayer(WMSLayer):\n\n    time = Unicode('').tag(sync=True, o=True)\n</pre> class TimeWMSLayer(WMSLayer):      time = Unicode('').tag(sync=True, o=True) In\u00a0[\u00a0]: Copied! <pre># create WMS layer with level3 style for DEA Land Cover Calendar Year (Landsat)\ndea_2020_basic_lc_wms = TimeWMSLayer(\n    url=\"https://ows.dea.ga.gov.au\",\n    layers=\"ga_ls_landcover\",\n    format=\"image/png\",\n    transparent=True,\n    styles=\"level3\",\n    time=\"2020-01-01\",\n    attribution=\"DEA Land Cover Calendar Year (Landsat)\"\n)\n\ndea_2000_basic_lc_wms = TimeWMSLayer(\n    url=\"https://ows.dea.ga.gov.au\",\n    layers=\"ga_ls_landcover\",\n    format=\"image/png\",\n    transparent=True,\n    styles=\"level3\",\n    time=\"2000-01-01\",\n    attribution=\"DEA Land Cover Calendar Year (Landsat)\"\n)\n\n# create dict to define legend elements and colour\nlevel_3_legend = {\n    \"Water\": \"#4d9fdc\",\n    \"Natural Bare Surface\": \"#f3ab69\",\n    \"Artificial Surface\": \"#da5c69\",\n    \"Natural Aquatic Vegetation\": \"#34bf79\",\n    \"Natural Terrestrial Vegetation\": \"#1f7912\",\n    \"Cultivated Terrestrial Vegetation\": \"#acbc2e\"\n}\n\n# create LegendControl object\nlegend = LegendControl(level_3_legend)\n</pre> # create WMS layer with level3 style for DEA Land Cover Calendar Year (Landsat) dea_2020_basic_lc_wms = TimeWMSLayer(     url=\"https://ows.dea.ga.gov.au\",     layers=\"ga_ls_landcover\",     format=\"image/png\",     transparent=True,     styles=\"level3\",     time=\"2020-01-01\",     attribution=\"DEA Land Cover Calendar Year (Landsat)\" )  dea_2000_basic_lc_wms = TimeWMSLayer(     url=\"https://ows.dea.ga.gov.au\",     layers=\"ga_ls_landcover\",     format=\"image/png\",     transparent=True,     styles=\"level3\",     time=\"2000-01-01\",     attribution=\"DEA Land Cover Calendar Year (Landsat)\" )  # create dict to define legend elements and colour level_3_legend = {     \"Water\": \"#4d9fdc\",     \"Natural Bare Surface\": \"#f3ab69\",     \"Artificial Surface\": \"#da5c69\",     \"Natural Aquatic Vegetation\": \"#34bf79\",     \"Natural Terrestrial Vegetation\": \"#1f7912\",     \"Cultivated Terrestrial Vegetation\": \"#acbc2e\" }  # create LegendControl object legend = LegendControl(level_3_legend) <p>We're now ready to create a split map. First we create a <code>Map</code> object. Then we create a <code>SplitMapControl</code> object using the <code>SplitMapControl()</code> constructor function. The <code>SplitMapControl()</code> constructor function has <code>left_layer</code> and <code>right_layer</code> arguments; we'll pass the <code>TimeWMSLayer</code> object for the 2000 land cover map (<code>dea_2000_basic_lc_wms</code>) to the <code>left_layer</code> and the <code>TimeWMSLayer</code> object for the 2020 land cover map (<code>dea_2020_basic_lc_wms</code>) to the <code>right_layer</code>.</p> <p>Then we add the <code>SplitMapControl</code> object to the <code>Map</code> object. This should generate two linked web maps centred on Perth. If you pan around in the web map you should see land cover change between 2000 and 2020 as Perth has urbanised. See if you can spot other cases of land cover change around Western Australia (e.g. conversion of natural terrestrial vegetation to cultivated terrestrial vegetation).</p> In\u00a0[\u00a0]: Copied! <pre># create a new map instance\nm = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 116), zoom=11)\n\nsplit_map_control = SplitMapControl(left_layer=dea_2000_basic_lc_wms, right_layer=dea_2020_basic_lc_wms)\n\nm.add_control(split_map_control)\n\nm.add_control(legend)\n\nm\n</pre> # create a new map instance m = Map(basemap=basemaps.CartoDB.Positron, center=(-32, 116), zoom=11)  split_map_control = SplitMapControl(left_layer=dea_2000_basic_lc_wms, right_layer=dea_2020_basic_lc_wms)  m.add_control(split_map_control)  m.add_control(legend)  m <p>At zoom level $n$ there are $2^{n} * 2^{n}$ map tiles. We can use this formula to compute the number of map tiles at different zoom levels. Zoom levels 0 to 18 are most common. We can see how the storage requirements for XYZ tiles at higher zoom levels quickly increases.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"number of tiles at zoom level 0: {(2**0) * (2**0)}\")\nprint(f\"number of tiles at zoom level 1: {(2**1) * (2**1)}\")\nprint(f\"number of tiles at zoom level 2: {(2**2) * (2**2)}\")\nprint(f\"number of tiles at zoom level 3: {(2**3) * (2**3)}\")\nprint(f\"number of tiles at zoom level 4: {(2**4) * (2**4)}\")\nprint(f\"number of tiles at zoom level 5: {(2**5) * (2**5)}\")\nprint(f\"number of tiles at zoom level 6: {(2**6) * (2**6)}\")\nprint(f\"number of tiles at zoom level 7: {(2**7) * (2**7)}\")\nprint(f\"number of tiles at zoom level 8: {(2**8) * (2**8)}\")\nprint(f\"number of tiles at zoom level 9: {(2**9) * (2**9)}\")\nprint(f\"number of tiles at zoom level 10: {(2**10) * (2**10)}\")\nprint(f\"number of tiles at zoom level 11: {(2**11) * (2**11)}\")\nprint(f\"number of tiles at zoom level 12: {(2**12) * (2**12)}\")\nprint(f\"number of tiles at zoom level 13: {(2**13) * (2**13)}\")\nprint(f\"number of tiles at zoom level 14: {(2**14) * (2**14)}\")\nprint(f\"number of tiles at zoom level 15: {(2**15) * (2**15)}\")\nprint(f\"number of tiles at zoom level 16: {(2**16) * (2**16)}\")\nprint(f\"number of tiles at zoom level 17: {(2**17) * (2**17)}\")\nprint(f\"number of tiles at zoom level 18: {(2**18) * (2**18)}\")\n</pre> print(f\"number of tiles at zoom level 0: {(2**0) * (2**0)}\") print(f\"number of tiles at zoom level 1: {(2**1) * (2**1)}\") print(f\"number of tiles at zoom level 2: {(2**2) * (2**2)}\") print(f\"number of tiles at zoom level 3: {(2**3) * (2**3)}\") print(f\"number of tiles at zoom level 4: {(2**4) * (2**4)}\") print(f\"number of tiles at zoom level 5: {(2**5) * (2**5)}\") print(f\"number of tiles at zoom level 6: {(2**6) * (2**6)}\") print(f\"number of tiles at zoom level 7: {(2**7) * (2**7)}\") print(f\"number of tiles at zoom level 8: {(2**8) * (2**8)}\") print(f\"number of tiles at zoom level 9: {(2**9) * (2**9)}\") print(f\"number of tiles at zoom level 10: {(2**10) * (2**10)}\") print(f\"number of tiles at zoom level 11: {(2**11) * (2**11)}\") print(f\"number of tiles at zoom level 12: {(2**12) * (2**12)}\") print(f\"number of tiles at zoom level 13: {(2**13) * (2**13)}\") print(f\"number of tiles at zoom level 14: {(2**14) * (2**14)}\") print(f\"number of tiles at zoom level 15: {(2**15) * (2**15)}\") print(f\"number of tiles at zoom level 16: {(2**16) * (2**16)}\") print(f\"number of tiles at zoom level 17: {(2**17) * (2**17)}\") print(f\"number of tiles at zoom level 18: {(2**18) * (2**18)}\") <p>XYZ tiles at higher zoom levels are rendered with progressively more spatial detail.</p> <p> OpenStreetMap zoom = 5, x = 9, y = 20, request =  <code>https://tile.openstreetmap.org/5/9/20.png</code></p> <p>We can convert latitude and longitude coordinates to XYZ map tile coordinates. The following is a convenient function from the OpenStreetMap Wiki for this task.</p> In\u00a0[\u00a0]: Copied! <pre>import math\ndef deg2num(lat_deg, lon_deg, zoom):\n  lat_rad = math.radians(lat_deg)\n  n = 2.0 ** zoom\n  xtile = int((lon_deg + 180.0) / 360.0 * n)\n  ytile = int((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)\n  return (xtile, ytile)\n</pre> import math def deg2num(lat_deg, lon_deg, zoom):   lat_rad = math.radians(lat_deg)   n = 2.0 ** zoom   xtile = int((lon_deg + 180.0) / 360.0 * n)   ytile = int((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)   return (xtile, ytile) <p>Perth's coordinates in latitude and longitude are -31.9523 and 115.8613. Let's pass these numbers into the <code>deg2num()</code> function to return the appropriate tile numbers.</p> In\u00a0[\u00a0]: Copied! <pre># zoom level 6\nprint(f\"x coordinate and y coordinate for Perth at zoom level 6: {deg2num(-31.9523, 115.8613, 6)}\")\n\n# zoom level 12\nprint(f\"x coordinate and y coordinate for Perth at zoom level 12: {deg2num(-31.9523, 115.8613, 12)}\")\n</pre> # zoom level 6 print(f\"x coordinate and y coordinate for Perth at zoom level 6: {deg2num(-31.9523, 115.8613, 6)}\")  # zoom level 12 print(f\"x coordinate and y coordinate for Perth at zoom level 12: {deg2num(-31.9523, 115.8613, 12)}\") <p> OpenStreetMap zoom = 6, x = 52, y = 38, request =  <code>https://tile.openstreetmap.org/6/52/38.png</code></p> <p> OpenStreetMap zoom = 12, x = 3366, y = 2432, request =  <code>https://tile.openstreetmap.org/12/3366/2432.png</code></p> <p>ipyleaflet has a <code>TileLayer()</code> class that can be used to add XYZ map tiles to a web map using a <code>Map</code> object's <code>add_layer()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>osm_tiles = TileLayer(\n    url=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",\n    attribution=\"Map data (c) &lt;a href=\u201dhttps://openstreetmap.org\u201d&gt;OpenStreetMap&lt;/a&gt; contributors\"\n)\n\nm = Map(center=(-31.9523, 115.8613), zoom=9)\n\nm.add_layer(osm_tiles)\n\nm\n</pre> osm_tiles = TileLayer(     url=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",     attribution=\"Map data (c) OpenStreetMap contributors\" )  m = Map(center=(-31.9523, 115.8613), zoom=9)  m.add_layer(osm_tiles)  m <p>You can find a list of XYZ map tile providers here.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>esri_tiles = TileLayer(\n    url=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png\",\n    attribution=\"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\n)\n\nm = Map(center=(-31.9523, 115.8613), zoom=9)\n\nm.add_layer(esri_tiles)\n\nm\n</pre> <p></p> <p>Data WA's Data Catalogue lists a Farm Dams of Western Australia dataset which is available as a WMS layer from the following URL: https://services.slip.wa.gov.au/public/services/SLIP_Public_Services/Water/MapServer/WMSServer at <code>layers</code> <code>0</code>. Can you create a web map where the farm dams are rendered as a WMS layer on top of the ESRI World Imagery XYZ map tiles?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>esri_tiles = TileLayer(\n    url=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png\",\n    attribution=\"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\n)\n\nfarm_dams_wms = WMSLayer(\n    url=\"https://services.slip.wa.gov.au/public/services/SLIP_Public_Services/Water/MapServer/WMSServer\",\n    layers=\"0\",\n    format=\"image/png\",\n    transparent=True,\n    attribution=\"Farm dams of Western Australia (DPIRD-083) \"\n)\n    \n\nm = Map(center=(-32, 117), zoom=14)\n\nm.add_layer(esri_tiles)\n\nm.add_layer(farm_dams_wms)\n\nm\n</pre>"},{"location":"notebooks/week-7_1/#introduction","title":"Introduction\u00b6","text":""},{"location":"notebooks/week-7_1/#web-mapping","title":"Web mapping\u00b6","text":"<p>Static maps are rendered images with detail and styling to highlight a specific location, feature, or phenomenon. However, they are limited. A static map of a wide-area will omit local detail, and a static map of a small area will omit wide-area context. They are not interactive.</p> <p>To addresss these limitations, and using web technologies such as making HTTP requests, web maps are interactive maps that update the information shown in response to user input (zooming in and out, panning).</p> <p>To generate a web map your web browser, or another client application such as an app on a mobile device, makes a HTTP request to a server to GET a web map. The client's HTTP request will include some parameters that describe the map it wishes to receive such as the location and zoom level. In response, the server will either generate the map dynamically (i.e. query a spatial database and render an image) or retrieve a pre-generated map image and respond by sending the map image to the client. Often, this image is in JPEG or PNG format. The client will then render the map in the client application that made the call to the web map server.</p> <p>As you interact with the web map on your display (e.g. with your mouse zooming in and panning), the client will send new requests to the server which will respond by sending updated image data. This is what makes web mapping interactice.</p>"},{"location":"notebooks/week-7_1/#web-maps","title":"Web maps\u00b6","text":"<p>Quin (2020) describe a web map as comprising three components or layers:</p> <ul> <li>basemap: underlying spatial layers designed to provide geographic context. These are often pre-rendered map tiles that fuse together several spatial layers (e.g. roads, water bodies, buildings) that are not the main focus of the map.</li> <li>thematic elements: geographic features that are the main focus of the map and are rendered on top of the basemap layers. Often, these elements are styled to emphasise patterns in the data or distinguish these elements from the basemap. These could be layers that you can toggle on and off.</li> <li>interactive elements: there can be interactive elements rendered on a web map that provide extra information in response to user input (e.g. hover or popup text windows).</li> </ul> <p>We can also think of there being three types of web maps:</p> <ul> <li>dynamic: dynamic web map images are rendered on demand following a client request by a map server. Depending on how the server is configured, it can generate bespoke web maps for a particular client from spatial data.</li> <li>tiled: tiled web map images are pre-generated and stored in cloud storage or in a database and a server retrieves the appropriate image tiles to fill the client's web map, as indicated by parameters in the client request, and sends them to the client.</li> <li>client-generated: coordinates representing a geometry (e.g. a GeoJSON object) are sent to the client and a client application, such as a web browser, renders the geometry on a web map.</li> </ul> <p>In reality, we often mix these three different types of web maps to produce a final composite web map. For example, dynamic map servers can generate tiles creating the \"illusion\" of a tiled map output. Or, we might use a pre-generated tiled basemap to provide geographic context and then use a dynamic web map to render \"real-time\" information in a spatial layer on top of it. An example of this is visualising traffic information with traffic-light shades on top of Google Maps pre-generated basemap tiles.</p>"},{"location":"notebooks/week-7_1/#task","title":"Task\u00b6","text":"<p>The focus of this lab is to provide an introduction to web mapping technologies, highlight their strengths and limitations for particular contexts, and provide some recipes for how you can request web map data using Python. First, we will demonstrate how to request web map layers from servers that implement the Web Map Services (WMS) specification (dynamic web maps); a range of data providers make their data accessible as WMS services (e.g. Digital Earth Australia and public datasets in Western Australia via the Data WA Data Catalogue). Second, we will explore the structure of pre-generated tiled web maps based on XYZ map tiles or \"slippy\" maps which are often used for generating basemap images (e.g. in OpenStreetMap or Google Maps).</p>"},{"location":"notebooks/week-7_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-7_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-7_1/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-7_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-7_1/#web-map-services-wms","title":"Web Map Services (WMS)\u00b6","text":"<p>Servers implmenting the Web Map Services (WMS) specification support dynamically drawn web maps. It is an Open Geospatial Consortium (OGC) specification that server-side applications implement to describe the requests they can receive to generate web map images on-demand for clients.</p> <p>The client request to a WMS server indicates the location, extent, coordinate reference system, and potentially a range of other parameters that determine how the web map is generated and styled from spatial data (e.g. a spatial database). The server responds by sending a rendered image of the spatial data as a JPEG or PNG file.</p> <p>As you move around and zoom in and out of your web map widget (e.g. in your browser), new requests are sent to the WMS server and it responds by generating new images and sending them back to the client to construct the web map in the user interface.</p>"},{"location":"notebooks/week-7_1/#recap-quiz","title":"Recap quiz\u00b6","text":"Can you think of a disadvantage of dynamically generating web map images from spatial data in response to client requests? If you move around quickly in your web map widget, you might generate lots of requests for the WMS server to handle (query a spatial database for relevant data, render spatial data into an image, and send the image back to the client). The server might not be able to keep up with the speed at which you are moving around in the web map widget and you will experience a lag. A lag could also be generated if a WMS server is handling requests from many different clients.   <p>A client makes a request to a server implementing the WMS specification by sending a HTTP message to URL of the server. The client includes information for server to use when handling the request as query parameters appended to the URL. The WMS specification has a <code>REQUEST</code> parameter; when this is set to the value <code>GetMap</code> it will request the server to generate a web map image. Setting the <code>REQUEST</code> value to <code>GetCapabilities</code> will return a metadata about the WMS server. Values set for the <code>REQUEST</code> parameter indicate the WMS operation the server should undertake - you can see the list of operations here.</p>"},{"location":"notebooks/week-7_1/#wms-getmap-operation","title":"WMS <code>GetMap</code> operation\u00b6","text":"<p>When requesting a <code>GetMap</code> operation from a WMS server, a client can specify some of the following query parameters:</p> <ul> <li><code>layers</code> - layers to display on the map</li> <li><code>srs</code> or <code>crs</code> - coordinate reference system of the map output</li> <li><code>styles</code> - styles that define how layers are rendered</li> <li><code>bbox</code> -  bounding box for map extent in units of <code>srs</code></li> </ul> <p>A full description of query parameters for a <code>GetMap</code> operation can be found in the GeoServer docs.</p> <p>Let's demonstrate this by making a single request to a WMS server hosted by Digital Earth Australia to retrieve a web map image of the DEA Land Cover Calendar Year (Landsat) dataset.</p> <p>This is an example request URL made by a web map client for land cover data for a region close to Perth (this URL was extracted from the Digital Earth Australia Map):</p> <p>https://ows.dea.ga.gov.au/?transparent=true&amp;format=image/png&amp;exceptions=XML&amp;styles=level4&amp;version=1.3.0&amp;time=2020-01-01&amp;service=WMS&amp;request=GetMap&amp;layers=ga_ls_landcover&amp;bbox=12885448.480201874,-3786384.6331344917,12895232.419822376,-3776600.6935139894&amp;width=256&amp;height=256&amp;crs=EPSG:3857</p> <p>You can see the URL for the WMS server <code>https://ows.dea.ga.gov.au/</code> and then several query parameters that define the <code>GetMap</code> request. The query parameters are appended after the domain name for the server as indicated by the <code>?</code> symbol. Query parameters have a <code>parameter=value</code> format and are separated by the <code>&amp;</code> symbol. You can find more information about query parameters here and here (you can also refer back to lab-6 for a recap on query parameters).</p> <p>You can see query parameters indicating the <code>format</code> for the returned image (<code>format=image/png</code>), that the service to generate the image is WMS (<code>service=WMS</code>), that the request operation is <code>GetMap</code> (<code>request=GetMap</code>), and that the layer to render is defined by <code>layers=ga_ls_landcover</code> (a web map server will often be able to access a range of spatial datasets from which it can generate an image). There are also other query parameters that define the location for the image, the size of the image, and coordinate reference system. All this information is used by the server to generate the image returned to the client.</p> <p>A <code>requests</code> object's <code>get()</code> method has a <code>params</code> argument that can take a Python dictionary object specifying key:value pairs to convert to query parameters and append to the URL for a HTTP GET message.</p>"},{"location":"notebooks/week-7_1/#ipyleaflet","title":"ipyleaflet\u00b6","text":"<p>It is rare that we'll need to manually generate URLs with query parameters to send to WMS servers; we'll use client web map applications, such as the DEA Map, for these tasks. Here, we'll work with ipyleaflet, which is based on the Leaflet web mapping software.</p> <p>We can use ipyleaflet to create a <code>Map</code> widget obejct using the <code>Map()</code> constructor function. The <code>Map</code> object is an interactive widget in our display that we can add web map layers to using the <code>add_layer()</code> method or add controls to using the <code>add_controls()</code> method.</p> <p>When we create a <code>Map</code> widget obejct we can select a <code>basemap</code>, <code>center</code> location, and <code>zoom</code> level.</p>"},{"location":"notebooks/week-7_1/#wms-getcapabilities-operation","title":"WMS <code>GetCapabilities</code> operation\u00b6","text":"<p>The WMS <code>GetCapabilities</code> operation returns metadata about a WMS server to a client. A <code>GetCapabilities</code> response is an XML document with three sections:</p> <ul> <li>service: metadata about the organisation providing the WMS server.</li> <li>request: the request operations the server provides and the input parameters and response data formats for each operation.</li> <li>layer: layers and coordinate systems the WMS server provides.</li> </ul> <p>We can use <code>GetCapabilities</code> requests to retrieve information from the WMS server necessary for generating WMS layers in our web maps. Let's make a <code>GetCapabilities</code> request to the Digital Earth Australia WMS server.</p>"},{"location":"notebooks/week-7_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you use the above information that describes the DEA Water Observations Calendar Year (Landsat) WMS layer to create an ipyleaflet <code>WMSLayer()</code> object and add it to a new <code>Map</code> object?</p> <p>The DEA Water Observations Calendar Year (Landsat) dataset is a satellite-derived water summary showing what percentage of clear satellite observations were detected as wet in a year. The legend for the WMS layer is below.</p> <p></p>"},{"location":"notebooks/week-7_1/#custom-wmslayer-objects","title":"Custom <code>WMSLayer()</code> objects\u00b6","text":"<p>If the WMS server accepts other query parameters to define what data and styling is used to create web map images, we need to extend the base <code>WMSLayer()</code> class to create a custom class to handle the extra query parameters. A common extra query parameter that WMS servers implement is <code>time</code> which specifies a date or time for the underlying data used to generate the web map images.</p> <p>The below code snipped demonstrates how to create a custom <code>TimeWMSLayer()</code> class with an extra <code>time</code> parameter.</p>"},{"location":"notebooks/week-7_1/#split-map-controls","title":"Split map controls\u00b6","text":"<p>The Digital Earth Australia WMS server endpoint for the DEA Land Cover Calendar Year (Landsat) dataset accepts a <code>time</code> parameter that allows us to define a year to generate the web map images for.</p> <p>ipyleaflet also provides an option for a <code>SplitMapControl</code> where one web map is rendered on the left panel and another on the right panel. We can create a <code>SplitMapControl</code> that displays the DEA Land Cover Calendar Year (Landsat) dataset for different years to allow us to see spatial patterns of land cover change over time.</p> <p>First, we need to create two <code>TimeWMSLayer</code> objects that represent the DEA Land Cover Calendar Year (Landsat) dataset for 2020 and 2000. You can see that we pass in a date string to the <code>time</code> argument. These are instances of the custom class we created above.</p>"},{"location":"notebooks/week-7_1/#xyz-tiles-and-slippy-maps","title":"XYZ Tiles and slippy maps\u00b6","text":"<p>Tiled web maps (or XYZ maps or \"slippy\" maps) are based on pre-generated images, drawn from spatial data, for a range of possible zoom levels and locations. Generally, these pre-generated images are JPEG or PNG files storing a 256 x 256 pixel image. Each image is a \"map tile\".</p> <p>The web map images that we rendered above are based on WMS layers and are generated dynamically by a server in response to client requests. In contrast, XYZ map tiles are already computed and are stored in a directory on the server or a cloud storage bucket. As a user pans around and zooms in and out on a web map, the server sends the required map tiles to fill the display.</p> <p>As the tiles are pre-generated, XYZ map tiles are more performant than dynamic web maps. The performance benefits of XYZ web maps is particularly apparent with larger spatial datasets which would be computationally intensive to generate on-demand (e.g. high-resolution images satellite and aerial images).</p>"},{"location":"notebooks/week-7_1/#recap-quiz","title":"Recap quiz\u00b6","text":"What are potential disadvantages of serving web map data as pre-generated XYZ map tiles? <ul> <li>There are storage costs associated with storing pre-generated map tiles for all locations and zoom levels (e.g. consider the storage costs associated with high-resolution RGB web maps with complete global coverage).</li> <li>The up-front computational costs to generate the map tiles can significant and problematic if the map tiles need to be updated frequently.</li> <li>As the map tile images are pre-generated it is not possible to customise their styling.</li> </ul> <p>Despite these limitations, most basemaps (e.g. the satellite basemap in Google Maps or Bing Maps) which are used to provide geographic context are served as XYZ tiles because of the performance benefits.</p> <p>XYZ map tiles are described by their zoom level (the Z) and coordinates (the X and Y) and comrpise a pyramid of images.</p> <p>At zoom level 0 the entire world fits into a 256 x 256 pixel image in the web mercator projection system (<code>EPSG:3857</code>). Let's get this most zoomed out map tile for OpenStreetMap. XYZ map tiles have a request format of <code>https://{URL}/{zoom level}/{x coordinate}/{y coordinate}.png</code>.</p> <p>For OpenStreetMap this request would be <code>https://tile.openstreetmap.org/0/0/0.png</code>.</p> <p> OpenStreetMap zoom = 0, x = 0, y = 0 <code>https://tile.openstreetmap.org/0/0/0.png</code></p> <p>At each zoom level, the number of 256 x 256 pixel map tiles increases by a factor of 4. The map tile coordinates always start with x = 0 and y = 0 in the top left.</p> <p> OpenStreetMap zoom = 1, x = 0, y = 0, request = <code>https://tile.openstreetmap.org/1/0/0.png</code></p> <p> OpenStreetMap zoom = 1, x = 0, y = 1, request =  <code>https://tile.openstreetmap.org/1/0/1.png</code></p> <p> OpenStreetMap zoom = 1, x = 1, y = 0, request =  <code>https://tile.openstreetmap.org/1/1/0.png</code></p> <p> OpenStreetMap zoom = 1, x = 1, y = 1, request =  <code>https://tile.openstreetmap.org/1/1/1.png</code></p>"},{"location":"notebooks/week-7_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you add ESRI World Imagery XYZ map tiles to a <code>Map</code> object centred on Perth? The URL is: <code>https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.png</code> and the attribution is: <code>\"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"</code>.</p>"},{"location":"notebooks/week-8_1/","title":"Week 8 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-7\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-7.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-7.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-7\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-7 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-7\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-7.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-7.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-7\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-7 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") <p>You will provided with:</p> <ol> <li>A GeoTIFF file for each camera band.</li> <li>A shapefile of the calibration panel's extent.</li> <li>A CSV file that contains the MicaSense RedEdge-MX band parmeters (you would normally need to look up wavelength centre and band width for each of the sensor bands in the camera specificaions).</li> </ol> In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n    !pip install rasterstats\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio     !pip install rasterstats In\u00a0[\u00a0]: Copied! <pre>import os\nimport geopandas as gpd\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport rioxarray as rxr\nimport plotly.io as pio\nimport plotly.express as px\nimport fiona\nfrom sklearn.linear_model import LinearRegression\nfrom rasterstats import zonal_stats\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os import geopandas as gpd import pandas as pd import numpy as np import xarray as xr import rioxarray as rxr import plotly.io as pio import plotly.express as px import fiona from sklearn.linear_model import LinearRegression from rasterstats import zonal_stats  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>path_to_week7_data = os.path.join(os.getcwd(), \"data_lab-7\")\nos.listdir(path_to_week7_data)\n</pre> path_to_week7_data = os.path.join(os.getcwd(), \"data_lab-7\") os.listdir(path_to_week7_data) In\u00a0[\u00a0]: Copied! <pre># visualise NIR data\nswan_nir_path = os.path.join(path_to_week7_data, \"swan_nir.tif\")\n\nnir = rxr.open_rasterio(swan_nir_path)\nnir.sel(band=1).plot.imshow(robust=True)\n</pre> # visualise NIR data swan_nir_path = os.path.join(path_to_week7_data, \"swan_nir.tif\")  nir = rxr.open_rasterio(swan_nir_path) nir.sel(band=1).plot.imshow(robust=True) In\u00a0[\u00a0]: Copied! <pre># visualise RED data\nswan_red_path = os.path.join(path_to_week7_data, \"swan_red.tif\")\n\nred = rxr.open_rasterio(swan_red_path)\nred.sel(band=1).plot.imshow(robust=True)\n</pre> # visualise RED data swan_red_path = os.path.join(path_to_week7_data, \"swan_red.tif\")  red = rxr.open_rasterio(swan_red_path) red.sel(band=1).plot.imshow(robust=True) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>#Import shape file for panels\npanel_path = os.path.join(os.getcwd(), \"data_lab-7\", \"calibration_panels.shp\")\npanel_gdf = gpd.read_file(panel_path)\n</pre> <p>Before moving on we need to reverse the ordering of the data in the <code>panel</code> column so it has a black-to-white ordering. Let's do this and also inspect the data.</p> In\u00a0[\u00a0]: Copied! <pre>#reverse b-w instead w-b \npanel_gdf = panel_gdf.copy().iloc[::-1]\npanel_gdf = panel_gdf.reset_index(drop=True)\npanel_gdf.head()\n</pre> #reverse b-w instead w-b  panel_gdf = panel_gdf.copy().iloc[::-1] panel_gdf = panel_gdf.reset_index(drop=True) panel_gdf.head() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>#import asd data\nasd_fp = os.path.join(os.getcwd(), \"data_lab-7\", \"panels_reflectance_values.csv\")\nasd = pd.read_csv(asd_fp)\nasd.head()\n</pre> In\u00a0[\u00a0]: Copied! <pre># summary of asd data\nasd.describe()\n</pre> # summary of asd data asd.describe() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>#read in camera band values\nmica_mx_fp = os.path.join(os.getcwd(), \"data_lab-7\", \"rededgemx_bands.csv\")\nmica_mx = pd.read_csv(mica_mx_fp)\nmica_mx.head()\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>asd_panels = []\n\nfor i in mica_mx[\"band\"]:\n    print(f\"processing MicaSense RedEdge-MX camera band: {i}\")\n    # For each band, subset the lower and upper band reflectance from the DataFrame mica_mx\n    band_lowerlim = pd.Series(mica_mx[mica_mx[\"band\"] == i][\"band_low\"]).item()\n    band_upperlim = pd.Series(mica_mx[mica_mx[\"band\"] == i][\"band_up\"]).item()\n    \n    # get asd values between the mica_mx band limits\n    mx_asd = asd[(asd[\"Wavelength\"] &gt;= band_lowerlim) &amp; (asd[\"Wavelength\"] &lt;= band_upperlim)]\n    \n    # compute the mean of asd values between mica_mx band limits\n    mx_asd_panel_mean = mx_asd.mean() \n    asd_panels.append(mx_asd_panel_mean.iloc[1:].values)\n\nsurface_reflectance = pd.DataFrame(asd_panels, columns=panel_gdf[\"panel\"])   \nsurface_reflectance.index= mica_mx[\"band\"]\n\nsurface_reflectance.head()\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>dn_panels = []\n\n#extract digital number mean for each pannel \nfor i in mica_mx[\"band\"]:\n    print(f\"zonal stats for MicaSense RedEdge-MX camera band: {i}\")\n    \n    # update path to GeoTIFF file for each camera band\n    mg_fp= os.path.join(os.getcwd(), \"data_lab-7\", f\"swan_{i}.tif\")\n    \n    # use the zonal_stats function to compute mean digital number values for each of the calibration panels\n    zs = zonal_stats(panel_gdf, mg_fp, stats=\"mean\")\n    \n    zs = pd.DataFrame(zs)\n    band_panel_dn = zs[\"mean\"].values\n    dn_panels.append(band_panel_dn)\n\nmx_dn_panels = pd.DataFrame(dn_panels, columns=panel_gdf[\"panel\"])\nmx_dn_panels.index= mica_mx[\"band\"]\n\nmx_dn_panels.head()\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>for i in mica_mx[\"band\"]:\n    fig= px.scatter(\n        x=mx_dn_panels.loc[i], \n        y=surface_reflectance.loc[i], \n        trendline=\"ols\",\n        title=i)\n    \n    fig.update_layout(\n        xaxis_title=\"Digital Number\",\n        yaxis_title=\"Surface Reflectance\"\n    )\n    \n    fig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre># create a list to save corrected outputs\ncorrected_images = []\n\nfor i in mica_mx[\"band\"]: \n    print(f\"\")\n    print(f\"correcting band {i}\")\n    \n    x = mx_dn_panels.loc[i].values.reshape(-1,1)\n    y = surface_reflectance.loc[i].values.reshape(-1,1)\n    \n    model = LinearRegression().fit(x, y)\n    #Obtain the coefficient of determination by calling the model with the score() function, then print the coefficient:\n    r_sq = model.score(x, y)\n    intercept = model.intercept_[0]\n    coef = model.coef_[0][0]\n    print(\"coefficient of determination:\", r_sq)\n    #Print the Intercept:\n    print(\"intercept:\", intercept)\n    #Print the Slope:\n    print(\"slope:\", coef)\n\n    #read in raw dn image and apply correction and save\n    mg_fp= os.path.join(os.getcwd(), \"data_lab-7\", f\"swan_{i}.tif\")\n    band = rxr.open_rasterio(mg_fp).sel(band=1).values\n    print(band)\n        \n    # use the model to radiometrically correct UAV image\n    band_corrected = coef*band + intercept\n    \n    # visualise outputs\n    fig_dn = px.imshow(band, title = f\"DN band {i}\")\n    fig_dn.show()\n    \n    fig_corr = px.imshow(band_corrected, title = f\"corrected band {i}\")\n    fig_corr.show()\n    \n    # store corrected images as a list\n    corrected_images.append(band_corrected)\n</pre> <p></p> <p>Let's check an RGB image of the vineyard looks sensible.</p> In\u00a0[\u00a0]: Copied! <pre># create an rgb image of the corrected data\nred_band = corrected_images[2]\ngreen_band = corrected_images[1]\nblue_band = corrected_images[0]\n\n# make RGB image\nrgb = np.stack((red_band, green_band, blue_band), axis=0)\n\n# plot the rgb image\npx.imshow(np.moveaxis(rgb, 0, 2), contrast_rescaling=\"minmax\", height=550)\n</pre> # create an rgb image of the corrected data red_band = corrected_images[2] green_band = corrected_images[1] blue_band = corrected_images[0]  # make RGB image rgb = np.stack((red_band, green_band, blue_band), axis=0)  # plot the rgb image px.imshow(np.moveaxis(rgb, 0, 2), contrast_rescaling=\"minmax\", height=550) answer <p>This one's on you... you've got it!</p>"},{"location":"notebooks/week-8_1/#using-remotely-piloted-aircraft-systems-rpas-for-environmental-monitoring","title":"Using Remotely Piloted Aircraft Systems (RPAS) for environmental monitoring\u00b6","text":"<p>Week 8 focuses on using UAVs for science and demonstrating the process to collect UAV images through to generating science-ready data for answering agricultural questions.</p>"},{"location":"notebooks/week-8_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-8_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-8_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-8_1/#background","title":"Background\u00b6","text":"<p>This notebook to demonstrate analysis techniques for working with UAV data.</p> <p>The lecture will provide a high-level overview of:</p> <ol> <li><p>Remote sensing- When are UAVs the right tool for the job?</p> <ul> <li>Spatial Resolution (pixel size).</li> <li>Scale (the area you need to cover and flight time etc.).<p></p> </li> </ul> </li> <li><p>UAV safety and regulations</p> <ul> <li>See Civil Aviation Safety Authority (CASA) - Drone rules.<p></p> </li> </ul> </li> <li><p>Mission Planning 101</p> <ul> <li>The key to good mission planning and data quality is to think through the question you are trying to answer. You should try and resolve these key questions at the mission planning stage:<ul> <li>What is the required ground pixel size, and what image overlap/flight path is needed?</li> <li>How accurately do you need to know the ground location? Do you require survey control?</li> <li>Is a snapshot of relative pixel difference within one flight sufficient, or will you be comparing changes in data across time from multiple flights? Do you need to consider calibration?</li> <li>Are there any hazards or safety considerations in the area of interest? What is the weather looking like for my planned flying day? What time of day should I be flying to get the best data quality?</li> </ul> </li> <li>Once you've thought through this, the next step is using mission planning software to develop a flight plan. Here are some examples of mission planning software:<ul> <li>DJI GS Pro</li> <li>Pix4D Capture</li> <li>Drone Deploy<p></p> </li> </ul> </li> </ul> </li> <li><p>Thinking about calibration</p> <ul> <li><p>Geometric Calibration</p> <ul> <li>Inertial Measurement Unit (IMU): IMU measures force, angular velocity, and attitude (accelerometer, barometer, gyroscope, and thermometer). The IMU keeps the UAV flying straight on the path outlined in your mission planning.</li> <li>Ground Control Points (GCP): GCPs are known locations in the imagery that are surveyed accurately and used to georeference the imagery. Identifiable objects are spaced across the flight area and surveyed with an RTK GPS. Some GCPs are left out of image processing and used as checkpoints to verify the accuracy of the final orthophoto.</li> <li>Real-Time Kinematic (RTK) Drones: RTK drone survey use real-time correction with location data from a GPS while the drone is capturing imagery. As these platforms reduce in price and depending on the accuracy required from the final product, it may remove the need for a lot of GCPs.</li> </ul> </li> <li><p>Radiometric Calibration</p> <ul> <li>Multispectral imaging sensors do not directly measure reflectance. Instead, they measure at-sensor radiance, the radiant flux received by the sensor. Each image is a matrix of digital numbers relative to flight conditions. Radiometric calibration is required to convert raw digital numbers recorded by the camera into surface reflectance</li> <li>Calibration Panels: Panels that are measured with a spectrometer capture their reflectance values and are placed within the flight area. Panels are commonly made from masonite board with a rough surface (Lambertian reflectance) and painted in scale from white to black (contrasting albedo). It is common for science grade data to use at least four pannels.</li> <li>Incident light sensor: This sensor can also be called the Downwelling Light Sensor (DLS). It can be mounted on top of a multispectral camera to record lighting conditions throughout the flight and writes this data to each image's metadata. This data can be used to improve the accuracy of calibration.</li> <li>Empirical Line Calibration: The empirical line method is widely used to calibrate remotely sensed images to surface reflectance. Reference targets (calibration panels) of known reflectance are placed in the image scene. The raw digital number for each panel is extracted from the image during preprocessing. Linear regression is applied between the panel's known surface reflectance and the digital number to build a calibration equation for each sensor band. The calibration equation is applied to each image band to get surface reflectance values. Note-this method assumes a linear relationship between surface reflectance and at-sensor radiance which may not always be the case.<p></p> </li> </ul> </li> </ul> </li> </ol> <p></p> <p>Source: Vegetation monitoring using multispectral sensors \u2014 best practices and lessons learned from high latitudes - Assmann et al. (2018)</p> <p></p> <ol> <li>Capturing the Images<ul> <li>On the day you are flying, you should. Check the weather to ensure it is within UAV limitations and won't adversely impact your data. If the light condition changes a lot across flights (i.e. clouds), rescheduling may be better. It is much easier to get good data than to try and correct for variance across the flight.</li> </ul> </li> </ol>"},{"location":"notebooks/week-8_1/#background-reading","title":"Background Reading\u00b6","text":"<p>Vegetation monitoring using multispectral sensors \u2014 best practices and lessons learned from high latitudes - Assmann et al. (2018)</p>"},{"location":"notebooks/week-8_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Why do you think calibration is required? What factors will affect that data? When does calibration matter?</p> answer Weather changes, sun angle. Calibration is essential to compare changes in spectral indices across days or seasons, e.g. if we wish to monitor crop growth or health over time."},{"location":"notebooks/week-8_1/#simple-radiometric-correction-of-swan-valley-imagery","title":"Simple radiometric correction of Swan Valley imagery\u00b6","text":"<p>You will use the skills you learnt across weeks 1-6 (working with GeoTIFF files, raster geoprocessing operations, band stacking, navigating directories and file systems with imagery stored different folders).</p> <p>Our post-processing will focus on the radiometric calibration of UAV imagery. This allows us to produce comparable images and vegetation indices across flights and days. Radiometric calibration is the process of converting digital numbers captured by the camera sensor to physical units such as surface reflectance (the ratio of incoming solar radiation to reflected light across different spectral wavebands).</p> <p>Why convert our image data to spectral reflectance values?</p> <p>The key objective for this week is to show the importance of calibration when working with UAV imagery. The satellite data we have worked with so far across weeks 1-6 has already had radiometric, spectral and geometric calibration applied by the data providers. For example, you can view the Sentinel data quality report.</p> <p>This notebook is focused on simple radiometric calibration of 5-band multispectral imagery. The imagery you will be using was collected over the Swan Valley a by MicaSense RedEdge-MX camera mounted on a DJI Phantom 4. For this lab, the imagery has been clipped to a small subsection containing some grapevines and the calibration panels to speed up processing.</p> <p>Imagery bands:</p> <ul> <li>1: Blue</li> <li>2: Greeb</li> <li>3: Red</li> <li>4: Red Edge</li> <li>5: Near Infrared</li> </ul>"},{"location":"notebooks/week-8_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-8_1/#data-exploration","title":"Data exploration\u00b6","text":"<p>Before getting started, let's check the data files for this exercise have downloaded without error. You should be able to identify what data is stored in each file by the file's name and extension.</p> <p>Let's also visualise the near infrared and red images captured by the MicaSense RedEdge-MX camera.</p> <p>Can you spot the calibration panels in the images?</p> <p>Can you explain why the vines appear bright in the near infrared image and dark in the red image?</p>"},{"location":"notebooks/week-8_1/#task-1","title":"Task 1\u00b6","text":"<p>Read in the calibration panel data stored in the shapefile <code>calibration_panels.shp</code>. Reference this data with a variable called <code>panel_gdf</code>.</p>"},{"location":"notebooks/week-8_1/#task-2","title":"Task 2\u00b6","text":"<p>Read in the asd data (measured spectra of the calibration panels). This data is stored in the CSV file <code>panels_reflectance_values.csv</code> and you should read it into a pandas <code>DataFrame</code> referenced by the variable <code>asd</code>. An ASD spectroradiometer is used to accurately measure the spectral reflectance of each of the calibration panels.</p>"},{"location":"notebooks/week-8_1/#task-3","title":"Task 3\u00b6","text":"<p>Read in camera band values for the MicaSense RedEdge-MX band parmeters. This data is stored in the file <code>rededgemx_bands.csv</code>. Read this data into a pandas <code>DataFrame</code> referenced by the variable <code>mica_mx</code>.</p>"},{"location":"notebooks/week-8_1/#task-4","title":"Task 4\u00b6","text":"<p>Retrieve the known spectral reflectance values of the calibration panels for each of the MicaSense RedEdge-MX camera bands. There are a few steps to this task. So, take a few minutes to work through what is happenning in the below code snippet. We loop over each spectral waveband in the MicaSense RedEdge-MX camera band:</p> <ol> <li>subset the upper and lower spectral reflectance values for each band.</li> <li>subset the asd measured spectral reflectance of the calibration panels within the wavelength range corresponding to each of the MicaSense RedEdge-MX camera bands.</li> <li>compute the mean of the asd measured spectral reflectance of calibration panels within the wavelength range corresponding to each of the MicaSense RedEdge-MX camera bands.</li> </ol> <p>We assume that the asd measured spectral reflectance for each of the calibration panels is accurate and we implement these steps to compute what the asd measured spectral reflectance is for each of the MicaSense RedEdge-MX camera bands. Thus, we expect our UAV imagery's spectral reflectance values over the calibration panels to match the asd data. If there are discrepancies, we adjust the UAV imagery so it matches the asd measured spectral reflectance. This is radiometric image calibration.</p> <p>Remember you can use <code>print()</code> statements to help visualise what your code is doing.</p>"},{"location":"notebooks/week-8_1/#task-5","title":"Task 5\u00b6","text":"<p>Extract the mean digital number for the extent of each calibration panel in the UAV imagery for each camera band. You will need to use the locations of the calibration panels that you read into your program in Task 1 and are stored as polygons in the <code>GeoDataFrame</code> <code>panel_gdf</code>. You will need to use these polygons in a zonal statistics operation for each of the UAV imagery's camera bands. Each band is stored in a separate GeoTIFF file.</p> <p>You can use the <code>zonal_stats()</code> function from the rasterstats package for this task.</p> <p>Try and use a for loop to iterate over the GeoTIFF files and apply the <code>zonal_stats()</code> function. This is a good example of how we can use a for loop to automate a repetitive task. Store the mean band digital numbers for the extent of each calibration panel in the UAV imagery as a pandas <code>DataFrame</code>.</p>"},{"location":"notebooks/week-8_1/#task-6","title":"Task 6\u00b6","text":"<p>Can you generate scatter plots that visualise the relationship between calibration panel's surface reflectance and their digital numbers in the UAV images? The UAV image digital numbers for the panels are stored in the <code>DataFrame</code> <code>mx_dn_panels</code> and the calibration panel's surface reflectance is stored in the <code>DataFrame</code> <code>surface_reflectance</code>. Remember you can use the <code>head()</code> function to glimpse at the structure of the data in these <code>DataFrame</code>s. Also, remember you can look up reference information for creating scatter plots in the Plotly Express docs.</p>"},{"location":"notebooks/week-8_1/#task-7","title":"Task 7\u00b6","text":"<p>In week 5 you used Scikit-learn to train machine learning models. We need to create a linear regression model that will learn relationships between a calibration panel's surface reflectance (measured by the asd) and the calibration panel's digital number captured by the UAV for each camera band. We can then use this model to correct the UAV images to surface reflectance. Have a go at fitting this linear regression model using Scikit learn and correcting the UAV images. If you get stuck, check the answer and make sure you take time to understand what each step in the code is doing.</p>"},{"location":"notebooks/week-8_1/#task-8","title":"Task 8\u00b6","text":"<p>Create a vegetation index (e.g NDVI) from your corrected data. NumPy <code>ndarray</code> objects storing the corrected data are elements in the list <code>corrected_images</code> at the following index locations:</p> <ol> <li>blue</li> <li>green</li> <li>red</li> <li>red edge</li> <li>near infrared</li> </ol>"},{"location":"notebooks/week-9/","title":"Week 9","text":"In\u00a0[1]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-9\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget -O data_lab-9.zip \"https://dl.dropboxusercontent.com/scl/fi/em9snffp00tqxcq05j2f5/data_lab-9.zip?rlkey=hky02us6z0t48sugnkgx3h0x8&amp;dl=0\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-9.zip\"', shell=True, capture_output=True, text=True)\n</pre> import os import subprocess  if \"data_lab-9\" not in os.listdir(os.getcwd()):     subprocess.run('wget -O data_lab-9.zip \"https://dl.dropboxusercontent.com/scl/fi/em9snffp00tqxcq05j2f5/data_lab-9.zip?rlkey=hky02us6z0t48sugnkgx3h0x8&amp;dl=0\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-9.zip\"', shell=True, capture_output=True, text=True) In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[4]: Copied! <pre>import rioxarray as rxr\nimport xarray as xr\nimport pandas as pd\nimport numpy as np\nfrom scipy.signal import savgol_filter\n\nimport rioxarray as rxr\nimport geopandas as gpd\nimport rasterio\n\nfrom ipyleaflet import Map, ImageOverlay, basemaps\nfrom ipywidgets import Output, HBox\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import rioxarray as rxr import xarray as xr import pandas as pd import numpy as np from scipy.signal import savgol_filter  import rioxarray as rxr import geopandas as gpd import rasterio  from ipyleaflet import Map, ImageOverlay, basemaps from ipywidgets import Output, HBox import matplotlib.pyplot as plt import plotly.express as px import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[5]: Copied! <pre>bands_nm = [\n    399.309, 402.826, 406.344, 409.861, 413.378, 416.896, 420.413, 423.93,\n    427.448, 430.965, 434.482, 438.0,   441.517, 445.034, 448.552, 452.069,\n    455.586, 459.104, 462.621, 466.138, 469.656, 473.173, 476.69,  480.208,\n    483.725, 487.242, 490.76,  494.277, 497.794, 501.312, 504.829, 508.346,\n    511.864, 515.381, 518.898, 522.416, 525.933, 529.45,  532.968, 536.485,\n    540.002, 543.519, 547.037, 550.554, 554.071, 557.589, 561.106, 564.623,\n    568.141, 571.658, 575.175, 578.693, 582.21,  585.727, 589.245, 592.762,\n    596.279, 599.797, 603.314, 606.831, 610.349, 613.866, 617.383, 620.901,\n    624.418, 627.935, 631.453, 634.97,  638.487, 642.005, 645.522, 649.039,\n    652.557, 656.074, 659.591, 663.109, 666.626, 670.143, 673.661, 677.178,\n    680.695, 684.213, 687.73,  691.247, 694.765, 698.282, 701.799, 705.317,\n    708.834, 712.351, 715.869, 719.386, 722.903, 726.421, 729.938, 733.455,\n    736.973, 740.49,  744.007, 747.525, 751.042, 754.559, 758.077, 761.594,\n    765.111, 768.628, 772.146, 775.663, 779.18,  782.698, 786.215, 789.732,\n    793.25,  796.767, 800.284, 803.802, 807.319, 810.836, 814.354, 817.871,\n    821.388, 824.906, 828.423, 831.94,  835.458, 838.975, 842.492, 846.01,\n    849.527, 853.044, 856.562, 860.079, 863.596, 867.114, 870.631, 874.148,\n    877.666, 881.183, 884.7,  888.218, 891.735, 895.252, 898.77,  902.287,\n    905.804, 909.322, 912.839, 916.356, 919.874, 923.391, 926.908, 930.426,\n    933.943, 937.46,  940.978, 944.495, 948.012, 951.53,  955.047, 958.564,\n    962.082, 965.599, 969.116, 972.634, 976.151, 979.668, 983.185, 986.703,\n    990.22,  993.737, 997.255, 1000.77\n]\n</pre> bands_nm = [     399.309, 402.826, 406.344, 409.861, 413.378, 416.896, 420.413, 423.93,     427.448, 430.965, 434.482, 438.0,   441.517, 445.034, 448.552, 452.069,     455.586, 459.104, 462.621, 466.138, 469.656, 473.173, 476.69,  480.208,     483.725, 487.242, 490.76,  494.277, 497.794, 501.312, 504.829, 508.346,     511.864, 515.381, 518.898, 522.416, 525.933, 529.45,  532.968, 536.485,     540.002, 543.519, 547.037, 550.554, 554.071, 557.589, 561.106, 564.623,     568.141, 571.658, 575.175, 578.693, 582.21,  585.727, 589.245, 592.762,     596.279, 599.797, 603.314, 606.831, 610.349, 613.866, 617.383, 620.901,     624.418, 627.935, 631.453, 634.97,  638.487, 642.005, 645.522, 649.039,     652.557, 656.074, 659.591, 663.109, 666.626, 670.143, 673.661, 677.178,     680.695, 684.213, 687.73,  691.247, 694.765, 698.282, 701.799, 705.317,     708.834, 712.351, 715.869, 719.386, 722.903, 726.421, 729.938, 733.455,     736.973, 740.49,  744.007, 747.525, 751.042, 754.559, 758.077, 761.594,     765.111, 768.628, 772.146, 775.663, 779.18,  782.698, 786.215, 789.732,     793.25,  796.767, 800.284, 803.802, 807.319, 810.836, 814.354, 817.871,     821.388, 824.906, 828.423, 831.94,  835.458, 838.975, 842.492, 846.01,     849.527, 853.044, 856.562, 860.079, 863.596, 867.114, 870.631, 874.148,     877.666, 881.183, 884.7,  888.218, 891.735, 895.252, 898.77,  902.287,     905.804, 909.322, 912.839, 916.356, 919.874, 923.391, 926.908, 930.426,     933.943, 937.46,  940.978, 944.495, 948.012, 951.53,  955.047, 958.564,     962.082, 965.599, 969.116, 972.634, 976.151, 979.668, 983.185, 986.703,     990.22,  993.737, 997.255, 1000.77 ] In\u00a0[7]: Copied! <pre>vnir_path = os.path.join(\"SAMPLE_AOI_CSIRO_Cotton1_VNIR_Orthomosaic_ESPG4326_v1.tif\")\nvnir_da = rxr.open_rasterio(vnir_path)\nvnir_da[\"band\"] = bands_nm\n# scale to reflectance\nvnir_da = vnir_da * 0.0001\n</pre> vnir_path = os.path.join(\"SAMPLE_AOI_CSIRO_Cotton1_VNIR_Orthomosaic_ESPG4326_v1.tif\") vnir_da = rxr.open_rasterio(vnir_path) vnir_da[\"band\"] = bands_nm # scale to reflectance vnir_da = vnir_da * 0.0001 <p>If we print out a description of our <code>vnir_da</code> <code>DataArray</code> object we can see it has cube-like dimensions. The <code>x</code> and <code>y</code> dimensions correspond to a pixel's position and the <code>band</code> dimension corresponds to reflectance in different wavelengths.</p> In\u00a0[\u00a0]: Copied! <pre>vnir_da\n</pre> vnir_da In\u00a0[38]: Copied! <pre>def interactive_spectral_profile(spectral_da, spectra_da_2=None):\n    # Define map\n    m = Map(center=(-30.267316273, 149.899925737), zoom=22, max_zoom=26)\n\n    # Output widget for map and spectral profile plot\n    out = Output()\n\n    # Define click handler\n    def handle_click_raw_vnir(**kwargs):\n        \"\"\"\n        Display the spectral profile of a clicked point.\n        Uses the raw hyperspectral reflectance.\n        \"\"\"\n        if kwargs.get(\"type\") == \"click\":\n            latlng = kwargs.get(\"coordinates\")\n            with out:\n                out.clear_output(wait=True)\n                df = pd.DataFrame({\"wavelength (nm)\": bands_nm, \"reflectance\": spectral_da.sel(x=latlng[1], y=latlng[0], method=\"nearest\").values})\n                fig = px.line(\n                    df,\n                    x=\"wavelength (nm)\",\n                    y=\"reflectance\",\n                    template=\"plotly_white\",\n                )\n                fig.show()\n\n\n    # Add RGB image overlay\n    overlay = ImageOverlay(\n        url=\"https://raw.githubusercontent.com/geog3300-agri3003/lab-data/main/SAMPLE_AOI_CSIRO_Cotton1_RGB_Orthomosaic_6mm_EPSG_v1.png\",\n        bounds=[(-30.267316273, 149.899925737), (-30.267254275, 149.900012578)],\n        opacity=0.6\n    )\n    m.add(overlay)\n\n    # Register the click event handler with the map\n    m.on_interaction(handle_click_raw_vnir)\n\n    # Display map and spectral profile plot\n    return HBox([m, out])\n</pre> def interactive_spectral_profile(spectral_da, spectra_da_2=None):     # Define map     m = Map(center=(-30.267316273, 149.899925737), zoom=22, max_zoom=26)      # Output widget for map and spectral profile plot     out = Output()      # Define click handler     def handle_click_raw_vnir(**kwargs):         \"\"\"         Display the spectral profile of a clicked point.         Uses the raw hyperspectral reflectance.         \"\"\"         if kwargs.get(\"type\") == \"click\":             latlng = kwargs.get(\"coordinates\")             with out:                 out.clear_output(wait=True)                 df = pd.DataFrame({\"wavelength (nm)\": bands_nm, \"reflectance\": spectral_da.sel(x=latlng[1], y=latlng[0], method=\"nearest\").values})                 fig = px.line(                     df,                     x=\"wavelength (nm)\",                     y=\"reflectance\",                     template=\"plotly_white\",                 )                 fig.show()       # Add RGB image overlay     overlay = ImageOverlay(         url=\"https://raw.githubusercontent.com/geog3300-agri3003/lab-data/main/SAMPLE_AOI_CSIRO_Cotton1_RGB_Orthomosaic_6mm_EPSG_v1.png\",         bounds=[(-30.267316273, 149.899925737), (-30.267254275, 149.900012578)],         opacity=0.6     )     m.add(overlay)      # Register the click event handler with the map     m.on_interaction(handle_click_raw_vnir)      # Display map and spectral profile plot     return HBox([m, out]) In\u00a0[\u00a0]: Copied! <pre>interactive_spectral_profile(vnir_da)\n</pre> interactive_spectral_profile(vnir_da) In\u00a0[17]: Copied! <pre># Savitzky-Golay smoothing\ndef sg_smooth(da, window_length=10, polyorder=2):\n    \"\"\"\n    Apply a Savitzky-Golay filter to a DataArray.\n    \"\"\"\n\n    # smooth spectral reflectance values\n    sg_arr = savgol_filter(\n        da.values,\n        window_length=window_length,\n        polyorder=polyorder,\n        axis=da.get_axis_num(\"band\")\n    )\n\n    da_sg = xr.DataArray(\n        sg_arr,\n        coords=da.coords,\n        dims=da.dims,\n    )\n\n    # compute 1st derivative of smoothed spectral reflectance\n    sg_1d_arr = savgol_filter(\n        da.values,\n        window_length=window_length,\n        polyorder=polyorder,\n        axis=da.get_axis_num(\"band\"),\n        deriv=1,\n    )\n\n    da_sg_1d = xr.DataArray(\n        sg_1d_arr,\n        coords=da.coords,\n        dims=da.dims,\n    )\n\n    return da_sg, da_sg_1d\n</pre> # Savitzky-Golay smoothing def sg_smooth(da, window_length=10, polyorder=2):     \"\"\"     Apply a Savitzky-Golay filter to a DataArray.     \"\"\"      # smooth spectral reflectance values     sg_arr = savgol_filter(         da.values,         window_length=window_length,         polyorder=polyorder,         axis=da.get_axis_num(\"band\")     )      da_sg = xr.DataArray(         sg_arr,         coords=da.coords,         dims=da.dims,     )      # compute 1st derivative of smoothed spectral reflectance     sg_1d_arr = savgol_filter(         da.values,         window_length=window_length,         polyorder=polyorder,         axis=da.get_axis_num(\"band\"),         deriv=1,     )      da_sg_1d = xr.DataArray(         sg_1d_arr,         coords=da.coords,         dims=da.dims,     )      return da_sg, da_sg_1d <p>Let's pass in our raw hyperspectral data to <code>sg_smooth()</code> and smooth it using a Savitzky-Golay filter. Then, let's use it to make an interactive plot. As you click on the  RGB image you should a smoother spectral profile for each clicked location.</p> In\u00a0[18]: Copied! <pre>vnir_da_sg, vnir_da_sg_1d = sg_smooth(vnir_da)\n</pre> vnir_da_sg, vnir_da_sg_1d = sg_smooth(vnir_da) In\u00a0[\u00a0]: Copied! <pre>interactive_spectral_profile(vnir_da_sg)\n</pre> interactive_spectral_profile(vnir_da_sg) In\u00a0[20]: Copied! <pre>def normalised_difference(da, band_1_nm, band_2_nm):\n    \"\"\"\n    Calculate the normalised difference between two bands.\n    \"\"\"\n    b1 = da.sel(band=band_1_nm, method=\"nearest\")\n    b2 = da.sel(band=band_2_nm, method=\"nearest\")\n\n    return (b1 - b2) / (b1 + b2)\n</pre> def normalised_difference(da, band_1_nm, band_2_nm):     \"\"\"     Calculate the normalised difference between two bands.     \"\"\"     b1 = da.sel(band=band_1_nm, method=\"nearest\")     b2 = da.sel(band=band_2_nm, method=\"nearest\")      return (b1 - b2) / (b1 + b2) <p>Let's use the <code>normalised_difference()</code> function to compute the NDVI. To keep things simple we'll select the centre of the Sentinel-2 wavebands to represent red and near infrared reflectance.</p> <p>Look at the NDVI plot. What do you think would be a good threshold value to separate vegetation (cotton plants) and non-vegetation surfaces?</p> In\u00a0[\u00a0]: Copied! <pre># mask out soil background\n# S2 red = 665\n# S2 nir = 865\nndvi = normalised_difference(vnir_da_sg, 865, 665)\nndvi.plot.imshow(cmap=\"Greens\", robust=True, add_labels=False)\n</pre> # mask out soil background # S2 red = 665 # S2 nir = 865 ndvi = normalised_difference(vnir_da_sg, 865, 665) ndvi.plot.imshow(cmap=\"Greens\", robust=True, add_labels=False) <p>We can use xarray's <code>where()</code> function to set all NDVI values greater than a threshold to one and all other pixels to zero.</p> <p>The <code>where()</code> function takes a condition as its first argument (e.g. <code>ndvi &gt;= 0.7</code>). It's second argument is what value to assign the output in all locations where the condition is true and the third argument is the value to assign locations where the condition is false.</p> <p>These are the docs for the where function. It's worth being familiar with <code>where()</code> as it's commonly used for processing raster and imagery data.</p> In\u00a0[\u00a0]: Copied! <pre>veg_mask = xr.where(ndvi &gt;= 0.7, 1, 0)\nveg_mask.plot.imshow()\n</pre> veg_mask = xr.where(ndvi &gt;= 0.7, 1, 0) veg_mask.plot.imshow() <p>The final task is the apply the vegetation mask to our hyperspectral data cube setting all pixels where NDVI is less than the threshold to a no data value.</p> In\u00a0[23]: Copied! <pre>vnir_da_sg_veg = vnir_da_sg.where(veg_mask == 1)\n</pre> vnir_da_sg_veg = vnir_da_sg.where(veg_mask == 1) In\u00a0[\u00a0]: Copied! <pre># Landsat 8 band metadata: (center wavelength [nm], bandwidth [nm], color)\nbands = {\n    'B2 (Blue)': (482, 60, '#0000ff'),\n    'B3 (Green)': (561, 57, '#00ff00'),\n    'B4 (Red)': (655, 37, '#ff0000'),\n    'B5 (NIR)': (865, 28, '#fdae61'),\n}\n\nfig, ax = plt.subplots(figsize=(12, 6))\nspectral_profile_1 = vnir_da_sg.sel(x=149.89996779710054, y=-30.26729097043827, method=\"nearest\").values\nspectral_profile_2 = vnir_da_sg.sel(x=149.89997752010822, y=-30.267298788893758, method=\"nearest\").values\nax.plot(bands_nm, spectral_profile_1, color=\"red\")\nax.plot(bands_nm, spectral_profile_2, color=\"blue\")\n\n# Plot vertical Landsat 8 bands\nfor name, (center, width, color) in bands.items():\n    start = center - width / 2\n    end = center + width / 2\n    ax.axvspan(start, end, alpha=0.3, color=color, label=name)\n\nax.set_xlabel(\"Wavelength (nm)\")\nax.set_ylabel(\"Reflectance\")\nax.set_title(\"Landsat-8 Wavebands\")\nax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1), fontsize=\"small\")\nplt.tight_layout()\nplt.show()\n</pre> # Landsat 8 band metadata: (center wavelength [nm], bandwidth [nm], color) bands = {     'B2 (Blue)': (482, 60, '#0000ff'),     'B3 (Green)': (561, 57, '#00ff00'),     'B4 (Red)': (655, 37, '#ff0000'),     'B5 (NIR)': (865, 28, '#fdae61'), }  fig, ax = plt.subplots(figsize=(12, 6)) spectral_profile_1 = vnir_da_sg.sel(x=149.89996779710054, y=-30.26729097043827, method=\"nearest\").values spectral_profile_2 = vnir_da_sg.sel(x=149.89997752010822, y=-30.267298788893758, method=\"nearest\").values ax.plot(bands_nm, spectral_profile_1, color=\"red\") ax.plot(bands_nm, spectral_profile_2, color=\"blue\")  # Plot vertical Landsat 8 bands for name, (center, width, color) in bands.items():     start = center - width / 2     end = center + width / 2     ax.axvspan(start, end, alpha=0.3, color=color, label=name)  ax.set_xlabel(\"Wavelength (nm)\") ax.set_ylabel(\"Reflectance\") ax.set_title(\"Landsat-8 Wavebands\") ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1), fontsize=\"small\") plt.tight_layout() plt.show() <p>The Sentinel-2 sensor has wavebands located within the red-edge spectrum (bands 5, 6 and 7 with a 20 m spatial resolution). This permits monitoring change in the reflectance properties of vegetation in red-edge portion of the spectrum, which could reflect change in plant condition.</p> In\u00a0[\u00a0]: Copied! <pre># Sentinel-2 waveband data (central wavelength in nm and bandwidth in nm)\nbands = {\n    'B1 (Coastal aerosol)': (443, 20, '#5e4fa2'),\n    'B2 (Blue)': (490, 65, '#0000ff'),\n    'B3 (Green)': (560, 35, '#00ff00'),\n    'B4 (Red)': (665, 30, '#ff0000'),\n    'B5 (Red Edge 1)': (705, 15, '#e6f598'),\n    'B6 (Red Edge 2)': (740, 15, '#ffffbf'),\n    'B7 (Red Edge 3)': (783, 20, '#fee08b'),\n    'B8 (NIR)': (842, 115, '#fdae61'),\n    'B8A (Narrow NIR)': (865, 20, '#f46d43'),\n}\n\nfig, ax = plt.subplots(figsize=(12, 6))\nspectral_profile_1 = vnir_da_sg.sel(x=149.89996779710054, y=-30.26729097043827, method=\"nearest\").values\nspectral_profile_2 = vnir_da_sg.sel(x=149.89997752010822, y=-30.267298788893758, method=\"nearest\").values\nax.plot(bands_nm, spectral_profile_1, color=\"red\")\nax.plot(bands_nm, spectral_profile_2, color=\"blue\")\n\n# Plot vertical S2 bands\nfor name, (center, width, color) in bands.items():\n    start = center - width / 2\n    end = center + width / 2\n    ax.axvspan(start, end, alpha=0.3, color=color, label=name)\n\nax.set_xlabel(\"Wavelength (nm)\")\nax.set_ylabel(\"Reflectance\")\nax.set_title(\"Sentinel-2 Wavebands\")\nax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1), fontsize=\"small\")\nplt.tight_layout()\nplt.show()\n</pre> # Sentinel-2 waveband data (central wavelength in nm and bandwidth in nm) bands = {     'B1 (Coastal aerosol)': (443, 20, '#5e4fa2'),     'B2 (Blue)': (490, 65, '#0000ff'),     'B3 (Green)': (560, 35, '#00ff00'),     'B4 (Red)': (665, 30, '#ff0000'),     'B5 (Red Edge 1)': (705, 15, '#e6f598'),     'B6 (Red Edge 2)': (740, 15, '#ffffbf'),     'B7 (Red Edge 3)': (783, 20, '#fee08b'),     'B8 (NIR)': (842, 115, '#fdae61'),     'B8A (Narrow NIR)': (865, 20, '#f46d43'), }  fig, ax = plt.subplots(figsize=(12, 6)) spectral_profile_1 = vnir_da_sg.sel(x=149.89996779710054, y=-30.26729097043827, method=\"nearest\").values spectral_profile_2 = vnir_da_sg.sel(x=149.89997752010822, y=-30.267298788893758, method=\"nearest\").values ax.plot(bands_nm, spectral_profile_1, color=\"red\") ax.plot(bands_nm, spectral_profile_2, color=\"blue\")  # Plot vertical S2 bands for name, (center, width, color) in bands.items():     start = center - width / 2     end = center + width / 2     ax.axvspan(start, end, alpha=0.3, color=color, label=name)  ax.set_xlabel(\"Wavelength (nm)\") ax.set_ylabel(\"Reflectance\") ax.set_title(\"Sentinel-2 Wavebands\") ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1), fontsize=\"small\") plt.tight_layout() plt.show()  In\u00a0[26]: Copied! <pre>def marang_n_conc_iprvi(da):\n    \"\"\"\n    Calculate the N concentration from the IPRVI based on Marang et al., (2021).\n    \"\"\"\n    iprvi = (da.sel(band=695, method=\"nearest\") - da.sel(band=700, method=\"nearest\")) / da.sel(band=705, method=\"nearest\")\n    n_conc = (39.521 * iprvi) + 9.475\n\n    # plot N predictions and VI\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n    n_conc.plot.imshow(robust=True, vmin=3.5, ax=axes[0])\n    axes[0].set_title(\"Pred. N (%)\")\n    axes[0].set_xticks([])\n    axes[0].set_yticks([])\n    axes[0].set_xlabel(None)\n    axes[0].set_ylabel(None)\n    iprvi.plot.imshow(robust=True, ax=axes[1])\n    axes[1].set_title(\"IPRVI\")\n    axes[1].set_xticks([])\n    axes[1].set_yticks([])\n    axes[1].set_xlabel(None)\n    axes[1].set_ylabel(None)\n    plt.tight_layout()\n    plt.show()\n\n    return n_conc, iprvi\n</pre> def marang_n_conc_iprvi(da):     \"\"\"     Calculate the N concentration from the IPRVI based on Marang et al., (2021).     \"\"\"     iprvi = (da.sel(band=695, method=\"nearest\") - da.sel(band=700, method=\"nearest\")) / da.sel(band=705, method=\"nearest\")     n_conc = (39.521 * iprvi) + 9.475      # plot N predictions and VI     fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))     n_conc.plot.imshow(robust=True, vmin=3.5, ax=axes[0])     axes[0].set_title(\"Pred. N (%)\")     axes[0].set_xticks([])     axes[0].set_yticks([])     axes[0].set_xlabel(None)     axes[0].set_ylabel(None)     iprvi.plot.imshow(robust=True, ax=axes[1])     axes[1].set_title(\"IPRVI\")     axes[1].set_xticks([])     axes[1].set_yticks([])     axes[1].set_xlabel(None)     axes[1].set_ylabel(None)     plt.tight_layout()     plt.show()      return n_conc, iprvi In\u00a0[\u00a0]: Copied! <pre>n_conc, iprvi = marang_n_conc_iprvi(vnir_da_sg_veg)\n</pre> n_conc, iprvi = marang_n_conc_iprvi(vnir_da_sg_veg) In\u00a0[\u00a0]: Copied! <pre>dsm_path = os.path.join(\"SAMPLE_AOI_CSIRO_Cotton1_LiDAR_DSM_2cm_EPSG4326_v1.tif\")\ndsm_da = rxr.open_rasterio(dsm_path)\ndsm_da\n</pre> dsm_path = os.path.join(\"SAMPLE_AOI_CSIRO_Cotton1_LiDAR_DSM_2cm_EPSG4326_v1.tif\") dsm_da = rxr.open_rasterio(dsm_path) dsm_da In\u00a0[\u00a0]: Copied! <pre>dsm_da.sel(band=1).plot.imshow()\n</pre> dsm_da.sel(band=1).plot.imshow() In\u00a0[\u00a0]: Copied! <pre>dtm_path = os.path.join(\"SAMPLE_AOI_CSIRO_Cotton1_LiDAR_DTM_100cm_EPSG4326_v1.tif\")\ndtm_da = rxr.open_rasterio(dtm_path)\ndtm_da\n</pre> dtm_path = os.path.join(\"SAMPLE_AOI_CSIRO_Cotton1_LiDAR_DTM_100cm_EPSG4326_v1.tif\") dtm_da = rxr.open_rasterio(dtm_path) dtm_da In\u00a0[\u00a0]: Copied! <pre>dtm_da.sel(band=1).plot.imshow()\n</pre> dtm_da.sel(band=1).plot.imshow() In\u00a0[33]: Copied! <pre>dsm_veg = dsm_da.where(veg_mask == 1)\ndtm_veg = dtm_da.where(veg_mask == 1)\n</pre> dsm_veg = dsm_da.where(veg_mask == 1) dtm_veg = dtm_da.where(veg_mask == 1) <p>Then we can subtract the DTM from the DSM to produce a canopy height model.</p> In\u00a0[\u00a0]: Copied! <pre>chm = dsm_veg - dtm_veg\nchm.sel(band=1).plot.imshow(robust=True)\n</pre> chm = dsm_veg - dtm_veg chm.sel(band=1).plot.imshow(robust=True) In\u00a0[\u00a0]: Copied! <pre>cvm = chm * (15 * 15)\ncvm.sel(band=1).plot.imshow(robust=True)\n</pre> cvm = chm * (15 * 15) cvm.sel(band=1).plot.imshow(robust=True) Can you think of some limitations of using canopy volume models, in the way we've computed it above, as a representation of plant biomass? <p>It may not fully represent the morphometry of complex canopies. For example, if there is \"overhanging\" vegetation this will be assumed to be vegetation from the surface to canopy height at that location.</p> <p>We're not able to separate green leaves from other features such as woody stems, flowers or air gaps.</p>"},{"location":"notebooks/week-9/#high-resolution-uav-data","title":"High-resolution UAV data\u00b6","text":"<p>This lab presents a series of examples working with high-resolution geospatial datasets captured by sensors on UAVs.</p> <p>The data is captured over a cotton trial in New South Wales and the focus of lab activities is extracting variables that describe cotton plant condition from hyperspectral images and LiDAR derived surface models that can be converted into maps of canopy height and volume. The hyperspectral images were captured by a GRYFN Gobi sensor.</p> <p>This lab will cover:</p> <ul> <li>Introducing hyperspectral data cubes and data structures and exploring hyperspectral data.</li> <li>Hyperspectral data pre-processing (smoothing and creating vegetation masks).</li> <li>Statistical modelling to predict cotton plant nitrogen levels from hyperspectral data.</li> <li>Combining hyperspectral data and LiDAR-derived data to compute canopy height and volume models.</li> </ul> <p>For your reference, Ram et al. (2024) provide a good review of hyperspectral remote sensing with a focus on agricultural applications.</p>"},{"location":"notebooks/week-9/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-9/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and, if required, you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-9/#download-data","title":"Download data\u00b6","text":"<p>To download the data for this lab run the following code snippet.</p>"},{"location":"notebooks/week-9/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-9/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-9/#hyperspectral-data-cubes","title":"Hyperspectral data cubes\u00b6","text":"<p>Hyperspectral data is stored in a data cube structure. You can think of this as a 3D stack of arrays where the pixels in each array represent a location on the land surface. Each array in the stack represents a waveband.</p> <p>The hyperspectral data we're working with has 172 wavebands with their wavelengths listed below. Let's also read the hyperspectral data into our Python environment as a xarray <code>DataArray</code> object and explore its cube-like structure.</p>"},{"location":"notebooks/week-9/#exploring-hyperspectral-data","title":"Exploring hyperspectral data\u00b6","text":"<p>Click on the image in the slippy map to generate spectral profile plots for the clicked location.</p> <p>What is the characteristic spectral reflectance feature of green vegetation?</p>"},{"location":"notebooks/week-9/#recap-quiz","title":"Recap quiz\u00b6","text":"Click on the RGB image of the cotton plants and view the spectral reflectance profiles. What are some of the dominant characteristics of cotton reflectance? <ul> <li>There is a sharp increase in reflectance between 600 and 800 nm.</li> <li>There is a small peak in reflectance between 500 and 600 nm.</li> <li>There is short-run variation (noise) in the reflectance profiles, especially at wavelenghts &gt; 800 nm.</li></ul>"},{"location":"notebooks/week-9/#hyperspectral-data-processing","title":"Hyperspectral data processing\u00b6","text":"<p>Prior to analysing hyperspectral data cubes (e.g. prediction plant characteristics from spectral data), several data pre-processing steps are applied. Here, we'll focus on two pre-processing operations:</p> <ul> <li>Smoothing the reflectance values at each pixel to generate a smooth continuous spectral profile. There can be short-run noise in hyperspectral data due to atmospheric artefacts, illumination conditions, scattering effects due to plants and leaves, and noise introduced through sensing and capture conditions.</li> <li>Vegetation masking to remove background soil and other non-vegetative influenses. This allows us to focus only on using hyperspectral data to monitor plant conditions.</li> </ul>"},{"location":"notebooks/week-9/#smoothing","title":"Smoothing\u00b6","text":"<p>A technique commonly used to smooth hyperspectral profiles is a Savitzky-Golay filter. This works by passing a moving window along the spectral data within each pixel and fitting a smooth curve through the values that fall within the moving window.</p> <p>The below function demonstrates how to apply a Savitzky-Golay filter to an xarray <code>DataArray</code> and return a smoothed dataset. The function is setup to use a moving window length of 10 values (see the <code>window_length</code> parameter). You can change this and using a larger window will result in a smoother spectral profile. The goal is to pick a window length that's large enough to smooth out short-run noise in the time-series but the dominant spectral signatures relating to land surface conditions persist.</p> <p>Note, this function also returns the first derivative (rate of change) in spectral reflectance. This is just here for your reference as often the first derivative of hyperspectral data is used to detect change in vegetation conditions and charactersitics</p>"},{"location":"notebooks/week-9/#vegetation-masks","title":"Vegetation masks\u00b6","text":"<p>There are a variety of techniques that are used for vegetation masking. Here, we'll use a simple NDVI threshold to create a binary vegetation and non-vegetation mask.</p> <p>Computing normalised difference spectral indices is a common task when working with hyperspectral data (we have lots of wavebands we can use - we can do more than compute NDVI). Therefore, let's create a convenience function that we can pass in two wavelengths and it will use these wavelengths to compute a normalised difference spectral index from the hyperspectral data cube.</p>"},{"location":"notebooks/week-9/#recap-quiz","title":"Recap quiz\u00b6","text":"What optical satellite data pre-processing step would we use the <code>where()</code> function for? <p>Cloud masking.</p> <p>Here, we'll use an NDVI threshold of 0.7 and create a binary vegetation mask.</p>"},{"location":"notebooks/week-9/#recap-quiz","title":"Recap quiz\u00b6","text":"Consider the strengths and weaknesses of using a NDVI threshold approach to create vegetation masks? What other approaches could we use to do this? <p>Using an NDVI threshold approach to create vegetation masks is relatively simple to apply and it's easy to tune the threshold for for a given scene or image to obtain good results. You might want to use different thresholds at different stages of the growing season. There is clear explainable link between NDVI values and the presence and reflectance characteristics of green vegetation.</p> <p>However, a drawback of using a simple NDVI thresholding approach is that it doesn't segment individual plants so you might get \"noisy\" isolated pixels classed as vegetation. Look up Segment Anything and consider how that could be used to segment plants in a scene. If you have multiple types of vegetation in a scene and you are only interested in one species, it can be hard to discriminate between them using a simple threshold. This could be the case if you want to separate a crop from the interrow vegetation. Consider how you could use machine learning to segment out only pixels that relate to plant types of interest.</p>"},{"location":"notebooks/week-9/#exploring-the-red-edge","title":"Exploring the red-edge\u00b6","text":"<p>Several studies have highlighted that spectral reflectance between the red (~ 670 nm) and the near infrared (~ 800 nm) wavelengths characterise the condition and status of cotton plants. This portion of the electromagnetic spectrum is termed the red-edge due to the sharp increase in reflectance between the red wavelengths and the near infrared wavelengths in green vegetation. The red-edge is defined as the inflection point of the largest increase in reflectance between red and near infrared reflectance.</p> <p>Chlorophyll in green plants absorb red light and near infrared light is reflected by the leaf's internal cell structure. Plants absorb red light for photosynthesis. Near infrared light is reflected (or transmitted through the leaf and re-scattered by the canopy) to avoid the energy damaging the plant.</p> <p>A shift in the red-edge position towards longer wavelengths is associated with an increase in chlorophyll and a shift towards shorter, red, wavelength is indicative of plant stress.</p> <p>Studies have highlighted that changes in spectral reflectance in the red-edge portion of electromagnetic spectrum relate to biophyscical and biochemical variables such as chlorophyll concentration, leaf area index and nitrogren concentration:</p> <ul> <li>Marang et al. (2021) - predicting nitrogren concentration in cotton using hyperspectral images.</li> <li>Zhaoe et al. (2007) - relate hyperspectral images to cotton LAI and canopy chlorophyll density.</li> </ul> <p>With broadband multi-spectral images it is difficult to detect changes in the reflectance around red-edge, which can limit our ability to detect variation in plant condition. Consider the wavebands of the Landsat 8 OLI sensor below. It does not measure reflectance in the red-edge region. This means that any change in plant condition that cause in reflectance changes in the red-edge region would not be captured by the Landsat 8 sensor.</p>"},{"location":"notebooks/week-9/#recap-quiz","title":"Recap quiz\u00b6","text":"Given that the Sentinel-2 satellite captures reflectance in three red-edge bands, what is the value of UAV-based hyperspectral data for monitoring plant condition? <p>The spatial resolution of Sentinel-2's red-edge bands is 20 m (i.e. a 20 m pixel size). This is too coarse for characterising within-plant variability in biophysical and biochemical conditions and is too coarse for many within-field applications.</p> <p>Reflectance in the red-edge portion of the spectrum is represented by three wavebands. In contrast, hyperspectral sensors capture a continuum of reflectance between the red and near infrared wavelengths. This might allow for detecting more subtle shifts in plant condition and for verifying insights derived from Sentinel-2 satellite images.</p>"},{"location":"notebooks/week-9/#predicting-nitrogen-concentration","title":"Predicting nitrogen concentration\u00b6","text":"<p>Marang et al., (2021) combined hyperspectral data with field nitrogen (N) samples over a cotton trial (mature cotton stand) in New South Wales. This study explored how features in hyperspectral data could be used to predict N concentration of cotton leaves from hyperspectral images. They used machine learning to detect wavelengths in a hyperspectral image that were predictive of N concentration and created the following spectral index that aims to capture the red-edge position (inflection point ratio vegetation index).</p> <p>$$ IPRVI = \\frac{R_{695} - R_{700}}{R_{705}} $$</p> <p>Where $R_{695}$ is reflectance at 695 nm. They estimated a linear regression model to predict N concentration (%) as a function of IPRVI:</p> <p>$$ N = 39.521 \\centerdot IPRVI + 9.475 $$</p> <p>We can apply this model to the hyperspectral data we are working with to create a predictive map of N concentration. The following is a function that takes in the hyperspectral data cube as an xarray <code>DataArray</code> object and returns arrays representing predicted N concentration and the IPRVI.</p> <p>Let's pass in the <code>DataArray</code> object that has had the vegetation mask applied so we only compute the IPRVI and N concentration for cotton plants.</p>"},{"location":"notebooks/week-9/#recap-quiz","title":"Recap quiz\u00b6","text":"Look at the map of predicted N concentration above, which areas have the highest N concentrations (i.e. yellow shades)? Can you look at the slippy RGB image of the cotton trial above and consider why these areas have high apparent N concentrations? <p>The areas with the highest predicted N concentrations appear on the left side of the middle rows of cotton plants. These areas look like shadow affected leaves and the high N concentrations could be an image artefact and not actual variability in N.</p> <p>The lesson here is to always check imagery-derived products for artefacts. If we did not scrutinise this data we could risk using shadows to guide N management. When working with imagery-derived products it's important to be inquisitive and check the methods used to generate the imagery underlying predicted products. If possible, try and see the raw imagery.</p> <p>How could we try and remove the shadow affected leaves?</p> <p>Marang et al. (2021) also publish a model that predicts N concentration using the normalised difference red edge index (NDRE). The NDRE is computed as:</p> <p>$$ NDRE = \\frac{R_{790} - R_{720}}{R_{790} + R_{720}} $$</p> <p>with the model to predict N concentration as:</p> <p>$$ N = 12.856 \\centerdot NDRE + 2.935 $$</p> Using the example above that predicts N concentration as a function of the IPRVI, can you convert data in the hyperspectral data cube into a predicted N layer by applying the model above to NDRE values and plot the results? <pre># predict N concentration\ndef marang_n_conc_ndre(da):\n    \"\"\"\n    Calculate the N concentration from the NDRE based on Marang et al., (2021).\n    \"\"\"\n    ndre = normalised_difference(da, 790, 720)\n    n_conc = (12.856 * ndre) - 2.935\n\n    # plot N predictions and VI\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n    n_conc.plot.imshow(robust=True, vmin=0, ax=axes[0])\n    axes[0].set_title(\"Pred. N (%)\")\n    axes[0].set_xticks([])\n    axes[0].set_yticks([])\n    axes[0].set_xlabel(None)\n    axes[0].set_ylabel(None)\n    ndre.plot.imshow(robust=True, vmin=0.3, ax=axes[1])\n    axes[1].set_title(\"NDRE\")\n    axes[1].set_xticks([])\n    axes[1].set_yticks([])\n    axes[1].set_xlabel(None)\n    axes[1].set_ylabel(None)\n    plt.tight_layout()\n    plt.show()\n\n    return n_conc, ndre\n\nn_conc, ndre = marang_n_conc_ndre(vnir_da_sg_veg)\n</pre> How similar are the predicted N concentration layers using IPRVI and NDRE data as inputs? How could / should you interpret the differences between these models when using this data for farm management? What could be potential causes of differences in N predictions? <p>Predicted N from the model using IPRVI is a lot higher than predicted N from the model using NDRE data as an input. We should explore why there are differences in predicted N before using these models for decision making.</p> <p>There could be a few reasons why the models return different N predictions:</p> <ul> <li>Marang et al. 2021 use a relatively small ground truth dataset to fit the models (30 data points). This study is great at highlighting the potential to use hyperspectral data to characterise N variability within cotton fields. However, it's likely that more training and test data would be required to develop general N prediction models.</li> <li>The sensors used to capture the hyperspectral data are different (spatial resolution and spectral resolution). Marang et al. (2021) also do additional pre-processing of the hyperspectral data with spatial as well as spectral smoothing.</li> <li>The timing of image captures could be different. It could be that the relationships between spectral reflectance and N concentration vary through the growing season.</li> </ul>"},{"location":"notebooks/week-9/#canopy-surface-models","title":"Canopy surface models\u00b6","text":"<p>This example demonstrates working with a very different type of high-resolution dataset from UAV mounted sensors. Here, we will be working with raster elevation models derived from a LiDAR sensor that has scanned the canopy.</p> <p>LiDAR stands for light detection and ranging where a laser scanner emits a laser pulse and measures the return time for pulse after it has bounced back off an object. The returns are converted into a point cloud representing the 3D structure of a scene.</p> <p>LiDAR processing software can filter the point clouds to create digital surface models and digital terrain models. A digital surface model (DSM) is a representation of the height of features on the land surface (e.g. the height of plants, trees, buildings). A digital terrain model (DTM) represents the height of the bare earth surface.</p> <p>Let's read in a DSM and DTM for the cotton trial where we have just been working with hyperspectral data. We should be able to pick out the cotton plants in the DSM and as we're working in a small area of the field there should not be much variation in the DTM.</p>"},{"location":"notebooks/week-9/#canopy-height-models","title":"Canopy height models\u00b6","text":"<p>A canopy height model is just the height of the vegetation canopy at each location. To compute the canopy height model we need to mask out non-vegetated areas and then take the difference between the DSM and DTM. First, let's apply our vegetation mask to the DSM and DTM data.</p> What spatial operations need to be applied before applying the vegetation mask to the DTM and DSM layers? (Note, this has already be done in the data provided here). <p>The mask operation works by matching pixels in the DSM and DTM layers to pixels in the vegetation mask layer. Therefore, we need to make sure the mask and the DSM and DTM layers have the same dimensions and the same coordinate reference system (CRS). To ensure the pixels align, we can apply a resampling operation.</p>"},{"location":"notebooks/week-9/#canopy-volume-models","title":"Canopy volume models\u00b6","text":"<p>A canopy volume model is an estimate of the volume of vegetation at a location. It's computed as the area of a pixel multiplied by the height of vegetation at that location. It's a variable that's often used as an indicator of biomass.</p> <p>The pixel dimensions for the data we're working with are approximately 15 cm. Thus, we can multiply each pixel in the canopy height model by <code>15 * 15</code> to estimate the canopy volume.</p>"}]}