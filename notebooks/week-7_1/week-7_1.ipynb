{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Remotely Piloted Aircraft Systems (RPAS) for environmental monitoring\n",
    "\n",
    "Week 8 focuses on using UAVs for science and demonstrating the process to collect UAV images through to generating science-ready data for answering agricultural questions.\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. **If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/geog3300-agri3003/coursebook/blob/main/docs/notebooks/week-7_1.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data_lab-7\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-7.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data_lab-7.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data_lab-7\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data_lab-7 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background \n",
    "\n",
    "A background to UAV remote sensing will be provided as a short lecture, along with a *show and tell* of the platforms and cameras used by researchers within the Centre for Water and Spatial Science (CWSS). Then, we'll work through this notebook to demonstrate analysis techniques for working with UAV data.\n",
    "\n",
    "The lecture will provide a high-level overview of:\n",
    "\n",
    "1. **Remote sensing- When are UAVs the right tool for the job?**\n",
    "    * Spatial Resolution (pixel size). \n",
    "    * Scale (the area you need to cover and flight time etc.).\n",
    "<p></p>\n",
    "\n",
    "2. **UAV safety and regulations** \n",
    "    * See Civil Aviation Safety Authority (CASA) - <a href=\"https://www.casa.gov.au/knowyourdrone/drone-rules\" target=\"_blank\">Drone rules</a>.\n",
    "<p></p>\n",
    "\n",
    "3. **Overview of Types of UAVs and Sensor**\n",
    "    * Interactive show and tell of platforms and cameras used by researchers within the Centre for Water and Spatial Science.\n",
    "<p></p>\n",
    "\n",
    "4. **Mission Planning 101**\n",
    "    * The key to good mission planning and data quality is to think through the question you are trying to answer. You should try and resolve these key questions at the mission planning stage:\n",
    "        * What is the required ground pixel size, and what image overlap/flight path is needed? \n",
    "        * How accurately do you need to know the ground location? Do you require survey control?\n",
    "        * Is a snapshot of relative pixel difference within one flight sufficient, or will you be comparing changes in data across time from multiple flights? Do you need to consider calibration?\n",
    "        * Are there any hazards or safety considerations in the area of interest? What is the weather looking like for my planned flying day? What time of day should I be flying to get the best data quality?\n",
    "    * Once you've thought through this, the next step is using mission planning software to develop a flight plan. Here are some examples of mission planning software:\n",
    "        * <a href=\"https://www.dji.com/au/ground-station-pro\" target=\"_blank\">DJI GS Pro</a>\n",
    "        * <a href=\"https://www.pix4d.com/product/pix4dcapture/\" target=\"_blank\">Pix4D Capture</a>\n",
    "        * <a href=\"https://www.dronedeploy.com/product/mobile/\" target=\"_blank\">Drone Deploy</a>\n",
    "<p></p>\n",
    "\n",
    "5. **Thinking about calibration**\n",
    "    * ***Geometric Calibration***\n",
    "        * ***Inertial Measurement Unit (IMU)***: IMU measures force, angular velocity, and attitude (accelerometer, barometer, gyroscope, and thermometer). The IMU keeps the UAV flying straight on the path outlined in your mission planning.  \n",
    "        * ***Ground Control Points (GCP)***: GCPs are known locations in the imagery that are surveyed accurately and used to georeference the imagery. Identifiable objects are spaced across the flight area and surveyed with an RTK GPS. Some GCPs are commonly left out of image processing and used as checkpoints to verify the accuracy of the final orthophoto. \n",
    "        * ***Real-Time Kinematic (RTK) Drones***: RTK drone survey use real-time correction with location data from a GPS while the drone is capturing imagery. As these platforms reduce in price and depending on the accuracy required from the final product, it may remove the need for a lot of GCPs. \n",
    "\n",
    "    * ***Radiometric Calibration***\n",
    "        * Multispectral imaging sensors do not directly measure reflectance. Instead, they measure at-sensor radiance, the radiant flux received by the sensor. Each image is a matrix of digital numbers relative to flight conditions. Radiometric calibration is required to convert raw digital numbers recorded by the camera into surface reflectance\n",
    "        * ***Calibration Panels***: Panels that are measured with a spectrometer capture their reflectance values- and are placed within the flight area. Panels are commonly made from masonite board with a rough surface (Lambertian reflectance) and painted in scale from white to black (contrasting albedo). It is common for science grade data to use at least four pannels. \n",
    "        * ***Incident light sensor***: This sensor can also be called the Downwelling Light Sensor (DLS). It can be mounted on top of a multispectral camera to record lighting conditions throughout the flight and writes this data to each image's metadata. This data can be used to improve the accuracy of calibration.   \n",
    "        * ***Empirical Line Calibration***: The empirical line method is widely used to calibrate remotely sensed images to surface reflectance. Reference targets (calibration panels) of known reflectance are placed in the image scene. The raw digital number for each panel is extracted from the image during preprocessing. Linear regression is applied between the panel's known surface reflectance and the digital number to build a calibration equation for each sensor band. The calibration equation is applied to each image band to get surface reflectance values. Note-this method assumes a linear relationship between surface reflectance and at-sensor radiance which may not always be the case.\n",
    "<p></p>\n",
    "\n",
    "![](https://cdnsciencepub.com/cms/10.1139/juvs-2018-0018/asset/images/large/juvs-2018-0018f1.jpeg)\n",
    "\n",
    "Source: <a href=\"https://doi.org/10.1139/juvs-2018-0018\" target=\"_blank\">Vegetation monitoring using multispectral sensors — best practices and lessons learned from high latitudes - Assmann et al. (2018)</a>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "6. **Capturing the Images**\n",
    "    * On the day you are flying, you should. Check the weather to ensure it is within UAV limitations and won't adversely impact your data. If the light condition changes a lot across flights (i.e. clouds), rescheduling may be better. It is much easier to get good data than to try and correct for variance across the flight. \n",
    "\n",
    "## Background Reading\n",
    "\n",
    "<a href=\"https://cdnsciencepub.com/doi/10.1139/juvs-2018-0018\" target=\"_blank\">Vegetation monitoring using multispectral sensors — best practices and lessons learned from high latitudes - Assmann et al. (2018)</a>\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Why do you think calibration is required? What factors will affect that data? When does calibration matter?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "Weather changes, sun angle. Calibration is essential to compare changes in spectral indices across days or seasons, e.g. if we wish to monitor crop growth or health over time.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple radiometric correction of Swan Valley imagery\n",
    "\n",
    "You will use the skills you learnt across weeks 1-6 (working with GeoTIFF files, raster geoprocessing operations, band stacking, navigating directories and file systems with imagery stored different folders etc.). \n",
    "\n",
    "Our post-processing will focus on the radiometric calibration of UAV imagery. This allows us to produce comparable images and vegetation indices across flights and days. Radiometric calibration is the process of converting digital numbers captured by the camera sensor to physical units such as surface reflectance (the ratio of incoming solar radiation to reflected light across different spectral wavebands). \n",
    "\n",
    "**Why do want to convert our image data to spectral reflectance values?**\n",
    "\n",
    "**The key objective for this week is to show the importance of calibration when working with UAV imagery**. The satellite data we have worked with so far across weeks 1-6 has already had radiometric, spectral and geometric calibration applied by the data providers. For example, you can view the <a href=\"https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/document-library\" target=\"_blank\">Sentinel data quality report</a>.\n",
    "\n",
    "This notebook is focused on simple radiometric calibration of 5-band multispectral imagery. The imagery you will be using was collected over the Swan Valley a by MicaSense RedEdge-MX camera mounted on a DJI Phantom 4. For this lab, the imagery has been clipped to a small subsection containing some grapevines and the calibration panels to speed up processing.\n",
    "\n",
    "Imagery bands:\n",
    "\n",
    "* 1: Blue  \n",
    "* 2: Greeb \n",
    "* 3: Red \n",
    "* 4: Red Edge \n",
    "* 5: Near Infrared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will provided with:\n",
    "\n",
    "1. A GeoTIFF file for each camera band.\n",
    "2. A shapefile of the calibration panel's extent.\n",
    "3. A CSV file that contains the MicaSense RedEdge-MX band parmeters (you would normally need to look up wavelength centre and band width for each of the sensor bands in the camera specificaions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install xarray[complete]\n",
    "    !pip install rioxarray\n",
    "    !pip install mapclassify\n",
    "    !pip install rasterio\n",
    "    !pip install rasterstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import fiona\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "# setup renderer\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration \n",
    "\n",
    "Before getting started, let's check the data files for this exercise have downloaded without error. You should be able to identify what data is stored in each file by the file's name and extension. \n",
    "\n",
    "Let's also visualise the near infrared and red images captured by the MicaSense RedEdge-MX camera.\n",
    "\n",
    "**Can you spot the calibration panels in the images?**\n",
    "\n",
    "**Can you explain why the vines appear bright in the near infrared image and dark in the red image?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_week7_data = os.path.join(os.getcwd(), \"data_lab-7\")\n",
    "os.listdir(path_to_week7_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise NIR data\n",
    "swan_nir_path = os.path.join(path_to_week7_data, \"swan_nir.tif\")\n",
    "\n",
    "nir = rxr.open_rasterio(swan_nir_path)\n",
    "nir.sel(band=1).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise RED data\n",
    "swan_red_path = os.path.join(path_to_week7_data, \"swan_red.tif\")\n",
    "\n",
    "red = rxr.open_rasterio(swan_red_path)\n",
    "red.sel(band=1).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Read in the calibration panel data stored in the shapefile `calibration_panels.shp`. Reference this data with a variable called `panel_gdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "#Import shape file for panels\n",
    "panel_path = os.path.join(os.getcwd(), \"data_lab-7\", \"calibration_panels.shp\")\n",
    "panel_gdf = gpd.read_file(panel_path)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on we need to reverse the ordering of the data in the `panel` column so it has a black-to-white ordering. Let's do this and also inspect the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse b-w instead w-b \n",
    "panel_gdf = panel_gdf.copy().iloc[::-1]\n",
    "panel_gdf = panel_gdf.reset_index(drop=True)\n",
    "panel_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Read in the asd data (measured spectra of the calibration panels). This data is stored in the CSV file `panels_reflectance_values.csv` and you should read it into a pandas `DataFrame` referenced by the variable `asd`. An ASD spectroradiometer is used to accurately measure the spectral reflectance of each of the calibration panels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "    \n",
    "```python\n",
    "#import asd data\n",
    "asd_fp = os.path.join(os.getcwd(), \"data_lab-7\", \"panels_reflectance_values.csv\")\n",
    "asd = pd.read_csv(asd_fp)\n",
    "asd.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of asd data\n",
    "asd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Read in camera band values for the MicaSense RedEdge-MX band parmeters. This data is stored in the file `rededgemx_bands.csv`. Read this data into a pandas `DataFrame` referenced by the variable `mica_mx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    " \n",
    "```python\n",
    "#read in camera band values\n",
    "mica_mx_fp = os.path.join(os.getcwd(), \"data_lab-7\", \"rededgemx_bands.csv\")\n",
    "mica_mx = pd.read_csv(mica_mx_fp)\n",
    "mica_mx.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Retrieve the known spectral reflectance values of the calibration panels for each of the MicaSense RedEdge-MX camera bands. There are a few steps to this task. So, take a few minutes to work through what is happenning in the below code snippet. We loop over each spectral waveband in the MicaSense RedEdge-MX camera band:\n",
    "\n",
    "1. subset the upper and lower spectral reflectance values for each band.\n",
    "2. subset the asd measured spectral reflectance of the calibration panels within the wavelength range corresponding to each of the MicaSense RedEdge-MX camera bands.\n",
    "3. compute the mean of the asd measured spectral reflectance of calibration panels within the wavelength range corresponding to each of the MicaSense RedEdge-MX camera bands.\n",
    "\n",
    "We assume that the asd measured spectral reflectance for each of the calibration panels is accurate and we implement these steps to compute what the asd measured spectral reflectance is for each of the MicaSense RedEdge-MX camera bands. Thus, we expect our UAV imagery's spectral reflectance values over the calibration panels to match the asd data. If there are discrepancies, we adjust the UAV imagery so it matches the asd measured spectral reflectance. This is radiometric image calibration.\n",
    "\n",
    "Remember you can use `print()` statements to help visualise what your code is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    " \n",
    "```python\n",
    "asd_panels = []\n",
    "\n",
    "for i in mica_mx[\"band\"]:\n",
    "    print(f\"processing MicaSense RedEdge-MX camera band: {i}\")\n",
    "    # For each band, subset the lower and upper band reflectance from the DataFrame mica_mx\n",
    "    band_lowerlim = pd.Series(mica_mx[mica_mx[\"band\"] == i][\"band_low\"]).item()\n",
    "    band_upperlim = pd.Series(mica_mx[mica_mx[\"band\"] == i][\"band_up\"]).item()\n",
    "    \n",
    "    # get asd values between the mica_mx band limits\n",
    "    mx_asd = asd[(asd[\"Wavelength\"] >= band_lowerlim) & (asd[\"Wavelength\"] <= band_upperlim)]\n",
    "    \n",
    "    # compute the mean of asd values between mica_mx band limits\n",
    "    mx_asd_panel_mean = mx_asd.mean() \n",
    "    asd_panels.append(mx_asd_panel_mean.iloc[1:].values)\n",
    "\n",
    "surface_reflectance = pd.DataFrame(asd_panels, columns=panel_gdf[\"panel\"])   \n",
    "surface_reflectance.index= mica_mx[\"band\"]\n",
    "\n",
    "surface_reflectance.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Extract the mean digital number for the extent of each calibration panel in the UAV imagery for each camera band. You will need to use the locations of the calibration panels that you read into your program in *Task 1* and are stored as polygons in the `GeoDataFrame` `panel_gdf`. You will need to use these polygons in a zonal statistics operation for each of the UAV imagery's camera bands. Each band is stored in a separate GeoTIFF file. \n",
    "\n",
    "You can use the `zonal_stats()` function from the rasterstats package for this task. \n",
    "\n",
    "Try and use a for loop to iterate over the GeoTIFF files and apply the `zonal_stats()` function. This is a good example of how we can use a for loop to automate a repetitive task. Store the mean band digital numbers for the extent of each calibration panel in the UAV imagery as a pandas `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    " \n",
    "```python\n",
    "dn_panels = []\n",
    "\n",
    "#extract digital number mean for each pannel \n",
    "for i in mica_mx[\"band\"]:\n",
    "    print(f\"zonal stats for MicaSense RedEdge-MX camera band: {i}\")\n",
    "    \n",
    "    # update path to GeoTIFF file for each camera band\n",
    "    mg_fp= os.path.join(os.getcwd(), \"data_lab-7\", f\"swan_{i}.tif\")\n",
    "    \n",
    "    # use the zonal_stats function to compute mean digital number values for each of the calibration panels\n",
    "    zs = zonal_stats(panel_gdf, mg_fp, stats=\"mean\")\n",
    "    \n",
    "    zs = pd.DataFrame(zs)\n",
    "    band_panel_dn = zs[\"mean\"].values\n",
    "    dn_panels.append(band_panel_dn)\n",
    "\n",
    "mx_dn_panels = pd.DataFrame(dn_panels, columns=panel_gdf[\"panel\"])\n",
    "mx_dn_panels.index= mica_mx[\"band\"]\n",
    "\n",
    "mx_dn_panels.head()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Can you generate scatter plots that visualise the relationship between calibration panel's surface reflectance and their digital numbers in the UAV images? The UAV image digital numbers for the panels are stored in the `DataFrame` `mx_dn_panels` and the calibration panel's surface reflectance is stored in the `DataFrame` `surface_reflectance`. Remember you can use the `head()` function to glimpse at the structure of the data in these `DataFrame`s. Also, remember you can look up reference information for creating scatter plots in the Plotly Express docs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    " \n",
    "```python\n",
    "for i in mica_mx[\"band\"]:\n",
    "    fig= px.scatter(\n",
    "        x=mx_dn_panels.loc[i], \n",
    "        y=surface_reflectance.loc[i], \n",
    "        trendline=\"ols\",\n",
    "        title=i)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Digital Number\",\n",
    "        yaxis_title=\"Surface Reflectance\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 \n",
    "\n",
    "In week 5 you used Scikit-learn to train machine learning models. We need to create a linear regression model that will learn relationships between a calibration panel's surface reflectance (measured by the asd) and the calibration panel's digital number captured by the UAV for each camera band. We can then use this model to correct the UAV images to surface reflectance. Have a go at fitting this linear regression model using Scikit learn and correcting the UAV images. If you get stuck, check the answer and make sure you take time to understand what each step in the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    " \n",
    "```python\n",
    "# create a list to save corrected outputs\n",
    "corrected_images = []\n",
    "\n",
    "for i in mica_mx[\"band\"]: \n",
    "    print(f\"\")\n",
    "    print(f\"correcting band {i}\")\n",
    "    \n",
    "    x = mx_dn_panels.loc[i].values.reshape(-1,1)\n",
    "    y = surface_reflectance.loc[i].values.reshape(-1,1)\n",
    "    \n",
    "    model = LinearRegression().fit(x, y)\n",
    "    #Obtain the coefficient of determination by calling the model with the score() function, then print the coefficient:\n",
    "    r_sq = model.score(x, y)\n",
    "    intercept = model.intercept_[0]\n",
    "    coef = model.coef_[0][0]\n",
    "    print(\"coefficient of determination:\", r_sq)\n",
    "    #Print the Intercept:\n",
    "    print(\"intercept:\", intercept)\n",
    "    #Print the Slope:\n",
    "    print(\"slope:\", coef)\n",
    "\n",
    "    #read in raw dn image and apply correction and save\n",
    "    mg_fp= os.path.join(os.getcwd(), \"data_lab-7\", f\"swan_{i}.tif\")\n",
    "    band = rxr.open_rasterio(mg_fp).sel(band=1).values\n",
    "    print(band)\n",
    "        \n",
    "    # use the model to radiometrically correct UAV image\n",
    "    band_corrected = coef*band + intercept\n",
    "    \n",
    "    # visualise outputs\n",
    "    fig_dn = px.imshow(band, title = f\"DN band {i}\")\n",
    "    fig_dn.show()\n",
    "    \n",
    "    fig_corr = px.imshow(band_corrected, title = f\"corrected band {i}\")\n",
    "    fig_corr.show()\n",
    "    \n",
    "    # store corrected images as a list\n",
    "    corrected_images.append(band_corrected)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "Let's check an RGB image of the vineyard looks sensible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an rgb image of the corrected data\n",
    "red_band = corrected_images[2]\n",
    "green_band = corrected_images[1]\n",
    "blue_band = corrected_images[0]\n",
    "\n",
    "# make RGB image\n",
    "rgb = np.stack((red_band, green_band, blue_band), axis=0)\n",
    "\n",
    "# plot the rgb image\n",
    "px.imshow(np.moveaxis(rgb, 0, 2), contrast_rescaling=\"minmax\", height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "\n",
    "Create a vegetation index (e.g NDVI) from your corrected data. NumPy `ndarray` objects storing the corrected data are elements in the list `corrected_images` at the following index locations:\n",
    "\n",
    "0. blue\n",
    "1. green\n",
    "2. red\n",
    "3. red edge\n",
    "4. near infrared\n",
    "\n",
    "You can see examples of computing the NDVI from NumPy `ndarray`s in *lab-4.ipynb*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    " \n",
    "This one's on you... you've got it!\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "7ed7e0716c69a9a10d96b6dc385914e15f3509e467d87d4a6c9c4ad11571f27f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
