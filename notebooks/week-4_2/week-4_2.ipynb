{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e90f97c6-e8a5-480c-b14d-4cc8f64803b7",
   "metadata": {},
   "source": [
    "# Data wrangling: vector and tabular data\n",
    "\n",
    "Often, datasets need to go through a series of data wrangling and transformation steps before they are ready for analysis or visualisation tasks. This lab will demonstrate several data wrangling and transformation operations focussing on vector and tabular data. \n",
    "\n",
    "We will work with a subset of the AgriFieldNet Competition Dataset <a href=\"https://mlhub.earth/data/ref_agrifieldnet_competition_v1\" target=\"_blank\">(Radiant Earth Foundation and IDinsight, 2022)</a> which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images. This dataset consists of a series of directories with each directory corresponding to a 256 x 256 pixel Sentinel-2 satellite image footprint and information about field boundaries and crop types within each image. \n",
    "\n",
    "This data is subset from a <a href=\"https://beta.source.coop/radiantearth/agrifieldnet-competition/\" target=\"_blank\">larger dataset</a> covering agricultural fields in four Indian states: Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsight's Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation. \n",
    "\n",
    "Often, datasets for machine learning computer vision tasks (e.g. see the datasets on Radiant Earth's <a href=\"https://beta.source.coop/radiantearth/agrifieldnet-competition/\" target=\"_blank\">Source Cooperative</a>) are provided with data samples for model development spread across many sub-directories. Prior to model training you need to extract the data from these directories and assemble it in a way that it can be passed into a model. The process of transforming data to a format ready for machine learning model development is called **feature engineering**. This lab will demonstrate how to convert the image format data into a vector-tabular dataset where each row corresponds to a field with columns for spectral reflectance measured by the Sentinel-2 sensor and a label for the crop type of the field. This dataset can then be used for machine learning tasks to predict what crop is growing in a field using satellite images, specifically relating patterns of spectral reflectance to a crop type. In this lab you will use the following data wrangling operations:\n",
    "\n",
    "* *zonal statistics* using raster and vector data to compute mean Sentinel-2 spectral reflectance within each field boundary.\n",
    "* *combining datasets* by stacking columns and rows to create larger tables and implementing spatial and non-spatial joins.\n",
    "* *subsetting operations* to extract data from tabular `DataFrame`s.\n",
    "* *geometry operations* to reproject and change the shape of geometries representing field locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691d67c-7a5e-4b32-92fe-b2d8eef9e78b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. **If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/geog3300-agri3003/coursebook/blob/main/docs/notebooks/week-4_2.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12657fbb-43a9-49cb-b120-a50dc9458113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data_lab-4_2\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_2.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data_lab-4_2.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data_lab-4_2\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data_lab-4_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04617f-b032-4968-8d73-c2e197e202b1",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "\n",
    "If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8d9ec-a797-4905-be88-46a69aad9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install xarray[complete]\n",
    "    !pip install rioxarray\n",
    "    !pip install mapclassify\n",
    "    !pip install rasterio\n",
    "    !pip install rasterstats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccbb80a-7b18-4028-bfb9-497a48e1ea07",
   "metadata": {},
   "source": [
    "## Quick recap: What is data wrangling?\n",
    "\n",
    "<a href=\"https://r4ds.had.co.nz/wrangle-intro.html\" target=\"_blank\">Wickham and Grolemund (2017)</a> and <a href=\"https://wesmckinney.com/book/\" target=\"_blank\">McKinney (2022)</a> state that data wrangling consists of data import, data cleaning, and data transformation. \n",
    "\n",
    "#### Data import\n",
    "\n",
    "Data import was covered in week 3 with examples of how to read tabular, vector, and raster data into Python programs. \n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "Data cleaning includes handling outliers and missing data. Here, we'll cloud mask Sentinel-2 remote sensing images, which is a data cleaning exercise. \n",
    "\n",
    "#### Data transformation\n",
    "\n",
    "<a href=\"https://wesmckinney.com/book/\" target=\"_blank\">McKinney (2022)</a> define data transformation as mathematical or statistical operations applied to data to generate new datasets. Data transformation can also include operations that reshape datasets or combine two or more datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28666257-ed70-4d82-bb93-f3279c707b1b",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6aa15-7da0-4356-acdd-e6a236ab4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "# setup renderer\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd0e1c-9f6a-4210-bf76-27508a22e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data_lab-4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c06d95-0245-418e-bc0a-a71a1f1c7735",
   "metadata": {},
   "source": [
    "### Lab data\n",
    "\n",
    "Let's quickly inspect the files that we have downloaded for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe500f0a-06a8-4fd6-9dca-ea1f682d8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b7ced-f51f-4b69-8764-2c79bcb91385",
   "metadata": {},
   "source": [
    "* *agrifieldnet_field_ids.parquet* is a geospatial file storing the field boundaries as a polygon geometry and an integer field id.\n",
    "* *raster_labels.tif* is a GeoTIFF file where each pixel value indicates what crop was grown in that pixel.\n",
    "* *s2_reflectance_agrifieldnet_competition_v1_source_0a664.tif* is a multispectral <a href=\"https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED\" target=\"_blank\">Sentinel-2 image covering the extent of the fields.\n",
    "\n",
    "Let's start by exploring the Sentinel-2 image data. If we visualise it as an RGB image it should look like it covers and agricultural region.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c430c-0b40-435b-9e8a-b84b25811ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_path = os.path.join(data_path, \"s2_reflectance_agrifieldnet_competition_v1_source_0a664.tif\")\n",
    "s2 = rxr.open_rasterio(s2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815118b7-463e-401a-a7d1-ee2fbe1640bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46eaa5-c86d-44c4-a236-d25c935596a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.sel(band=[4, 3, 2]).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab111d29-7eac-4127-98ff-1c71dae85674",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Why have we used the `sel()` method to select the bands 4, 3, and 2 in that order?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "Bands 4, 3, and 2 correspond to reflectance in the red, green, and blue wavelengths. Subsetting these bands allows us to render the image as an RGB true colour composite on our display. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c687d77-1877-43a9-9835-b866e9df91a6",
   "metadata": {},
   "source": [
    "Next, let's explore data in the `raster_labels.tif` file. This should be a GeoTIFF file storing raster data with the same projection system and x and y dimensions as the satellite image. Each pixel is assigned a numeric value that corresponds to a crop type. Based on the dataset's documentation, this is the mapping between numeric values and crop types in the labels dataset. \n",
    "\n",
    "* 1 - Wheat\n",
    "* 2 - Mustard\n",
    "* 3 - Lentil\n",
    "* 4 - No crop/Fallow\n",
    "* 5 - Green pea\n",
    "* 6 - Sugarcane\n",
    "* 8 - Garlic\n",
    "* 9 - Maize\n",
    "* 13 - Gram\n",
    "* 14 - Coriander\n",
    "* 15 - Potato\n",
    "* 16 - Bersem\n",
    "* 36 - Rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37762cfb-a493-4870-b508-21a5a1e88432",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = os.path.join(data_path, \"raster_labels.tif\")\n",
    "labels = rxr.open_rasterio(labels_path)\n",
    "labels.sel(band=1).plot.imshow(cmap=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d769bd3-54a4-4777-b312-48e23b8f1789",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you check that the `xarray.DataArray` object storing the crop type labels has the same coordinate reference system (CRS) and X and Y dimensions as the `xarray.DataArray` object storing the satellite image?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a18444-1f57-443b-a3dd-bd4bb5894114",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d71ed9-3bd1-4d1d-95d0-2cd7470f09cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "print(f\"The shape of the s2 array is {s2.shape}\")\n",
    "print(f\"The shape of the crop type labels array is {labels.shape}\")\n",
    "print(\"\")\n",
    "print(f\"The CRS of the s2 array is {s2.rio.crs}\")\n",
    "print(f\"The shape of the crop type labels array is {labels.rio.crs}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf34923-1251-463c-bbbc-85625ccf0a7c",
   "metadata": {},
   "source": [
    "Finally, let's read in the field boundaries and display them on a web map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8136f417-2d5d-4c63-a16e-713e5801523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_field_ids = gpd.read_parquet(os.path.join(data_path, \"agrifieldnet_field_ids.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a3537-f0df-4f3b-9cb7-55086dfc8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_field_ids.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6b3c99-cb89-4f8f-9f74-5aa8f4adac6c",
   "metadata": {},
   "source": [
    "## Zonal statistics\n",
    "\n",
    "We can use the `zonal_stats()` function from the <a href=\"https://pythonhosted.org/rasterstats/manual.html#statistics\" target=\"_blank\">rasterstats package</a> to perform zonal statistics in Python. Zonal statistics summarise raster pixel values within zones. Here, we're going to compute the average spectral reflectance values recorded in our Sentinel-2 image dataset for each of the fields that we have a crop type label for. \n",
    "\n",
    "The `zonal_stats()` function takes in vector data of geometries (zones) that summary statistics are computed for as its first argument (or a file path to a vector file), the path to a raster dataset (i.e. a GeoTIFF file) as its second argument, a band argument that specifies which band of the raster data to compute zonal statistics for, and optionally a list of statistics to compute to the `stats` argument. You can find the full list of parameters for the `zonal_stats()` function in the package documentation.\n",
    "\n",
    "**Can you look up what the `all_touched` parameter of the `zonal_stats()` function is used for in the <a href=\"https://pythonhosted.org/rasterstats/manual.html#rasterization-strategy\" target=\"_blank\">rasterstats docs</a>?**\n",
    "\n",
    "Let's write a small routine that will loop over each band in our `xarray.DataArray` object `s2`, compute the zonal statistics for that band, and convert the result to a pandas `DataFrame` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb0e2c-cc84-4712-8560-b508fa4cab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zstats_s2_tmp = []\n",
    "s2_bands = list(range(1, 13))\n",
    "\n",
    "for b in s2_bands:\n",
    "    zstats = zonal_stats(gdf_field_ids, s2_path, stats=[\"mean\"], band=b)\n",
    "    zstats_s2_tmp.append(pd.DataFrame(zstats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c077a3-2fd3-4082-8d8f-f83fe6b880a3",
   "metadata": {},
   "source": [
    "`zstats_tmp` is a list of pandas `DataFrame`s. Each `DataFrame` stores the mean spectral reflectance values for the fields for separate spectral bands. Let's look at the zonal statistics results for the first band. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84b70fb-2d6e-4490-b89a-12169a805d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zstats_s2_tmp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033e798-d4f5-486f-b328-31f381e0db1e",
   "metadata": {},
   "source": [
    "The zonal statistics operation has returned `None` for every field. \n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Can you explore the `xarray.DataArray` object `s2` and the `GeoDataFrame` object `gdf_field_ids` to see why our zonal statistics operation is returning `None`?**\n",
    "\n",
    "**Tip: for the zonal statistics operation to work, the raster and vector data need to be aligned in space. Consider checking the CRS of the datasets, and, if necessary changing the CRS of the `GeoDataFrame` `gdf_field_ids` to see if that helps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc4280-58aa-43c0-aa4d-ec2949198cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e39fc0-7900-491a-8646-e1e44fee5240",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "For the zonal statistics operation to work, you will need to align and reproject the vector data to match the raster dataset. We can do this by calling the <code>to_crs()</code> method on a <code>GeoDataFrame</code> and passing in the CRS of the raster dataset. This is a geometry transformation operation.\n",
    "\n",
    "```python\n",
    "# get the CRS for the s2 DataArray\n",
    "s2_crs = s2.rio.crs\n",
    "\n",
    "zstats_s2_tmp = []\n",
    "s2_bands = list(range(1, 13))\n",
    "\n",
    "## Look at how we call to_crs() on gdf_field_ids\n",
    "for b in s2_bands:\n",
    "    zstats = zonal_stats(gdf_field_ids.to_crs(s2_crs), s2_path, stats=[\"mean\"], band=b)\n",
    "    zstats_s2_tmp.append(pd.DataFrame(zstats))\n",
    "\n",
    "zstats_s2_tmp[0]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb1a81-0096-45f8-8ec9-2dcfc3c71227",
   "metadata": {},
   "source": [
    "We also need to convert the crop type labels into a tabular dataset where the crop type of each field is recorded. We can do this using a zonal statistics operation, but we do not want to use the mean as our summarising function. \n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Can you compute the zonal statistics for each field using the crop type labels GeoTIFF file referenced by `labels_path` and using the majority function to summarise the raster data within zones?**\n",
    "\n",
    "**Refer to the <a href=\"https://pythonhosted.org/rasterstats/manual.html#zonal-statistics\" target=\"_blank\">rasterstats</a> docs for information on selecting the summarising function for the zonal statistics operation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2fc6f-5129-404a-9fb3-851e67c700e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7feaf4e-1025-4734-a772-4978094af838",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "zstats = zonal_stats(gdf_field_ids.to_crs(s2.rio.crs), labels_path, band=1, stats=[\"majority\"])\n",
    "labels_df = pd.DataFrame(zstats)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed6dc1-3fff-44b6-8e3f-bcc36767cbec",
   "metadata": {},
   "source": [
    "## Combining datasets\n",
    "\n",
    "Often, we have to deal with datasets that are spread across files or objects in our programs and we need to <a href=\"https://wesmckinney.com/book/data-wrangling#prep_merge_join\" target=\"_blank\">combine</a> them into one dataset for analysis. There are two common techniques for combining datasets: concatenating (stacking) or joining.\n",
    "\n",
    "Concatenating or stacking datasets is the process of appending data items (e.g. adding new columns or rows to an existing `DataFrame`). \n",
    "\n",
    "Joins refer to combining datasets based on the relationships between the two dataset. Key-based / tabular joins combine datasets by matching on common variables. Spatial joins combine datasets by matching datasets based on feature's relationships in space. \n",
    "\n",
    "### Concatenating / stacking datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d149d78-b05d-4415-8ccd-573c2bac5fb9",
   "metadata": {},
   "source": [
    "The variable `zstats_s2_tmp` should reference a list of `DataFrame` objects, where each `DataFrame` stores the zonal statistics results for a spectral band. However, we need to combine the separate `DataFrame` objects into one `DataFrame`. We can do this by concatenating (stacking) the `DataFrame`'s along an axis. Let's inspect the first `DataFrame` in `zstats_s2_tmp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f9910-444e-4681-b187-4061277712a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zstats_s2_tmp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4813c6-85ec-46af-af2d-a7ca3c462b5f",
   "metadata": {},
   "source": [
    "We can see that there is a single column of data which represents the mean spectral reflectance values for band 1 for each of the nine fields. We can combine this data with the `GeoDataFrame` storing the field id and boundary geometry using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html#pandas.concat\" target=\"_blank\">pandas `concat()` function</a>. `concat()` takes a list of `DataFrame`s as an argument and an axis to stack the data items along; here, we want to append columns so we stack along the second axis (axis 1 - remember Python indexes from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3736ba-e716-4b1a-b30e-3c888c5cac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's concatenate all the zonal statistics results for the Sentinel-2 bands\n",
    "s2_df = pd.concat(zstats_s2_tmp, axis=1, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794185b-b703-4118-ac1c-ad981497af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, let's concatenate the zonal statistics results for the Sentinel-2 bands with the field id and boundaries\n",
    "gdf_s2 = pd.concat([gdf_field_ids, s2_df], axis=1, ignore_index=True)\n",
    "gdf_s2.columns = [\"field_id\", \"geometry\"] + [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"] \n",
    "gdf_s2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4a47b-6d8c-4af9-ba54-561f504ba296",
   "metadata": {},
   "source": [
    "You will note that we updated the column names by assigning a list to the `columns` attribute of the `gdf_s2` `GeoDataFrame`.\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**Can you combine the `labels_df` `DataFrame` with the `gdf_field_ids` `GeoDataFrame` using the pandas `concat()` function? Assign the result to the variable `gdf_labels`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0b7ed-11b4-4053-93b3-4f531cb78250",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0744b-6b89-4811-b45b-fc8ef80d8729",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "gdf_labels = pd.concat([gdf_field_ids, labels_df], axis=1, ignore_index=True)\n",
    "gdf_labels.columns = [\"field_id\", \"geometry\"] + [\"crop_label\"]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ca6b4-127c-449c-a1d1-79587d01f5a0",
   "metadata": {},
   "source": [
    "### Key-based joins\n",
    "\n",
    "We now have two separate `GeoDataFrame` objects in our program. We have a `GeoDataFrame` storing average spectral reflectance values for each field referenced by `gdf_s2` and a `GeoDataFrame` storing the crop type label for each field in `gdf_labels`. There is a matching column in both of these `GeoDataFrame`s - `field_id`.\n",
    "\n",
    "When two tables have a matching column(s) we can use join operations to merge them. Rows in both tables are matched using common values in the matching column(s) and the joined table has columns from both tables. \n",
    "\n",
    "Joining tables is a common operation in relational databases using SQL and the same operations can be implemented in Pandas using the <a href=\"https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\" target=\"_blank\">`merge()`</a> functions. \n",
    "\n",
    "Some important concepts for join operations:\n",
    "\n",
    "* The columns with values used to match rows are called **keys**.\n",
    "* **one-to-one** joins are where there is exactly one match between rows in the two tables being joined.\n",
    "* **many-to-one** joins are where a row in one table can match one or more rows in another table.\n",
    "* **left joins** keep all rows in the left table and only matching rows in the right table. \n",
    "* **inner joins** keep only matching rows in the left and right tables. \n",
    "\n",
    "The Pandas <a href=\"https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging\" target=\"_blank\">`merge()`</a> docs and <a href=\"https://wesmckinney.com/book/data-wrangling.html#prep_merge_join\" target=\"_blank\">McKinney (2022)</a> provide useful explanations for how join operations work.\n",
    "\n",
    "![](https://github.com/data-analysis-3300-3003/figs/raw/main/week-4-joins.jpg)\n",
    "\n",
    "Let's consider these concepts in the context of joining our `GeoDataFrame` of each field's spectral reflectance values with a `GeoDataFrame` of each field's crop type label. \n",
    "\n",
    "The matching column in both tables is `field_id`. This the joining key. \n",
    "\n",
    "We are joining the two tables on `field_id` which should be unique to each field. Therefore, we are implementing a one-to-one join. We only want to keep fields where there is a crop type value and spectral reflectance values. Therefore, we'll use an inner join.\n",
    "\n",
    "Pandas `merge()` function can take the following arguments:\n",
    "\n",
    "* `left` - left table in the join.\n",
    "* `right` - right table in the join.\n",
    "* `how` - whether to use a left or inner join.\n",
    "* `left_on` - columns in left table to use as keys.\n",
    "* `right_on` - columns in the right table to use as keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8dc591-0861-4177-a449-daa7bbfd9df9",
   "metadata": {},
   "source": [
    "As there is a `geometry` column in both `GeoDataFrame`s, let's drop it from `gdf_s2` to avoid it being duplicated in the joined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a02b2b6-7393-4797-9e16-b52f9eff3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s2 = gdf_s2.drop(columns=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6699f8ef-b945-4626-9e4f-a91031df16db",
   "metadata": {},
   "source": [
    "Review the <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html\" target=\"_blank\">pandas `merge()`</a> documentation to see how we're setting up our join. The `how` argument specifies the type of join (`\"inner\"` means we only want to keep rows in both datasets where there is a match on the joining column) and the column to join on (`\"field_id\"` is the common column in both datasets that we're joining on).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2458f6-60c5-42c4-8863-5aa304c99618",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_joined = gdf_labels.merge(df_s2, how=\"inner\", on=\"field_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48632e75-bfba-4904-af90-ae881a4c6bc4",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you check that our join has been completed correctly? Think about what the `DataFrame` should look like after a completed join and how you can inspect the `DataFrame` to check this.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b833f-ecf4-4536-abf3-94b5790653f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48554108-e741-46ce-9b06-9f2e310c7e36",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "You could print out the columns of the joined dataset to check that we have columns corresponding to the crop type label and the spectral reflectance bands. `GeoDataFrame`s and `DataFrame`s have a `columns` attribute that stores the column names as a `list`. \n",
    "\n",
    "```python\n",
    "print(gdf_joined.columns)\n",
    "```\n",
    "<p></p>\n",
    "\n",
    "You could also print out the first few rows of the dataset to check it looks sensible. The `DataFrame`'s `head()` method does this.\n",
    "\n",
    "```python\n",
    "gdf_joined.columns()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbbbd30-9b8a-48ce-b62e-fb023715e878",
   "metadata": {},
   "source": [
    "## Spatial Joins\n",
    "\n",
    "Spatial join operations join the attributes of two vector layers based on their relationship in space. For example, if we have a `GeoDataFrame` storing field boundaries (polygon geometries) and field attributes and another `GeoDataFrame` storing district boundaries (polygon geometries) and a district name as an attribute, we can join the the two tables based on the largest intersection (overlap) between field boundaries and district boundaries. If the field boundaries `GeoDataFrame` was the left table in the spatial join, for each row (or geometry feature) the district name from the district with largest intersection would be joined to that table in a new column. \n",
    "\n",
    "GeoPandas provides an <a href=\"https://geopandas.org/en/stable/docs/user_guide/mergingdata.html#spatial-joins\" target=\"_blank\">`sjoin()` function</a> that can be used for spatial joins of two `GeoDataFrames`. The `sjoin()` function expects the following as arguments:\n",
    "\n",
    "* `left_df` - left `GeoDataFrame` in the spatial join.\n",
    "* `right_df` - right `GeoDataFrame` in the spatial join - columns from the `right_df` will be joined to `left_df`.\n",
    "* `how` - whether to use a left, inner, or right join.\n",
    "* `predicate` - a binary predicate that defines the spatial relationship between features in `right_df` and `left_df`. \n",
    "\n",
    "Binary predicates that can be used are:\n",
    "\n",
    "* intersects\n",
    "* contains\n",
    "* crosses\n",
    "* within\n",
    "* touches\n",
    "* overlaps\n",
    "\n",
    "Intersects is the default predicate for spatial joins in GeoPandas. \n",
    "\n",
    "![](https://github.com/data-analysis-3300-3003/figs/raw/main/week-4-spatial-join.jpg)\n",
    "\n",
    "It would be useful to include some information about where in India the fields we're working with are located. To do this can perform a spatial join to combine our field boundaries with the boundaries of administrative units in India, districts in this case.\n",
    "\n",
    "We need to read in district geometries for India obtained from <a href=\"https://www.geoboundaries.org\" target=\"_blank\">geoBoundaries</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52c51b-225e-4239-b0c3-bb89589647c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-4_2\", \"geoBoundaries-IND-ADM2_simplified.topojson\"))\n",
    "india_districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a386a69-5855-4c1a-91cd-fcf808f51868",
   "metadata": {},
   "source": [
    "### Subsetting pandas `DataFrame`s\n",
    "\n",
    "To <a href=\"https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html#how-do-i-select-a-subset-of-a-dataframe\" target=\"_blank\">subset data from a pandas `DataFrame`</a> we use the square brackets `[]` to specify the data values we'd like to extract. This is an example of non-spatial subsetting.\n",
    "\n",
    "**Selection by labels**\n",
    "\n",
    "*Selection by labels* refers to selecting values from a `DataFrame` by their label (i.e. column name).\n",
    "\n",
    "* To select a column from a `DataFrame` we pass the column name into `[]` (e.g. `india_districts = india_districts[\"shapeName\"]` where `shapeName` is a `Series` object).\n",
    "* To select many columns from a `DataFrame` we pass a list of column names into `[]` (e.g. (e.g. `india_districts = india_districts[[\"shapeName\", \"geometry\"]]`).\n",
    "\n",
    "**Selection by position**\n",
    "\n",
    "*Selection by position* refers to selecting values from a `DataFrame` by their index position (i.e. row or column number starting at 0).\n",
    "\n",
    "* To select the $n^{th}$ row pass in `[n-1:n]`. Remember that Python indexes from zero so the $n-1$ index position is the $n^{th}$ row. The slice operator `:` is exclusive so `[n-1:n]` will only select the row at `n-1` (e.g. to select the $2^{nd}$ row use `df_row_2 = gdf[1:2]`).\n",
    "* To select a slice of rows use the slice operator (e.g. to select the first 10 rows use `df_10_rows = gdf[0:10]`).\n",
    "\n",
    "**Selection by condition**\n",
    "\n",
    "*Selection by condition* selects rows that are `True` based on a condition (e.g. selecting all rows with a `shapeArea` greater than 1 - `india_districts_gt1 = india_districts[india_districts[\"shapeArea\"] > 1]`).\n",
    "\n",
    "**`loc[]` and `iloc[]`**\n",
    "\n",
    "The more robust approach to subsetting data from `DataFrame`s is using the <a href=\"https://pandas.pydata.org/docs/user_guide/indexing.html#different-choices-for-indexing\" target=\"_blank\">`loc` and `iloc` methods</a>. \n",
    "\n",
    "* `loc` is used for selecting by labels (e.g. `india_districts = india_districts.loc[:, [\"shapeName\", \"geometry\"]]` - note the `[:, [\"shapeName\", \"geometry\"]]` syntax where `:` means select all rows).\n",
    "* `iloc` is used for selecting by position (e.g. to select the first 10 rows use `df_10_rows = gdf.iloc[0:10, :]`).\n",
    "* To select the first 10 rows and columns we'd use `df_10_rows_10_cols = gdf.iloc[0:10, 0:10]`).\n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "<details>\n",
    "    <summary><b>How many rows will the object referenced by <code>df_temp</code> have after calling <code>df_temp=df.iloc[0:5, :]</code>?</b></summary>\n",
    "5 rows at index positions 0 to 4 from the <code>DataFrame</code> <code>df</code>.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details>\n",
    "    <summary><b>How many columns from <code>df</code> will the object referenced by <code>df_temp</code> have after calling <code>df_temp=df.iloc[0:5, :]</code>?</b></summary>\n",
    "All the columns from <code>df</code>.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "**Use `loc` and the `GeoDataFrame` referenced by `india_districts` to select the `shapeName` and `geometry` columns. Assign the result to the variable `india_districts`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3457c9-cd0a-4681-9463-b8cf5fb26822",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ea3fb-500e-4c44-bab4-7ec962088949",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "india_districts = india_districts.loc[:, [\"shapeName\", \"geometry\"]]\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f327a5-c6bc-4b30-8871-cdcb74ba1014",
   "metadata": {},
   "source": [
    "Next, let's tidy up our subsetted dataset by giving it neat column names and setting the CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d66f3fc-2b57-4501-93a8-8a63ad67b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts.columns = [\"district\", \"geometry\"]\n",
    "india_districts = india_districts.set_crs(\"EPSG:4326\")\n",
    "india_districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25035b1-2bd0-4807-ba10-4413782fbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "india_districts.plot(column=\"district\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de7ce9-52ba-4cb8-a9ce-bf43e7332842",
   "metadata": {},
   "source": [
    "Now we can implement our spatial join. We use the `sjoin()` function from `GeoPandas` and specify the two datasets we are spatially joining. Again, we use an inner join where we keep only rows in the left and right `GeoDataFrame`s where there is a match in space. Here, we determine a match in space where the polygon in the left `GeoDataFrame` intersects with a polygon in the right `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7897cc-fd7f-4312-9985-39efaa51920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_district = gpd.sjoin(\n",
    "    left_df=gdf_joined, \n",
    "    right_df=india_districts, \n",
    "    how=\"inner\", \n",
    "    predicate=\"intersects\"\n",
    ")\n",
    "gdf_district.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b56cee-48d6-493b-910b-3c9ac5d22cda",
   "metadata": {},
   "source": [
    "We now have a clean and tidy tabular dataset ready for analysis (e.g. building machine learning models to classify crop type based on spectral reflectance). \n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**If you were to expand this workflow to be handle a large number of fields across many satellite image scenes, what Python tools might help you automate this process?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "For loops. You could include this data transformation process inside a for loop and iterate over a sequence of satellite images and converting them to tabular datasets. \n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
