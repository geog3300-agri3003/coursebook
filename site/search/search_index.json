{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GEOG3300 and AGRI3003 - Semester 1 2024","text":""},{"location":"#welcome","title":"Welcome","text":"<p>This is the online course book for Advanced Spatial Analysis (3300) and Decisions from Data in Agriculture (3003).</p> <p>There are rapid changes in data science and technology/sensors that are driving change across the environmental and agricultural sectors. This unit focuses on student learning through practical and applied laboratories, using Python, and collecting field data using uncrewed aerial vehicles (UAVs). Students will develop skills to manipulate, transform, analyse, and visualise big spatial and non-spatial data to provide solutions to the big landscape and agricultural questions.</p> <p>This course will teach students foundational skills and knowledge required to implement spatial and non-spatial data analysis workflows programmatically, to handle and analyse \u201cbig\u201d data, and to operate in cloud-based environments.</p> <p>There are three overarching themes are:</p> <ul> <li> <p>Programming: analysis doesn\u2019t scale without code, programming skills are required to use many software packages and tools for spatial data analysis.</p> </li> <li> <p>Big data: various sensors are generating databases that are growing in size and variety. Increasingly specialist software and data formats are required to handle big data.</p> </li> <li> <p>Cloud-based environments: it\u2019s easier to move the question to the data than to make big data mobile. Data and computer processing are often located on remote computers - we need to know how to get there. </p> </li> </ul>"},{"location":"#computer-labs","title":"Computer labs","text":"<p>The material and concepts will be delivered in computer workshops. In the first session, the instructor will introduce the week's content and explain key concepts. Then for the reminder of the first session and the second session students will work through practical examples and practice exercises in a Jupyter notebook environment with support from instructors. </p>"},{"location":"notebooks/week-1_2/","title":"Week 1 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-1_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_1\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-1_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-1_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-1_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-2_1\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-1_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre># Import modules\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> # Import modules import os import pandas as pd import geopandas as gpd import plotly.express as px import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\")\n\n# Get a list of canola yield data\nharvester_data_files = os.listdir(harvester_data_path)\n\n# Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n\ndf = pd.concat(dfs, axis=0)\n\n# Inspect the data frame\ndf.head()\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"data_lab-1_2\")  # Get a list of canola yield data harvester_data_files = os.listdir(harvester_data_path)  # Combine the csv files into one data frame dfs = []  for i in harvester_data_files:     if i.endswith(\".csv\"):         tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))         dfs.append(tmp_df)  df = pd.concat(dfs, axis=0)  # Inspect the data frame df.head() In\u00a0[\u00a0]: Copied! <pre># Transform the yield data to a spatial format\npoints = gpd.points_from_xy(df[\"Longitude\"], df[\"Latitude\"], crs=\"EPSG:4326\")\ngdf = gpd.GeoDataFrame(df, geometry=points)\n\n# Visualise the crop yield data on a web map\nbasemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nattribution = \"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\ngdf.explore(column=\"DryYield\", cmap=\"plasma\", tooltip=\"DryYield\", vmax=2, tiles=basemap, attr=attribution)\n</pre> # Transform the yield data to a spatial format points = gpd.points_from_xy(df[\"Longitude\"], df[\"Latitude\"], crs=\"EPSG:4326\") gdf = gpd.GeoDataFrame(df, geometry=points)  # Visualise the crop yield data on a web map basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\" attribution = \"Tiles \u00a9 Esri \u2014 Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\" gdf.explore(column=\"DryYield\", cmap=\"plasma\", tooltip=\"DryYield\", vmax=2, tiles=basemap, attr=attribution) In\u00a0[\u00a0]: Copied! <pre>print(type(0.227))\n</pre> print(type(0.227)) In\u00a0[\u00a0]: Copied! <pre>print(type(\"a string\"))\nprint(\"a string\")\n</pre> print(type(\"a string\")) print(\"a string\") <p>Let's check how the sample id names in the canola yield dataset are represented. First, let's remind ourselves what the <code>DataFrame</code> of the yield dataset looks like.</p> In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre># get the first field name value\nfield_name = df.loc[:, \"sample_id\"].to_list()[0]\nprint(type(field_name))\nprint(field_name)\n</pre> # get the first field name value field_name = df.loc[:, \"sample_id\"].to_list()[0] print(type(field_name)) print(field_name) In\u00a0[\u00a0]: Copied! <pre>print(type(0.277 &gt; 0.2))\nprint(0.227 &gt; 0.2)\n</pre> print(type(0.277 &gt; 0.2)) print(0.227 &gt; 0.2) <p>We can use the <code>isinstance(value, type)</code> function to test if a data value matches a data type. Let's test if our crop yield value is numeric or string.</p> In\u00a0[\u00a0]: Copied! <pre>print(isinstance(0.227, float))\nprint(isinstance(0.227, str))\n</pre> print(isinstance(0.227, float)) print(isinstance(0.227, str)) In\u00a0[\u00a0]: Copied! <pre>[\"bf66_sample_1\", \"bf66_sample_2\"]\n</pre> [\"bf66_sample_1\", \"bf66_sample_2\"] <p>Lists allow us to store duplicate values. The following is a valid list.</p> In\u00a0[\u00a0]: Copied! <pre>[\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"]\n</pre> [\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"] <p>A key feature of lists are that they can be modified in place. This makes lists useful data structures for tasks when the number of objects that we want to store in a collection can change during our program's execution. We can use functions such as <code>.append()</code> to add objects to the end of a list.</p> In\u00a0[\u00a0]: Copied! <pre>sample_id_list = [\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"]\nsample_id_list.append(\"bf66_sample_3\")\nprint(sample_id_list)\n</pre> sample_id_list = [\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\"] sample_id_list.append(\"bf66_sample_3\") print(sample_id_list) Using lists in our Python program to read in CSV files on harvester crop yield data <p>In our Python program above, we took advantage of the fact that lists can be modified in place to loop over the CSV files in a directory, import them in as pandas <code>DataFrames</code>, and append the <code>DataFrames</code> to the list.</p> <pre># Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n</pre> <p>Here, we start with an empty list <code>dfs</code> denoted by just square brackets <code>[]</code>. Then, successively, a new object is added to <code>dfs</code> using the <code>append()</code> function. We finish with a list storing three pandas <code>DataFrames</code>. This demonstrates how we change the length and contents of a list during the execution of a program. We have gone from a list with zero elements to a list a list with three elements.</p> <p></p> <p>We can store any Python object in a list. We can also mix the types of objects stored in lists. The following is a valid list.</p> In\u00a0[\u00a0]: Copied! <pre>[1, 2, \"not a number\", None]\n</pre> [1, 2, \"not a number\", None] <p>Lists are ordered collections of data. If you add an element to a list it will be appended to the last position. List items can be accessed by their index location with the first element having index <code>0</code>.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_id_list)\nprint(\"The first element in sample_id_list is at index 0: \", sample_id_list[0])\nprint(\"The second element in sample_id_list is at index 1: \", sample_id_list[1])\nprint(\"The third element in sample_id_list is at index 2: \", sample_id_list[2])\n</pre> print(sample_id_list) print(\"The first element in sample_id_list is at index 0: \", sample_id_list[0]) print(\"The second element in sample_id_list is at index 1: \", sample_id_list[1]) print(\"The third element in sample_id_list is at index 2: \", sample_id_list[2]) <p>We can use index locations of elements in a list to create slices of list elements. For example, <code>sample_id_list[0:2]</code> would slice out the first two elements of the list. Note, the element referenced by the index in the final number of the slice is excluded.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_id_list[0:2])\n</pre> print(sample_id_list[0:2]) In\u00a0[\u00a0]: Copied! <pre>(116.804075, -33.889203)\n</pre> (116.804075, -33.889203) <p>Here, we have created a tuple with two numeric objects. Similar to lists we can access tuple elements by their index locations. Note the use of the <code>[]</code> operator to access elements by their index location.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"First tuple element: \", (116.804075, -33.889203)[0])\n</pre> print(\"First tuple element: \", (116.804075, -33.889203)[0]) In\u00a0[\u00a0]: Copied! <pre>print(\"Second tuple element: \", (116.804075, -33.889203)[1])\n</pre> print(\"Second tuple element: \", (116.804075, -33.889203)[1]) <p>As tuples are fixed-length and unchangeable, we cannot append elements to them in the same way we could with lists. This makes them useful data structures for storing data values which we don't want to change. For example, coorindate pairs that describe a location's x and y values (e.g. longitude and latitude) have two elements. Therefore, a tuple could be a suitable data format to store coordinate pairs.</p> <p>The shape of pandas <code>DataFrames</code> is also a tuple. A <code>DataFrame</code> has two dimensions: number of rows and number of columns. Thus, a tuple is a sensible data structure for storing the shape of <code>DataFrame</code> objects.</p> In\u00a0[\u00a0]: Copied! <pre>df.shape\n</pre> df.shape <p>To demonstrate that we cannot change tuple values, let's try and update the number of rows in the tuple storing the shape of <code>df</code>.</p> In\u00a0[\u00a0]: Copied! <pre>df.shape[0] = 5\n</pre> df.shape[0] = 5 <p>We have returned a <code>TypeError</code> informing us the tuple objects do not support item assigment (adding new items to the tuple).</p> <p>Similar to lists, elements of a tuple are ordered and can be duplicated.</p> In\u00a0[\u00a0]: Copied! <pre>{\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\", \"bf66_sample_3\"}\n</pre> {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_2\", \"bf66_sample_3\"} In\u00a0[\u00a0]: Copied! <pre>{1, 2, 3, 3, 4}\n</pre> {1, 2, 3, 3, 4} <p>As sets are not ordered, we cannot access their elements by numeric index locations.</p> In\u00a0[\u00a0]: Copied! <pre># This fails as set objects are not subscriptable\n{\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}[0]\n</pre> # This fails as set objects are not subscriptable {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}[0] <p>We can access set elements by looping over them or checking if a value is in the set.</p> In\u00a0[\u00a0]: Copied! <pre>for i in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}:\n    print(i)\n</pre> for i in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}:     print(i) In\u00a0[\u00a0]: Copied! <pre>print(\"Checking if 'bf66_sample_1' is in the set:\")\nprint(\"bf66_sample_1\" in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"})\n</pre> print(\"Checking if 'bf66_sample_1' is in the set:\") print(\"bf66_sample_1\" in {\"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"}) <p>Similar to tuples, sets are unchangeable (immutable). Once created, we cannot change the set's values in our programs.</p> In\u00a0[\u00a0]: Copied! <pre>{\n\"Elevation(m)\": [213, 222, 214, 254],\n\"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]\n}\n</pre> { \"Elevation(m)\": [213, 222, 214, 254], \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"], \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"] } <p>Keys of dictionary objects cannot be duplicated. For example:</p> In\u00a0[\u00a0]: Copied! <pre>{\n\"Elevation(m)\": [213, 222, 214, 254],\n\"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]\n}\n</pre> { \"Elevation(m)\": [213, 222, 214, 254], \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"], \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"], \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"] } <p>To access elements of a dictionary object we can refer to its key. The following code snippet extracts a list of dates from the dictionary object.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict = {\"Elevation(m)\": [213, 222, 214, 254],\n    \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n    \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]}\ndates = demo_dict[\"Date\"]\ndates\n</pre> demo_dict = {\"Elevation(m)\": [213, 222, 214, 254],     \"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],     \"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]} dates = demo_dict[\"Date\"] dates <p>Dictionary objects have a <code>get()</code> function that we can use to extract elements.</p> In\u00a0[\u00a0]: Copied! <pre>dates = demo_dict.get(\"Date\")\ndates\n</pre> dates = demo_dict.get(\"Date\") dates <p>Dictionary objects also have a <code>keys()</code> function that returns a list of keys.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict.keys()\n</pre> demo_dict.keys() <p>There is also a <code>values()</code> function that we can use to return a list of values.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict.values()\n</pre> demo_dict.values() <p>And, the <code>items()</code> function returns a tuple of key-value pairs.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict.items()\n</pre> demo_dict.items() <p>We can add elements to a dictionary object by providing a new key with corresponding values.</p> In\u00a0[\u00a0]: Copied! <pre>demo_dict[\"yield\"] = [1.45, 2.5, 3, 2.8, 5.5]\ndemo_dict\n</pre> demo_dict[\"yield\"] = [1.45, 2.5, 3, 2.8, 5.5] demo_dict <p>You will notice that the list of yield values we just added to the dictionary has a different number of elements to the other elements in the values slots.</p> <p>Edit the following code snippet to retrieve and print the 3rd element in <code>x</code>.</p> In\u00a0[\u00a0]: Copied! <pre>x = [1, 2, 3, 4]\n# add code here #\n</pre> x = [1, 2, 3, 4] # add code here # answer <pre>print(x[2])\n</pre> <p>Edit the following code snippet to retrieve and print the first element in <code>z</code>.</p> In\u00a0[\u00a0]: Copied! <pre>z = (4, 5)\n# add code here #\n</pre> z = (4, 5) # add code here # answer <pre>print(z[0])\n</pre> <p>What is the data type of the value associated with the <code>field</code> key in the dict <code>z</code>? Retrieve this value from the dict and print its type.</p> In\u00a0[\u00a0]: Copied! <pre>z = {\n\"name\": \"farm 1\",\n\"field\": 439,\n\"crop\": \"canola\"\n}\n# add code here #\n</pre> z = { \"name\": \"farm 1\", \"field\": 439, \"crop\": \"canola\" } # add code here # answer <pre>print(type(z[\"field\"]))\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\")\n</pre> print(\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\") <p>Now, let's assign the data object storing string URL to the name <code>basemap</code>.</p> In\u00a0[\u00a0]: Copied! <pre>basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nprint(basemap)\n</pre> basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\" print(basemap) <p>Calling <code>print()</code> on the variable name returns the data object that the variable name refers to.</p> <p>We can assign any Python objects to variable names. When the variable name is used in a Python statement, the data value which the variable points to is used in the operations.</p> In\u00a0[\u00a0]: Copied! <pre>x = 1\ny = 2\nz = x + y\nprint(z)\n</pre> x = 1 y = 2 z = x + y print(z) <p>Variables make a program's code easier to organise, write, and understand. We could have performed the above addition operation by just writing <code>1 + 2</code>. However, this doesn't provide us with a way to capture the result of that operation and use it again in our program without re-running <code>1 + 2</code>.</p> <p>Variables provide us with a mechanism by which we retrieve and use data objects (that are stored in the computer's memory) at various places in our program. A variable name points to the location in the computer's memory where a data object is stored.</p> <p>While using the result of <code>1 + 2</code> is a trivial example, there are many cases where using variables is important. This statement reads a CSV file into our Python program: <code>tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))</code>. To access the data stored in the CSV file we can use variable name <code>tmp_df</code> which points to where the data in the CSV file was loaded into memory. We don't need to read the CSV file from disk each time we want to access its data.</p> <p>Let's make these concepts concrete.</p> <p>When we assign the string <code>\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"</code> to the variable name <code>basemap</code>, the variable name is a pointer to the string's data object.</p> <p></p> <p>If we create another variable called <code>basemap_1</code> and assign it to <code>basemap</code>, both variables will point to the same data object (the string URL).</p> <p></p> <p>Let's check this.</p> In\u00a0[\u00a0]: Copied! <pre>basemap_1 = basemap\nprint(basemap_1)\n</pre> basemap_1 = basemap print(basemap_1) <p>If we assign a new data object to <code>basemap</code>, <code>basemap_1</code> will still point to the original string data.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>basemap = [1, 2, 3, 4, 5]\nprint(basemap)\nprint(basemap_1)\n</pre> basemap = [1, 2, 3, 4, 5] print(basemap) print(basemap_1) <p>If we assign <code>basemap_1</code> to a new data object the string data for URL will no longer be accessible in our program and will eventually be removed from the computer's memory.</p> In\u00a0[\u00a0]: Copied! <pre>print(\"this statement is executed first\")\nprint(\"and this statement is executed second\")\n</pre> print(\"this statement is executed first\") print(\"and this statement is executed second\") In\u00a0[\u00a0]: Copied! <pre>x = [1, 2, 3]\n\nfor i in x:\n    print(i + 10)\n</pre> x = [1, 2, 3]  for i in x:     print(i + 10) <p>It is important to note that the statements inside the for loop must be indented.</p> <p>Let's refer back to our program to read in crop yield data and see a use of for loops.</p> <pre># Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        print(f\"Loading file {i} into a Pandas DataFrame\")\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n</pre> <p>Here, the for loop is iterating over a list of file paths to CSV files storing crop yield data. For each of the CSV files in the list, the data is read into a variable <code>tmp_df</code> in turn, and, then appended to the list <code>dfs</code>. This is an example of how we can loop over a series of files in a folder on our computer and read their data into our Python program.</p> <p>Let's modify the for loop to illustrate this.</p> In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"week-1\")\n\n# Get a list of canola yield data\nharvester_data_files = os.listdir(harvester_data_path)\n\n# Check we have a list of csv files\nprint(harvester_data_files)\n\n# loop over elements in harvester_data_files\n# i takes on the value of a path to a csv file\nfor i in harvester_data_files: \n    if i.endswith(\".csv\"):\n        print(\" \")\n        print(\"**********************************************************\")\n        print(f\"We are currently loading file {i} into a Pandas DataFrame\")\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        print(tmp_df.head())\n        dfs.append(tmp_df)\n\nprint(\" \")\nprint(f\"We have a list of {len(dfs)} Pandas DataFrames read from csv files\")\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"week-1\")  # Get a list of canola yield data harvester_data_files = os.listdir(harvester_data_path)  # Check we have a list of csv files print(harvester_data_files)  # loop over elements in harvester_data_files # i takes on the value of a path to a csv file for i in harvester_data_files:      if i.endswith(\".csv\"):         print(\" \")         print(\"**********************************************************\")         print(f\"We are currently loading file {i} into a Pandas DataFrame\")         tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))         print(tmp_df.head())         dfs.append(tmp_df)  print(\" \") print(f\"We have a list of {len(dfs)} Pandas DataFrames read from csv files\")    In\u00a0[\u00a0]: Copied! <pre>x = 11\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre> x = 11  if x &gt;= 10:     print(\"x is greater than or equal to 10\") In\u00a0[\u00a0]: Copied! <pre># nothing should be printed\nx = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre> # nothing should be printed x = 9  if x &gt;= 10:     print(\"x is greater than or equal to 10\") <p>Similar to for loops, the statements inside the <code>if</code> block should be indented.</p> <p>We can use <code>else</code> blocks for statements that can be executed if the <code>if</code> statement evaluates to <code>False</code>.</p> <pre>x = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\nelse:\n    print(\"x is less than 10\")\n</pre> In\u00a0[\u00a0]: Copied! <pre>x = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\nelse:\n    print(\"x is less than 10\")\n</pre> x = 9  if x &gt;= 10:     print(\"x is greater than or equal to 10\") else:     print(\"x is less than 10\") <p>There are many uses for <code>if</code> statements in Python programs. An <code>if</code> statement was used when we read in CSV files of harvester data.</p> <pre># Combine the csv files into one data frame\ndfs = []\n\nfor i in harvester_data_files:\n    if i.endswith(\".csv\"):\n        print(f\"Loading file {i} into a Pandas DataFrame\")\n        tmp_df = pd.read_csv(os.path.join(harvester_data_path, i))\n        dfs.append(tmp_df)\n</pre> <p>Here, the <code>if</code> statement is being used to check the file path is referring to a CSV file. Only if this is <code>True</code> does our program try to read the file into a pandas <code>DataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre>s = (1, 2, 3)\n# add code here #\n</pre> s = (1, 2, 3) # add code here # answer <pre>for i in s: \n    print(i - 3)\n</pre> How would you iterate over list of file paths? Using a for loop.  <p>Write a for loop to subtract 3 from every element in the tuple <code>s</code> and append the result to a list <code>q</code>?</p> In\u00a0[\u00a0]: Copied! <pre>q = []\ns = (1, 2, 3)\n# add code here #\n</pre> q = [] s = (1, 2, 3) # add code here # answer <pre>for i in s: \n    q.append(i - 3)\n\nprint(q)\n</pre> <p>Write a loop to subtract 3 from every element in the tuple <code>s</code> and append the result to a list <code>q</code> if the result is less than 0?</p> In\u00a0[\u00a0]: Copied! <pre>q = []\ns = (1, 2, 3)\n# add code here #\n</pre> q = [] s = (1, 2, 3) # add code here # answer <pre>for i in s:\n    if i -3 &lt; 0:\n        q.append(i - 3)\n\nprint(q)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"week-1\", \"canola-yield-df-1.csv\")\n\ndf = pd.read_csv(harvester_data_path)\n\n# check that df is a DataFrame\ntype(df)\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"week-1\", \"canola-yield-df-1.csv\")  df = pd.read_csv(harvester_data_path)  # check that df is a DataFrame type(df) In\u00a0[\u00a0]: Copied! <pre># Get the shape property of the DataFrame\ndf.shape\n</pre> # Get the shape property of the DataFrame df.shape <p>A tuple data structure is used to store the <code>shape</code> property of a <code>DataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Get the values of the DataFrame\ndf.values\n</pre> # Get the values of the DataFrame df.values <p>The <code>DataFrame</code> referenced by <code>df</code> has an <code>info()</code> method. Let's use it.</p> In\u00a0[\u00a0]: Copied! <pre># Load the canola yield data from the harvester\nharvester_data_path = os.path.join(os.getcwd(), \"week-1\", \"canola-yield-df-1.csv\")\n\ndf = pd.read_csv(harvester_data_path)\n\ndf.info()\n</pre> # Load the canola yield data from the harvester harvester_data_path = os.path.join(os.getcwd(), \"week-1\", \"canola-yield-df-1.csv\")  df = pd.read_csv(harvester_data_path)  df.info() <p>The <code>info()</code> method has printed out a summary of the <code>DataFrame</code> for us. A <code>DataFrame</code> also has a <code>mean()</code> method to quickly compute the average of values in a column.</p> In\u00a0[\u00a0]: Copied! <pre>df.mean(numeric_only=True)\n</pre> df.mean(numeric_only=True) <p>You will notice that we passed <code>numeric_only=True</code> when using the <code>mean()</code> method. Head the the mean() documentation to see what this does.</p> In\u00a0[\u00a0]: Copied! <pre>farm_name = \"my farm\"\n# uncomment below to see string methods\n# farm_name.&lt;press_tab&gt;\n</pre> farm_name = \"my farm\" # uncomment below to see string methods # farm_name. In\u00a0[\u00a0]: Copied! <pre>a_list = [1, 2, 3]\n# uncomment below to see list methods\n# a_list.&lt;press_tab&gt;\n</pre> a_list = [1, 2, 3] # uncomment below to see list methods # a_list. In\u00a0[\u00a0]: Copied! <pre>harvester_data_path = os.path.join(os.getcwd(), \"data\", \"week-1\", \"canola-yield-df-1.csv\")\nprint(harvester_data_path)\n</pre> harvester_data_path = os.path.join(os.getcwd(), \"data\", \"week-1\", \"canola-yield-df-1.csv\") print(harvester_data_path) <p>Inside the <code>os.path.join()</code> function you will see that we use the <code>os.getcwd()</code> method from the os module. This returns the current working directory.</p> In\u00a0[\u00a0]: Copied! <pre>os.getcwd()\n</pre> os.getcwd() <p>You will see that we imported pandas as <code>pd</code>. This means we can refer to pandas as <code>pd</code> in our script. For example, to access the <code>read_csv()</code> function we write <code>pd.read_csv()</code> and not <code>pandas.read_csv()</code>. This is just a convenience to make the script less cluttered. The following would also be valid.</p> <pre>import pandas\ndf = pandas.read_csv(\"file.csv\")\n</pre>"},{"location":"notebooks/week-1_2/#python-programs","title":"Python programs\u00b6","text":"<p>This lab will provide an introduction to programming concepts in Python. The concepts this lab will cover are:</p> <ul> <li>data types and structures</li> <li>variables and bindings</li> <li>control flow, loops, and conditional execution</li> <li>classes and objects</li> </ul> <p>This lab will start by running a short Python program that loads some crop yield data collected by harvesters from disk and visualises this data on interactive maps and charts. We will then work through the program exploring how it uses a range of Python programming concepts to complete its task.</p>"},{"location":"notebooks/week-1_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-1_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and, if required, you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-1_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-1_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-1_2/#import-modules","title":"Import modules\u00b6","text":"<p>We'll introduce modules in detail later. But, for now, modules are collections of Python data structures and functions that we can <code>import</code> and use in our program. For example, pandas provides a <code>DataFrame</code> structure for storing tabular data and specific functions for working with tabular data. Plotly Express <code>plotly.express</code> provides functions for generating visualisations, importing Plotly Express means we don't need to write our own code to generate visualisations, we can just use existing functions which makes our lives easier.</p>"},{"location":"notebooks/week-1_2/#a-first-python-program","title":"A first Python program\u00b6","text":"<p>Python is a programming language.</p>"},{"location":"notebooks/week-1_2/#programs","title":"Programs\u00b6","text":"<ul> <li>A program is a series of statements that are executed to complete a task</li> <li>Programs complete tasks by loading, transforming, and visualising data</li> <li>Data in programs can be of different types (numeric, text, boolean)</li> <li>Data can be combined into data structures to represent complex concepts and objects</li> <li>Functions operate on data to complete tasks</li> </ul> Detailed notes on Python programs <p>As stated above, a program is a series of statements that are executed to complete a task. The ultimate goal of the program here is to generate visualisations of crop yield data recorded by harvesters. However, to achieve this the program consists of a series of sub-tasks that include reading the harvester data from CSV files on disk, inspecting the data, transforming it to a spatial data structure, and then rendering web maps and charts.</p> <p>Our program is loading, transforming, and visualising data. This means our program needs to be able store and represent different types and structures of data. If you look in the display of the <code>DataFrame</code> there are numeric, text, and date type data in the columns. Python provides tools for representing different types of data and structures for combining data to represent more complex concepts. For example, the <code>DataFrame</code> that is displayed is a data structure well suited to storing tabular data.</p> <p>In short, a Python program stores data as objects and then uses this data in a range functions that operate on this data to complete tasks.</p> <p></p> <p>Here, a short Python program is demonstrated that:</p> <ol> <li>reads in crop yield data collected by a harvester in Western Australia's Wheatbelt</li> <li>converts the data into a spatial format</li> <li>visualises the data on a web map and charts</li> </ol>"},{"location":"notebooks/week-1_2/#import-modules","title":"Import modules\u00b6","text":"<p>We'll introduce modules in detail later. But, for now, modules are collections of Python tools that we can <code>import</code> and use in our program. For example, pandas provides a <code>DataFrame</code> structure for storing tabular data and specific functions for working with tabular data. Plotly Express <code>plotly.express</code> provides functions for generating visualisations, importing Plotly Express means we don't need to write our own code to generate visualisations, we can just use existing functions which makes our lives easier.</p>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"In a Python program, are data types just used to store observations of scientific variables such as crop yield or temperature? <p>No. In a Python program built-in data types are used to store a range of information relevant to the execution of a program. This could include \"scientific\" data such as an array of crop yield values. However, data in a Python program could refer to file paths and directory structures, URLs to websites or data stored in the cloud, or credentials to log in to databases.</p> <p></p> <p>In this lab we will break down the Python program that generates visualisations of crop yield data recorded by harvesters into its fundamental building blocks, identify how different types of data are represented in Python programs, and demonstrate how we can do things with this data to produce useful outputs.</p>"},{"location":"notebooks/week-1_2/#computational-thinking","title":"Computational thinking\u00b6","text":"<p>A useful skill to develop when writing or analysing Python programs is computational thinking. This refers to breaking a larger task down into small sub-tasks (and possibly breaking sub-tasks into sub-tasks). Often, it is easier to reason with smaller more focused tasks than to grapple with a large complex problem as a whole.</p> <p>A good homework exercise to help build your understanding of Python concepts is to work through the program above and break it down into series of smaller tasks (e.g. find csv files, read in csv data, make data spatial)</p>"},{"location":"notebooks/week-1_2/#statements-and-comments","title":"Statements and comments\u00b6","text":"<p>A Python program consists of statements. A Python statement is a line of Python code which can be executed by the computer.</p> <p>A comment is a line of text that starts with the <code>#</code> symbol. It is not executed by the computer. We can use comments to make notes in our script to help us understand what the program is trying to achieve.</p> <p>For example, this code snippet contains two comments and one statement:</p> <pre># Inspect the yield data format\n# Display the first n rows\ndf.head()\n</pre>"},{"location":"notebooks/week-1_2/#objects","title":"Objects\u00b6","text":"<p>Everything in a Python program is an object. Built-in scalar data types are the most fundamental building blocks of a Python program. We can use these scalar data types to represent single numbers, words, and sentences. This is where we'll start unpicking this Python program.</p>"},{"location":"notebooks/week-1_2/#data-types","title":"Data types\u00b6","text":"<p>Python programs perform operations on data to complete tasks. This can be scientific data such as crop yield or temperature measurements or it can be other forms of data necessary for the program to execute such as a file path to where data is stored on disk.</p>"},{"location":"notebooks/week-1_2/#built-in-data-types","title":"Built-in data types\u00b6","text":"<p>Python comes with built-in data types that can be used to store different values. These are sometimes called scalar types as they store single values.</p> <ul> <li><code>float</code> - storing floating point numeric values.</li> <li><code>int</code> - storing integer numeric values.</li> <li><code>str</code> - storing text data as strings.</li> <li><code>bool</code> - storing <code>True</code> or <code>False</code> as boolean values.</li> <li><code>None</code> - storing null values.</li> <li><code>bytes</code> - storing raw binary data.</li> </ul>"},{"location":"notebooks/week-1_2/#numeric-data-types","title":"Numeric data types\u00b6","text":"<p><code>int</code> and <code>float</code> are numeric data types in Python and are used to store integer and floating point values, respectively.</p> <p>The canola yield data that we have imported is of numeric type with units of tonnes/ha. Let's represent a crop yield measurement of 0.227 tonnes/ha as data in our program and inspect its type.</p>"},{"location":"notebooks/week-1_2/#string-data-types","title":"String data types\u00b6","text":"<p>We also need to represent text data in our programs. In the crop yield dataset the sample id is text. In Python, text data is stored as a <code>str</code> type (or string type).</p> <p>Text data is stored as string types by enclosing the characters in double<code>\"</code> or single <code>'</code> quotes.</p>"},{"location":"notebooks/week-1_2/#boolean-data-types","title":"Boolean data types\u00b6","text":"<p>Boolean (<code>bool</code>) data types are used for storing <code>True</code> or <code>False</code> values. In Python, <code>True</code> or <code>False</code> are Boolean values and not string data types.</p> <p>Boolean data types are used to store the result of testing a condition that evaluates to true or false. For example, greater than and less than operations evaluate to true or false. We could test if our crop yield value of 0.227 is greater than 0.2 (it is and this expression should evaluate to true).</p>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"If you execute <code>x = 3.4</code>, what data type will <code>x</code> be? <code>float</code> Which data type would be most suited to record the number of apples harvested from an orchard? <code>int</code> -  we should not be able to harvest fractions of apples.  Which data type would we use to record a farm name? <code>str</code> - assuming the farm name is text data.  <code>y = 4 + 5</code> and <code>z = y &gt; 10</code> - what value will <code>z</code> be? <code>False</code> - <code>y</code> evaluates to 9 which is less than 10. Therefore, <code>z</code> will be <code>False</code> and of <code>bool</code> type."},{"location":"notebooks/week-1_2/#data-structures","title":"Data structures\u00b6","text":"<p>Python provides a series of built-in data structures that can be used to group together and store related data.</p> <p>Data structures can be used to model a range of practical and real-world problems or phenomenon by combining simple data types. For example, we could use a collection of numeric data to represent a time-series of precipitation, string data to represent the weather station name, and combine string and numeric data together in a data structure to create a bespoke weather station data structure.</p> <ul> <li><code>list</code> - a variable length collection of objects that can be modified</li> <li><code>tuple</code> - a fixed length collection of objects that cannot be modified</li> <li><code>set</code> - a collection of unique objects</li> <li><code>dict</code> - a collection of objects stored as key:value pairs</li> </ul>"},{"location":"notebooks/week-1_2/#lists","title":"Lists\u00b6","text":"<p>Lists in Python:</p> <ul> <li>can be modified during a program's execution</li> <li>can store duplicate values</li> <li>are created by placing elements in square brackets <code>[]</code></li> <li>elements of a list are ordered</li> <li>elements of a list can be of different data types</li> </ul> <p>We could store the sample names in our crop yield data set as a list:</p>"},{"location":"notebooks/week-1_2/#indexing-in-python","title":"Indexing in Python\u00b6","text":"<ul> <li>An index is an element's location within a data structure</li> <li>Often, in Python, elements are accessed by their index position using the square brackets operator and passing an index in as an argument (e.g. the first element of <code>x</code> is <code>x[0]</code>)</li> <li>In Python, indexing starts at 0 - this means the index for the first element in a data structure is 0</li> <li>It is important to become familiar with the concept of indexing - you will use it often as you handle data in Python</li> </ul> <p>Let's demonstrate this by accessing the elements of the <code>sample_id_list</code> we created above.</p>"},{"location":"notebooks/week-1_2/#tuples","title":"Tuples\u00b6","text":"<p>Tuple in Python:</p> <ul> <li>elements are unchangeable (immutable)</li> <li>tuple elements are ordered</li> <li>store a fixed-length number of elements</li> <li>created by placing Python objects inside parentheses <code>()</code></li> </ul>"},{"location":"notebooks/week-1_2/#sets","title":"Sets\u00b6","text":"<p>Set in Python:</p> <ul> <li>unordered collection objects</li> <li>set elements cannot be duplicated</li> <li>sets are immutable</li> <li>sets are created by placing elements inside curly brackets <code>{}</code></li> </ul> <p>Let's create a set and demonstrate that it cannot store duplicate values.</p>"},{"location":"notebooks/week-1_2/#dictionary-objects","title":"Dictionary objects\u00b6","text":"<p>Dictionary objects, or <code>dict</code> objects:</p> <ul> <li>store data as a collection of key:value pairs</li> <li>Dictionary objects can be changed and modified</li> <li>Dictionary object elements are ordered</li> <li>We can access dictionary elements by their key</li> <li>Dictionary objects are created by placing key:value pairs inside curly brackets <code>{}</code></li> <li>Keys of a dictionary object cannot be duplicated</li> <li>Elements (values) of dictionary objects can be of a different type</li> <li>Values can be of different lengths</li> </ul>"},{"location":"notebooks/week-1_2/#using-dictionary-objects-to-represent-tabular-data","title":"Using Dictionary objects to represent tabular data\u00b6","text":"<p>Tabular data has data values stored in rows and columns. Generally, a column corresponds to a particular variable or type of data and rows correspond to observed or measured values. We can use dictionary objects to represent tabular data in Python.</p> <p>For example, we can use the key:value pair pattern to represent a column header and column values.</p> <pre>\"Elevation(m)\": [213, 222, 214, 254]\n</pre> <p>Here, we've used a string object to represent the column header (the key) and a list object to represent column values (the value). Combining one or more key:value pairs in a dictionary object is a way of representing tabular data in Python.</p> <pre>{\n\"Elevation(m)\": [213, 222, 214, 254],\n\"Date\": [\"20/11/2017\", \"20/11/2017\", \"20/11/2017\", \"19/11/2017\"],\n\"sample_id\": [\"bf66_sample_1\", \"bf66_sample_1\", \"bf66_sample_2\", \"bf66_sample_3\"]\n}\n</pre> <p>Let's create this dictionary object.</p>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<code>x = [1, 2, 3, 4]</code> - what data structure is <code>x</code>? List  <code>q = {1, 2, 3, 4}</code> - what data structure is <code>q</code>? Set  Which data structure organises its elements using key:value pairs? Dictionary objects  Are lists immutable data structures? No, we can modify the values of a list and change its size (number of elements) during the program's execution."},{"location":"notebooks/week-1_2/#variables","title":"Variables\u00b6","text":"<p>In the program to read in and visualise crop yield data, you will have noticed this syntax pattern: <code>variable_name = data</code>.</p> <pre>basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\n</pre> <p><code>=</code> is the assignment operator (not equals) which is assigning (or binding) the <code>variable_name</code> to the <code>data</code> object.</p> <p>The statement <code>basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"</code> is assigning the data <code>\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"</code> to the variable name <code>basemap</code>. The data is string which is storing the URL for a satellite imagery basemap.</p> <p>Let's see what the data looks like.</p>"},{"location":"notebooks/week-1_2/#variable-names","title":"Variable names\u00b6","text":"<p>There are rules that need to be followed when creating variable names:</p> <ul> <li>variable names can only be text letters (A-Z, a-z), numbers (0-9), or underscores (_)</li> <li>variable names cannot begin with a number</li> <li>variable names cannot include whitespace</li> <li>variable names cannot be reserved keywords such True, False, if</li> </ul> <p>There are also some conventions that can be followed to help write programs that are easy to follow:</p> <ul> <li>use a consistent style for writing variable names - snake_case where words are separated by underscores is recommended by the PEP8 style guide for Python code</li> <li>use a descriptive variable name so it helps you understand what kind of data it is referring to</li> </ul>"},{"location":"notebooks/week-1_2/#control-flow","title":"Control flow\u00b6","text":"<p>So far we have demonstrated how we can represent information and concepts in our program as different data types and using different data structures. However, we are not using our program as a data storage device. We are using it to complete tasks with this data.</p> <p>Control flow refers to the sequence of steps that our program takes from start to finish when it is run.</p> <p>The most simple form of control flow is executing statements from top to bottom as they are written in your script.</p>"},{"location":"notebooks/week-1_2/#loops","title":"Loops\u00b6","text":"<p>Loops use a <code>for</code> statement that allows us to iterate over items in a sequence. For example, if <code>x</code> is a list <code>[1, 2, 3]</code> then:</p> <pre>for i in x:\n    print(i)\n</pre> <p>will print the values of 1, 2, and 3 in turn for each iteration of the loop, <code>i</code>, takes on the value of the corresponding element in <code>x</code>.</p> <p>For loops are useful if we want to iterate over (repeat) a block of statements using different values from a sequence of items in turn. If we wanted to add 10 to each element of <code>x</code> we could write a for loop as:</p> <pre>for i in x:\n    print(i + 10)\n</pre> <p>Let's demonstrate this.</p>"},{"location":"notebooks/week-1_2/#conditional-execution","title":"Conditional execution\u00b6","text":"<p>Conditional execution is a form of control flow that allows for branches in the sequence that statements are executed. A boolean condition is tested, and, <code>if</code> it evaluates to <code>True</code> then one set of statements are executed and <code>if</code> it evaluates to <code>False</code> a different set of statements are executed.</p> <p>Conditions are tested within <code>if</code> blocks:</p> <pre>if True:\n    these statements are executed\nelse:\n    these statements are executed\n</pre> <p>A simple example:</p> <pre>x = 11\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre> <p>In this instance, the statements inside the <code>if</code> block will be executed as <code>x</code> is 11 and so greater than 10.</p> <p>However, if we change the value of x to 9 the statements inside the <code>if</code> block will not be executed.</p> <pre>x = 9\n\nif x &gt;= 10:\n    print(\"x is greater than or equal to 10\")\n</pre>"},{"location":"notebooks/week-1_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Write a for loop to subtract 3 from every element in a tuple <code>s</code> and print the result?</p>"},{"location":"notebooks/week-1_2/#classes-and-objects","title":"Classes and Objects\u00b6","text":"<p>We have mentioned a few times that Python programs comprise objects. We will now define what an object is and how the concept of an object relates to the data types and structures we have looked at above.</p>"},{"location":"notebooks/week-1_2/#objects","title":"Objects\u00b6","text":"<p>An object is a more general concept in programming. An object in a computer program has:</p> <ul> <li>properties or attributes (data)</li> <li>behaviour (methods or functions that do things with object data).</li> </ul> <p>Objects provide a way to add structure to a program by combining related data and behaviours.</p> <p>Alongside the built-in data types and structures, we can create objects that are customised and useful for specific tasks or representing specific kinds of data or real-world problems. Often, custom data types will combine the built-in data types and structures, or other custom objects, to create objects that represent more complex concepts.</p>"},{"location":"notebooks/week-1_2/#classes","title":"Classes\u00b6","text":"<p>A class is a definition of an object. The class defines the object's data and methods. An object is an instance of a class in our programs.</p> <p>An object's data (sometimes called its properties or attributes) can be built-in Python data types or structures or custom classes. An object's methods are functions that can be used to transform data or implement other behaviour that is relevant to the object. For example, an object representing a table is more complex than a list.</p>"},{"location":"notebooks/week-1_2/#pandas-dataframes","title":"pandas <code>DataFrame</code>s\u00b6","text":"<p>For most use cases there are existing classes that have already been developed and can be used in our programs. Keeping with the idea of working with tabular data, pandas provides a <code>DataFrame</code> class to work with tabular data. In our Python program to visualise the crop yield data we read the tabular data stored in CSV files into pandas <code>DataFrames</code>.</p> <p>A pandas <code>DataFrame</code> is a far more comprehensive class for handling tabular data. Compared to our small example of using a dictionary object to store tabular data, a pandas <code>DataFrame</code> implements well defined data structures for tabular data and provides a suite of useful methods (functions) for analysing and processing tabular data.</p>"},{"location":"notebooks/week-1_2/#dataframe-properties","title":"<code>DataFrame</code> Properties\u00b6","text":"<p>The class <code>DataFrame</code> is a data structure for \"Two-dimensional, size-mutable, potentially heterogeneous tabular data\". It has the following properties:</p> <ul> <li>data - the tabular data stored in the data frame</li> <li>index - the row index (row labels)</li> <li>columns - the column headers</li> <li>dtype - the data types of each column</li> </ul> <p></p> <p>A pandas <code>DataFrame</code> stores its values in rows and columns. There is a list of column headers (which should provide a helpful description of what kind of data is in each column) and a row index.</p> <p>The values in columns are pandas <code>Series</code> objects. A <code>Series</code> is an array of the same type data.</p> <p>The pandas <code>DataFrame</code> object stores a range of properties that are useful for working with tabular data. You can look up the documentation for a <code>DataFrame</code> here. The properties listed do not have parentheses after their name <code>()</code>. Let's look at the properties under attributes and underlying data. We can see the <code>DataFrame</code> has a <code>values</code> property which is the actual data values in the table and a <code>shape</code> property which tells us the shape of the table (number of rows and columns).</p> <p></p> <p>Most of the time we will explore classes using their documentation websites. Let's read a CSV file into a pandas <code>DataFrame</code> and explore its properties.</p>"},{"location":"notebooks/week-1_2/#dataframe-methods","title":"<code>DataFrame</code> methods\u00b6","text":"<p>However, a <code>DataFrame</code> is more than just an object for storing data. It is also where we can find a wide range of functions for working with tabular data - the class methods. Go back to the documentation for a <code>DataFrame</code> here. The <code>DataFrame</code> methods are indicated by the parentheses after their name <code>()</code>.</p> <p>There are methods to help us inspect the <code>DataFrame</code> values. The <code>info()</code> method prints a concise summary table of the <code>DataFrame</code>, there a range of methods for performing mathemtatical computations / descriptive stats, and methods for reading data on disk into <code>DataFrame</code>s.</p> <p>Let's explore how we can use some of these methods to work with tabular data in <code>DataFrames</code>.</p>"},{"location":"notebooks/week-1_2/#methods-and-functions","title":"Methods and functions\u00b6","text":"<p>Functions are executed to perform a task using data. You can spot functions by <code>()</code> appearing after the function name. Functions can take input data - this is the function's arguments -  and use these inputs to complete it's task.</p> <p>Functions can return data back to the Python program, display data on your screen, or save data to disk.</p> <p>For example, the <code>print()</code> function takes data as an argument and prints it on your display as text. For example, <code>print(\"hello\")</code> would print <code>hello</code> on your screen. The <code>df = pd.read_csv(\"a_csv_file.csv\")</code> is a function within the pandas package the takes in a path to a CSV file as an argument and reads the data from that file into a <code>DataFrame</code> referenced by <code>df</code>.</p> <p>Methods are functions that belong to a class. For example, the <code>mean()</code> method of the pandas <code>DataFrame</code> can be called by accessing it via a <code>DataFrame()</code> object (e.g <code>df.mean()</code>). Methods often provide functionality related to the \"topic\" of the class they're associated with. For example, <code>DataFrame</code> methods provide functions related to working tabular data in <code>DataFrame</code> objects; dictionary objects have specialist methods geared for retrieving and setting data elements in dictionaries; string objects have methods for manipulating text (e.g. the <code>lower()</code> of a string object will convert all text characters to lower case).</p> <p>The general pattern for executing a function / method is:</p> <ol> <li>The function is called and any data are passed in as arguments inside <code>()</code></li> <li>The function performs operations on the data passed in</li> <li>The function returns the result of operating on the data</li> </ol> <p>As everything in Python is an object, lists and string objects have methods. You can see the methods associated with an object by typing the object name, dot operator, and pressing tab.</p> <p>For example, if <code>farm_name = \"my farm\"</code>, <code>farm_name.&lt;press_tab&gt;</code> will print a list of string object methods.</p> <p>Try this below and explore string and list methods, and how they can be used to transform your data.</p>"},{"location":"notebooks/week-1_2/#instances","title":"Instances\u00b6","text":"<p>Above we said that a class is a definition of an object. The class never has any data stored in it or executes any of its methods - it's just a template for a particular type of object. Objects are instances of a class.</p> <p>When we executed the statement <code>df = pd.read_csv(\"csv_file.csv\")</code> we are creating an instance object of the <code>DataFrame</code> class which is stored in the computer's memory.</p>"},{"location":"notebooks/week-1_2/#dot-notation","title":"Dot notation\u00b6","text":"<p>We use the <code>.</code> operator to access properties and methods of class. For example to access the <code>shape</code> property of a <code>DataFrame</code> we write <code>df.shape</code> and to access the <code>info()</code> method we write <code>df.info()</code>.</p>"},{"location":"notebooks/week-1_2/#modules","title":"Modules\u00b6","text":"<p>There is one final concept to cover that is used in our program to read and visualise harvester yield data. The very first lines of the program.</p> <pre># Import modules\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\n</pre> <p>Here, we are importing modules into our script.  A module is a Python file (a file with a <code>.py</code> ending) that contains class definitions and functions. When we import a module into our program we can use the classes defined in the module.</p> <p>For example, we have used the method <code>os.path.join()</code> from the os module in our program. This function takes string data describing parts of a file path and joins them together to create path to a directory or file.</p>"},{"location":"notebooks/week-2_1/","title":"Week 2 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-2_1\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_1\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-2_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-2_1\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-2_1.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-2_1\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-2_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre># Import modules\nimport os\nimport pandas as pd\nimport geopandas as gpd\nimport plotly.express as px\nimport numpy as np\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> # Import modules import os import pandas as pd import geopandas as gpd import plotly.express as px import numpy as np import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># Load the crop yield data\ncrop_yield_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\")\n\n# Read the canola and wheat crop yield data\ncanola_fpath = os.path.join(crop_yield_data_path, \"bf66-canola-yield-max-vi_sampled.geojson\")\ncanola_gdf = gpd.read_file(canola_fpath)\nwheat_fpath = os.path.join(crop_yield_data_path, \"bf66-wheat-yield-max-vi_sampled.geojson\")\nwheat_gdf = gpd.read_file(wheat_fpath)\n\n# Combine (stack) the geojson files into one GeoDataFrame\ngdf = pd.concat([canola_gdf, wheat_gdf], axis=0)\ngdf.head()\n</pre> # Load the crop yield data crop_yield_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\")  # Read the canola and wheat crop yield data canola_fpath = os.path.join(crop_yield_data_path, \"bf66-canola-yield-max-vi_sampled.geojson\") canola_gdf = gpd.read_file(canola_fpath) wheat_fpath = os.path.join(crop_yield_data_path, \"bf66-wheat-yield-max-vi_sampled.geojson\") wheat_gdf = gpd.read_file(wheat_fpath)  # Combine (stack) the geojson files into one GeoDataFrame gdf = pd.concat([canola_gdf, wheat_gdf], axis=0) gdf.head() <p>Displaying the <code>head</code> of the <code>GeoDataFrame</code> <code>gdf</code> demonstrates that we are working with tabular data. There is a <code>geometry</code> column,  which stores the geographic location that each row in the table's attributes correspond to. Other columns of note are:</p> <ul> <li><code>DryYield</code> - crop yield values for each location (tonnes / ha)</li> <li><code>Variety</code> - 43Y23 RR indicates canola  ninja indicates wheat</li> <li><code>gndvi</code> - green normalised difference vegetation index, a satellite derived measure of greenness</li> <li><code>ndyi</code> - normalised difference yellowness index, a satellite derived measure of yellowness</li> </ul> In\u00a0[\u00a0]: Copied! <pre>fig = px.histogram(\n    data_frame=gdf, \n    x=\"DryYield\", \n    color=\"Variety\", \n    marginal=\"box\", \n    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\nfig.show()\n</pre> fig = px.histogram(     data_frame=gdf,      x=\"DryYield\",      color=\"Variety\",      marginal=\"box\",      hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"]) fig.show() <p>There are more options that you can use to configure a histogram here.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>fig = px.histogram(\n    data_frame=gdf, \n    x=\"DryYield\", \n    color=\"Variety\", \n    marginal=\"box\", \n    range_x=[0, 7],\n    hover_data=[\"DryYield\", \"Elevation\", \"WetMass\"])\nfig.show()\n</pre> <p>Let's have a go at generating a scatter plot to consolidate our understanding of how to map variables in our data to elements of a graphic. The documentation for scatter plots is here and you should notice similarities in how we set up a scatter plot to a histogram.</p> <p>Let's use a scatter plot to see if there is a relationship beetween crop yield and elevation. We are plotting two variables here so we need to use the <code>y</code> parameter to specify what column in our <code>GeoDataFrame</code> will be mapped onto the y-axis.</p> <p>We can use the <code>marginal_x</code> and <code>marginal_y</code> parameters to attach plots to the x- and y-axes that show the distributions of variables mapped to each axis.</p> <p>Finally, we're going to use the <code>opacity</code> argument here to make the point elements on the figure semi-transparent; this will help reveal more information about the density of data values.</p> <p>Both canola and wheat crop yield data is displayed. To see the relationship between one crop type's yield and elevation, click on the variety in the legend.</p> In\u00a0[\u00a0]: Copied! <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"DryYield\", \n    y=\"Elevation\", \n    color=\"Variety\", \n    opacity=0.25, \n    marginal_x=\"box\", \n    marginal_y=\"violin\")\nfig.show()\n</pre> fig = px.scatter(     data_frame=gdf,      x=\"DryYield\",      y=\"Elevation\",      color=\"Variety\",      opacity=0.25,      marginal_x=\"box\",      marginal_y=\"violin\") fig.show() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"DryYield\", \n    y=\"Elevation\", \n    color=\"Variety\", \n    range_x=[0,10],\n    opacity=0.25,\n    marginal_y=\"violin\")\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"gndvi\", \n    y=\"DryYield\", \n    color=\"Variety\", \n    opacity=0.05, \n    range_y=[0.1, 6], \n    range_x=[0.3, 0.9], \n    marginal_x=\"box\", \n    marginal_y=\"box\", \n    trendline=\"ols\"\n)\nfig.show()\n</pre> fig = px.scatter(     data_frame=gdf,      x=\"gndvi\",      y=\"DryYield\",      color=\"Variety\",      opacity=0.05,      range_y=[0.1, 6],      range_x=[0.3, 0.9],      marginal_x=\"box\",      marginal_y=\"box\",      trendline=\"ols\" ) fig.show() In\u00a0[\u00a0]: Copied! <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"ndyi\", \n    y=\"DryYield\", \n    facet_col=\"Variety\", \n    opacity=0.05, \n    range_y=[0.1, 5], \n    range_x=[0.1, 0.6], \n    trendline=\"ols\"\n)\nfig.show()\n</pre> fig = px.scatter(     data_frame=gdf,      x=\"ndyi\",      y=\"DryYield\",      facet_col=\"Variety\",      opacity=0.05,      range_y=[0.1, 5],      range_x=[0.1, 0.6],      trendline=\"ols\" ) fig.show() In\u00a0[\u00a0]: Copied! <pre>fig = px.colors.sequential.swatches_continuous()\nfig.show()\n</pre> fig = px.colors.sequential.swatches_continuous() fig.show() <p>Let's use a sequential colour palette to visualise monthly precipitation over the field since 1981. The precipitation data is obtained from the TerraClimate: Monthly Climate and Climatic Water Balance for Global Terrestrial Surfaces dataset.</p> <p>Use the pandas <code>read_csv()</code> function to read in the precipitation data. Inside the <code>CSV</code> file each row represents a month-year combination and stores a monthly precipitation total in mm.</p> In\u00a0[\u00a0]: Copied! <pre># visualise monthly precipitation using a diverging palette\nclimate_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\")\nprecip_df = pd.read_csv(os.path.join(climate_data_path, \"bf66-terra-precip-monthly.csv\"))\nprecip_df[\"month\"] = precip_df[\"month\"].astype(str)\nprecip_df[\"year\"] = precip_df[\"year\"].astype(str)\nprecip_df.head()\n</pre> # visualise monthly precipitation using a diverging palette climate_data_path = os.path.join(os.getcwd(), \"data_lab-2_1\") precip_df = pd.read_csv(os.path.join(climate_data_path, \"bf66-terra-precip-monthly.csv\")) precip_df[\"month\"] = precip_df[\"month\"].astype(str) precip_df[\"year\"] = precip_df[\"year\"].astype(str) precip_df.head() <p>We can create a heatmap to visualise monthly precipitation across time and using a colour palette where darker blue shades indicate wetter months. Note how we pass in the colour palette <code>Blues</code> as an argument to the <code>color_continuous_scale</code> parameter.</p> In\u00a0[\u00a0]: Copied! <pre>fig = px.density_heatmap(\n    precip_df,\n    x=\"year\", \n    y=\"month\", \n    z=\"pr\", \n    histfunc=\"sum\",\n    nbinsy=12,\n    color_continuous_scale=\"Blues\",\n    range_color=(0, 75),\n)\nfig.show()\n</pre> fig = px.density_heatmap(     precip_df,     x=\"year\",      y=\"month\",      z=\"pr\",      histfunc=\"sum\",     nbinsy=12,     color_continuous_scale=\"Blues\",     range_color=(0, 75), ) fig.show() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>fig = px.density_heatmap(\n    precip_df,\n    x=\"year\", \n    y=\"month\", \n    z=\"pr\", \n    histfunc=\"sum\",\n    nbinsy=12,\n    color_continuous_scale=\"YlGnBu\",\n    range_color=(0, 75),\n)\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>fig = px.scatter(\n    data_frame=gdf, \n    x=\"gndvi\", \n    y=\"DryYield\", \n    facet_col=\"Variety\", \n    opacity=0.25, \n    range_y=[0.1, 5], \n    range_x=[0.4, 0.9], \n    color=\"gndvi\",\n    color_continuous_scale=\"Greens\",\n    range_color=(0.4, 0.8),\n)\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig = px.colors.diverging.swatches_continuous()\nfig.show()\n</pre> fig = px.colors.diverging.swatches_continuous() fig.show() <p>We can use a diverging colour palette to visualise the same precipitation data. Monthly precipitation values are converted to z-scores, which represent deviations in monthly precipitation away from the mean. A z-score of zero represents average rainfall and can be used as the mid-point for a diverging colour palette. Here, we can use red-to-blue colour palette, with drier months represented by red shades.</p> In\u00a0[\u00a0]: Copied! <pre># compute average rainfall and standard deviation of rainfall to compute z scores\n# use z score as 0 for the mid-point of a diverging colour palette\navg_pr = precip_df[\"pr\"].mean()\nstd_pr = precip_df[\"pr\"].std()\nprecip_df.loc[:, \"z_score\"] = (precip_df.loc[:, \"pr\"] - avg_pr) / std_pr\n\nfig = px.density_heatmap(\n    precip_df,\n    x=\"year\", \n    y=\"month\", \n    z=\"z_score\", \n    histfunc=\"sum\",\n    nbinsy=12,\n    color_continuous_scale=\"RdBu\",\n    color_continuous_midpoint=0,\n)\nfig.show()\n</pre> # compute average rainfall and standard deviation of rainfall to compute z scores # use z score as 0 for the mid-point of a diverging colour palette avg_pr = precip_df[\"pr\"].mean() std_pr = precip_df[\"pr\"].std() precip_df.loc[:, \"z_score\"] = (precip_df.loc[:, \"pr\"] - avg_pr) / std_pr  fig = px.density_heatmap(     precip_df,     x=\"year\",      y=\"month\",      z=\"z_score\",      histfunc=\"sum\",     nbinsy=12,     color_continuous_scale=\"RdBu\",     color_continuous_midpoint=0, ) fig.show()"},{"location":"notebooks/week-2_1/#data-visualisation","title":"Data visualisation\u00b6","text":"<p>This lab will generate interactive visualisations of crop yield data for wheat and canola collected by a harvester from a field in Western Australia. This lab will provide an introduction to:</p> <ul> <li>interactive visualisations using Plotly Express</li> <li>using figures to represent and explore different features of a dataset</li> <li>using colour to visualise patterns in a dataset</li> </ul>"},{"location":"notebooks/week-2_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-2_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-2_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-2_1/#what-is-a-figure","title":"What is a figure?\u00b6","text":"<p>Data visualisation is the process of relating data values to elements of a figure on a computer display.</p> <p>The Grammar of Graphics is an underlying model that describes the mapping of data values to the visual elements of a figure. It provides a consistent framework for guiding us in how to take our data values and convert them into a figure that effectively represents the data and conveys the messages and insights we seek to communicate.</p> <p>In the Grammar of Graphics a plot comprises data and a mapping. The mapping (not cartographic here) is a formal description of how data values map onto elements of a figure. The elements of a figure are termed aesthetics and consist of:</p> <ul> <li>layers - geometric elements that represent data values such as points (e.g. for scatter plots), lines (e.g. for lines of best fit), and polyons (e.g. for histograms or bar plots).</li> <li>scales - relate data values to visual display properties such as colour (e.g. a blue to red colour palette for temperature), size (e.g. larger points for larger numbers), position (e.g. location on axes), or shapes (e.g. using triangles for group A and circles for group B). Scales are used to draw axes and legends for figures.</li> <li>coords - coordinate systems are used to map data values onto locations on the figure. On most 2D figures the x- and y-axes describe the coordinate space and on maps latitude and longitude describe the coordinate space (or you can use different coordinate reference systems).</li> <li>theme - the background styling of the figure such as fonts for labels and background colours.</li> </ul> <p></p> <p>Reading the A Layered Grammar of Graphics paper by Hadley Wickham provides a detailed description of the core concepts for designing high-quality data visualisations.</p>"},{"location":"notebooks/week-2_1/#interactive-visualisations","title":"Interactive visualisations\u00b6","text":"<p>Interactive visualisations are important tools for exploring complex and multidimensional data. They enable users to quickly develop an understanding of a dataset's structure and patterns by enabling them to interactively generate different views of the dataset.</p> <p>Generally, interactive visualisations are controlled by user input from mouse events (click, drag, hover), and, in response to mouse events, change what data and information is rendered on the computer display.</p> <p>Interactive visualisations are important tools for both exploratory analysis and for communicating the results of analysis to a wider audience. For exploratory analysis the quick feedback provided by interactive visualisations allows analysts to quickly build up an understanding of the datasets they are working with, spot noise or missing data, refine and develop hypotheses and research questions, and select suitable analytical and statistical tools for further work. Interactive visualisations are useful for communication as they enable active engagement with your datasets and the message you are conveying in a user friendly and non-technical manner.</p> <p>Here, we will be using Plotly Express to develop interactive visualisations. Plotly Express is a Python module that contains functions that convert data in Python programs into interactive visualisations that can be rendered in web browser based environments.</p> <p>Plotly Express has several useful features for developing interactive visualisations:</p> <ul> <li>functions to generate a range of figure types to explore spatial and non-spatial data (see the gallery)</li> <li>consistent API for functions used to generate the figures (i.e. if you learn the syntax and format to generate scatter plots it can be applied to generate histograms, density plots, bar plots, violin plots, web maps, etc.)</li> <li>simple and intuitive functions to generate the figures (i.e. produce complex interactive figures with a single line of code)</li> </ul>"},{"location":"notebooks/week-2_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-2_1/#data-input","title":"Data input\u00b6","text":"<p>Let's read in some wheat and canola yield data collected by a harvester into a GeoPandas <code>GeoDataFrame</code>. The canola data corresponds to variety 43Y23 RR and the wheat data corresponds to variety ninja. We'll demonstrate how to create interactive visualisations using Plotly Express by generating a simple widget that displays the distribution of wheat and canola yields.</p>"},{"location":"notebooks/week-2_1/#interactive-visualisations-with-plotly-express","title":"Interactive visualisations with Plotly Express\u00b6","text":"<p>Now, let's unpick the syntax for specifying a Plotly Express visualisation. The functions to generate interactive figures are part of the plotly.express module which we've imported into our program as <code>px</code>.</p> <p><code>px.&lt;function name&gt;()</code> is how we'll access the function to generate a given figure. For example, to generate a histogram we call <code>px.histogram()</code> (if we wanted to generate a scatter plot we'd call <code>px.scatter()</code>, if we wanted to generate a line chart we'd call <code>px.line()</code>, if we wanted to generate a pie chart we'd call <code>px.pie()</code> - you get the pattern ...).</p> <p>Next, we need to pass data into the function that will be rendered on the computer display and specify arguments to map data values to elements on the figure. The Plotly Express documentation lists functions that can be used to generate figures and their parameters.</p> <p>Paramters for the <code>px.histogram()</code> function inclue:</p> <ul> <li><code>data_frame</code> - a <code>DataFrame</code> object containing the data to render on the histogram</li> <li><code>x</code> - specifies the column in the <code>DataFrame</code> to be mapped on the x-axis of the figure</li> <li><code>color</code> - a column whose values are used to assign colours to marks (elements) on the display</li> <li><code>marginal</code> - either violin, box, rug, or histogram that shows the distribution of the data</li> <li><code>hover_data</code> - list of column names with values that will be shown in a popup when the cursor hovers over a record on the display</li> </ul> <p>Use the Zoom tool to control what data is visualised and focus the figure on where most of the data is distributed.</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Look up the <code>range_x</code> paramter and consider how it could be used to remove the influence of outliers on the figure. Have a go at using it to restrict the range of values mapped to the x-axis.</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you limit the range of x-axis values to focus the figure on where most of the data is concentrated and remove the effect of outliers? (hint, you'll need to remove the <code>marginal_x</code> argument).</p>"},{"location":"notebooks/week-2_1/#adding-layers","title":"Adding layers\u00b6","text":"<p>The scatter plot we have generated above has layers of points for the scatter plot and layers of geometric elements for the box plot and violin plots. However, each of these layers are all rendered on their own sub-plot.</p> <p>There are often times when we want to overlay layers on the same plot. A common example of this is adding a trendline to a scatter plot to help the viewer see patterns and relationships in the data. If we refer back to the documentation for scatter plots we can see there is a <code>trendline</code> parameter. We can use this parameter to specify the kind of trendline we'd like to draw on our scatter plot:</p> <ul> <li><code>ols</code>: ordinary least squares (or linear line of best fit)</li> <li><code>loess</code>: locally weighted scatterplot smoothing line</li> <li><code>rolling</code>: rolling average or rolling median line</li> </ul> <p>Let's generate a scatter plot with a trendline to explore the relationship between the green normalised difference vegetation index (GNDVI, a satellite derived measure of vegetation greenness) and crop yield. Generally, higher maximum growing season GNDVI values are correlated with higher crop yields.</p> <p>If you hover your cursor over the trendline it will show you the equation for the trendline. You will also notice that we've used the the <code>range_x</code> and <code>range_y</code> parameters to focus the figure on the region where most of the data is concentrated and clip the outliers from the view.</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"Generally, it seems that maximum growing season GNDVI is higher for the wheat (Ninja) crop than canola (43Y23 RR). Can you think of an explanation for this? Canola canopies are characterised by yellow flowers which could reduce their greenness during the growing season."},{"location":"notebooks/week-2_1/#facets","title":"Facets\u00b6","text":"<p>So far we have distinguished groups of data points on the same figure by using a unique colour per-group. However, this can lead to cluttered figures which obscures important variation in the data. To avoid clutter we can create faceted figures where mutliple subplots of the same type are generated, which share axes, and different subsets (groups) of the data are rendered on each subplot.</p> <p>Wilke (2019) distinguish between faceted figures and compound figures. Compound figures are multiple figure types (e.g. scatter plots, histograms, maps), possibly of different datasets, combined into one figure. A key feature of a compound figure is that the subplots do not need to be arranged in a regular grid. The figures above with violin and box plots aligned on the margins of a scatter plot are examples of compound figures.</p> <p>In contrast, facet plots consist of subplots of the same type, showing subsets of the same dataset, and are arranged on a regular grid. You might see the term trellis or lattice plots used to describe facet plots. To ensure correct interpretation of faceted figures it is important that the axes on all plots share the same range and scalings.</p> <p>Let's create a faceted figure that shows the relationship between crop yield and the normalised difference yellowness index (NDYI) side-by-side. The NDYI is a spectral index computed from remote sensing data as a mathematical combination of green and blue reflectance values. Higher NDYI values are associated with a yellower land surface. The NDYI is often used to monitor canola flowering.</p> <p>We can use the <code>facet_row</code> parameter to align subplots on separate rows or the <code>facet_col</code> parameter to align the subplots on separate columns. We specify a column in our <code>GeoDataFrame</code> to use to create the facets. The dataset is split into subsets using unique values in the specified column and each subset is rendered on a subplot. Here, we pass in the <code>Variety</code> column to split the data by crop type.</p>"},{"location":"notebooks/week-2_1/#selecting-the-right-figure","title":"Selecting the \"right\" figure\u00b6","text":"<p>Chapter 5 of Wilke (2019) provides a directory of visualisations which serves as a useful guide for selecting the correct visualisation for different types of data.</p>"},{"location":"notebooks/week-2_1/#using-colour","title":"Using Colour\u00b6","text":"<p>A colour scale is used to map data values to colours on the display. Wilke (2019) outline three uses of colour on figures:</p> <ul> <li>colour to represent data values (e.g. using red shades for low precipitation and blue shades for high precipitation)</li> <li>colour to distinguish groups (e.g. using green for forest on a land cover map, blue for water, orange-red for desert, etc.)</li> <li>colour to highlight (e.g. using colour to highlight particular features on your visualisation)</li> </ul> <p>We can broadly characterise colour scales as being either continuous or qualitative.</p>"},{"location":"notebooks/week-2_1/#continuous-palettes","title":"Continuous palettes\u00b6","text":"<p>Continuous colour scales can be either sequential or diverging and are typically used when using colour to represent data values (often numeric continuous variables). Continuous colour scales can be used to visualise variation in attributes of vector geospatial data on chloropleth maps and variation in attributes of raster data as surfaces.</p>"},{"location":"notebooks/week-2_1/#sequential-palettes","title":"Sequential palettes\u00b6","text":"<p>A sequential colour scale is a palette which consists of single hue such as light green to dark green or light red to dark red. Multi-hue sequential colour scales often consist of hues that imply an intuitive and increasing order to the colours such as light yellows to dark red.</p> <p>Plotly express provides a range of inbuilt sequential colour scales:</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you create a heatmap of monthly precipitation over time using a <code>YlGnBu</code> colour palette?</p>"},{"location":"notebooks/week-2_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Using the <code>GeoDataFrame</code> <code>gdf</code> of crop yield values, can you create a scatter plot of crop yield (the <code>DryYield</code> column) and GNDVI (the <code>gndvi</code> column) and assign green shades to the points which reflect their GNDVI values? Tips: look up the <code>color</code>, <code>color_continuous_scale</code>, and <code>range_color</code> parameters of the <code>scatter()</code> function in the API docs.</p>"},{"location":"notebooks/week-2_1/#diverging-palettes","title":"Diverging palettes\u00b6","text":"<p>Diverging colour scales are used to represent data values deviating in two directions. Often a light colour (e.g. white) is used as the mid-point of a diverging colour scale with gradients of intensifying colour away from this mid-point. A common example of diverging colour scales are climate or weather anomalies where dry or hot years are represented with red colours and wet and cool years are represented with blue colours. Average conditions are often a pale red, pale blue, or white.</p> <p>Plotly also provides a range of diverging colour palettes we can use:</p>"},{"location":"notebooks/week-2_1/#qualitative-palettes","title":"Qualitative palettes\u00b6","text":"<p>Qualitative (or discrete) colour scales should be used to represent groups or categorical data (i.e. data where there is no logical ordering). Thus, qualitative colour scales should not represent gradients of light to dark or use colours that can be interpreted as having an implied ordering. Often, it is sensible to select colours that relate to the category (e.g. on land cover maps using green for vegetated categories, blue for water etc.).</p>"},{"location":"notebooks/week-2_2/","title":"Week 2 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-2_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-2_2\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-2_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-2_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-2_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-2_2\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-2_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport rioxarray as rxr\nimport xarray as xr\nimport plotly.express as px\nimport numpy as np\nimport geopandas as gpd\n\nimport plotly.io as pio\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import os  import rioxarray as rxr import xarray as xr import plotly.express as px import numpy as np import geopandas as gpd  import plotly.io as pio  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre>data_path = os.path.join(os.getcwd(), \"data_lab-2_2\")\n</pre> data_path = os.path.join(os.getcwd(), \"data_lab-2_2\") In\u00a0[\u00a0]: Copied! <pre># create a 2D ndarray\narr2d = np.array([[1, 2, 3], [4, 5, 6]])\narr2d\n</pre> # create a 2D ndarray arr2d = np.array([[1, 2, 3], [4, 5, 6]]) arr2d <p>The rank (or number of dimensions) of a <code>ndarray</code> is the number of axes.</p> In\u00a0[\u00a0]: Copied! <pre># the rank (ndim) of an ndarry is the number of axes \nprint(f\"the rank of the ndarray is {arr2d.ndim}\")\n</pre> # the rank (ndim) of an ndarry is the number of axes  print(f\"the rank of the ndarray is {arr2d.ndim}\") <p>The <code>shape</code> of an <code>ndarray</code> tells us the size of each axis (how many elements are arranged along that axis).</p> In\u00a0[\u00a0]: Copied! <pre># the shape of the ndarray \nprint(f\"the shape of the ndarray is {arr2d.shape}\")\n</pre> # the shape of the ndarray  print(f\"the shape of the ndarray is {arr2d.shape}\") <p><code>ndarray</code>s can be multidimensional. They can have more than two dimensions. Remote sensing images typically comprise multiple 2-dimensional arrays with each array corresponding to a raster of reflectance measured in a particular wavelength. This 3-dimensional raster data structure can be represented as a NumPy <code>ndarray</code> with the bands dimension on axis 0 (each band is a raster for a given wavelength), rows (height of each raster) on axis 1, and columns (width of each raster) on axis 2.</p> <p>Let's create a <code>ndarray</code> with 3-dimensions.</p> In\u00a0[\u00a0]: Copied! <pre># create a 3D ndarray\narr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\narr3d\n</pre> # create a 3D ndarray arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) arr3d In\u00a0[\u00a0]: Copied! <pre>print(f\"the rank of the ndarray is {arr3d.ndim}\")\nprint(f\"the shape of the ndarray is {arr3d.shape}\")\n</pre> print(f\"the rank of the ndarray is {arr3d.ndim}\") print(f\"the shape of the ndarray is {arr3d.shape}\") <p>The concept of N-dimensional arrays can be extended further. For example, a 4-dimensional <code>ndarray</code> could store a sequence of 3-dimensional <code>ndarray</code>s where the fourth dimension is time and the object represents remote sensing images captured across multiple dates.</p> In\u00a0[\u00a0]: Copied! <pre>first_element = arr2d[0, 0]\nprint(first_element)\n</pre> first_element = arr2d[0, 0] print(first_element) <p>We can use the <code>:</code> symbol to specify slices of a NumPy <code>ndarray</code> to subset. For example, the following are three different ways of slicing the first two rows.</p> <p>Note that the slice is not inclusive of the index location after the <code>:</code> symbol. So, <code>arr2d[0:2, ]</code> would select the first two rows of <code>arr2d</code> - row 0 and row 1 (remember Python indexes from 0).</p> In\u00a0[\u00a0]: Copied! <pre>two_rows_1 = arr2d[0:2, ]\nprint(two_rows_1)\n\ntwo_rows_2 = arr2d[0:2]\nprint(two_rows_2)\n\ntwo_rows_3 = arr2d[:2]\nprint(two_rows_3)\n</pre> two_rows_1 = arr2d[0:2, ] print(two_rows_1)  two_rows_2 = arr2d[0:2] print(two_rows_2)  two_rows_3 = arr2d[:2] print(two_rows_3) <p>We can use multiple slices for different axes. For example, if we wanted to subset values from a selection of rows and columns.</p> In\u00a0[\u00a0]: Copied! <pre>two_rows_cols = arr2d[:2, 1:]\nprint(two_rows_cols)\n</pre> two_rows_cols = arr2d[:2, 1:] print(two_rows_cols) <p>It is important to remember that subsetting a NumPy <code>ndarray</code> using index locations and slicing only considers the location of elements within an array. You will need to consider and understand how locations within an array relate to \"real world\" dimensions such as geographic location or time.</p> In\u00a0[\u00a0]: Copied! <pre>s2_summer_path = os.path.join(data_path, \"week-2-s2-summer-2020.tif\")\nrds = rxr.open_rasterio(s2_summer_path)\n</pre> s2_summer_path = os.path.join(data_path, \"week-2-s2-summer-2020.tif\") rds = rxr.open_rasterio(s2_summer_path) <p>We have used <code>rioxarray</code> to read raster data stored in a GeoTIFF file into our program as an <code>xarray.DataArray</code> object referenced by the variable <code>rds</code>. We can print the <code>xarray.DataArray</code> object to inspect its metadata.</p> In\u00a0[\u00a0]: Copied! <pre>rds\n</pre> rds <p>The raster data stored in the GeoTIFF file is a satellite image of a field in the Wheatbelt. The data is captured by the Sentinel-2 satellite and it measures reflectance in many spectral bands (e.g. blue light, green light, red light, and near infrared light ...). This raster dataset has 23 bands, including some ancillary bands providing information about image quality and atmospheric conditions when the image was captured (we want to know if clouds are obscuring the satellite's view of the land surface).</p> <p>The attributes property of <code>rds</code> stores geospatial metadata such as the coordinate reference system (CRS) and the extent of the dataset. We can access this information via the <code>rds</code> object's <code>rio</code> accessor</p> In\u00a0[\u00a0]: Copied! <pre>print('CRS:', rds.rio.crs)\nprint('Resolution:', rds.rio.resolution())\nprint('Bounds:', rds.rio.bounds())\nprint('Width:', rds.rio.width)\nprint('Height:', rds.rio.height)\n</pre> print('CRS:', rds.rio.crs) print('Resolution:', rds.rio.resolution()) print('Bounds:', rds.rio.bounds()) print('Width:', rds.rio.width) print('Height:', rds.rio.height) <p>The raster data values are stored as arrays within the <code>xarray.DataArray</code> object and can be accessed via the <code>values</code> property.</p> In\u00a0[\u00a0]: Copied! <pre>arr = rds.values\nprint(f\"the shape of the array is {arr.shape}\")\narr\n</pre> arr = rds.values print(f\"the shape of the array is {arr.shape}\") arr <p>In subsequent labs we will explore <code>xarray</code> and raster data formats in more detail. For now, let's focus on visualising raster data stored in <code>xarray.DataArray</code> objects.</p> In\u00a0[\u00a0]: Copied! <pre>rds.sel(band=4).plot.imshow()\n</pre> rds.sel(band=4).plot.imshow() <p>The default visualisation using <code>plot.imshow()</code> returns a static image with <code>xarray.DataArray</code> labels and coordinates plotted. The default colour palette is viridis (yellow-green-blue shades). However, we can change this to a colour palette that relates to red reflectance using the <code>cmap</code> parameter of the <code>plot.imshow()</code> method. Let's use the <code>\"Reds\"</code> colour palette here.</p> In\u00a0[\u00a0]: Copied! <pre>rds.sel(band=4).plot.imshow(cmap=\"Reds\")\n</pre> rds.sel(band=4).plot.imshow(cmap=\"Reds\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>rds.sel(band=2).plot.imshow(cmap=\"Blues\")\n</pre> <p>The plots that you have generated so far have the <code>xarray.DataArray</code> labels attached to them (e.g. plot titles and all the band names listed by the colourbar). Can you look at the different parameters of the <code>plot.imshow()</code> and identify which parameter we can use to stop labels being rendered? Generate a plot of green reflectance (band 3) without labels and using the <code>\"Greens\"</code> colour palette.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>Here, we need to use the <code>add_labels</code> parameter and pass in False as an argument.</p> <pre>rds.sel(band=3).plot.imshow(cmap=\"Greens\", add_labels=False)\n</pre> In\u00a0[\u00a0]: Copied! <pre>rds.sel(band=[4, 3, 2]).plot.imshow(vmin=0, vmax=0.4, add_labels=False)\n</pre> rds.sel(band=[4, 3, 2]).plot.imshow(vmin=0, vmax=0.4, add_labels=False) <p>Using the Plotly Express <code>imshow()</code> function we can create interactive visualisations of raster data stored in <code>xarray.DataArray</code> objects.</p> <p>The <code>px.imshow()</code> function takes in a 2D or 3D (for RGB images) NumPy <code>ndarray</code> as its first argument. The <code>values</code> of a <code>xarray.DataArray</code> are stored in NumPy <code>ndarray</code> objects, so we can select a band and pass it into <code>px.imshow()</code>. Let's visualise red band reflectance as an interactive image (hovering your cursor over the image will return a text popup with the red reflectance value at that location).</p> In\u00a0[\u00a0]: Copied! <pre>px.imshow(rds.sel(band=4).values)\n</pre> px.imshow(rds.sel(band=4).values) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>The <code>height</code> parameter lets you set the figure height in pixels.</p> <p>The <code>color_continuous_scale</code> parameter lets you set the colour palette.</p> <pre>px.imshow(rds.sel(band=4).values, color_continuous_scale=\"Reds\", height=500)\n</pre> In\u00a0[\u00a0]: Copied! <pre>s2_gndvi_path = os.path.join(data_path, \"gndvi_2020_bf66_fitted.tif\")\ngndvi_rds = rxr.open_rasterio(s2_gndvi_path)\n</pre> s2_gndvi_path = os.path.join(data_path, \"gndvi_2020_bf66_fitted.tif\") gndvi_rds = rxr.open_rasterio(s2_gndvi_path) In\u00a0[\u00a0]: Copied! <pre>gndvi_rds\n</pre> gndvi_rds <p>If we inspect the dataset, we can see that it has 52 bands. Each band is an array of GNDVI corresponding to a week of the year. In 2020, canola was grown in this field.</p> <p>In a previous lab we introduced the concept of a subplot. This is a good use case for a faceted figure. We can represent each week as a subplot, align the subplots sequentially through time, and use the same mapping of data values to colour to make comparisons of greenness across weeks easy.</p> <p>The <code>xarray.DataArray</code> <code>plot.imshow()</code> method has <code>col</code> and a <code>col_wrap</code> parameters. The <code>col</code> parameter can be passed a <code>dim</code> for which the faceted subplots are created. Passing in the <code>\"band\"</code> <code>dim</code> here will generate a separate image for each of the arrays along the dimension specified. Here, that will create a new GNDVI image for each week. The <code>col_wrap</code> parameter specifies how many images are laid out along one row on the display.</p> In\u00a0[\u00a0]: Copied! <pre>gndvi_rds.plot.imshow(col=\"band\", col_wrap=10)\n</pre> gndvi_rds.plot.imshow(col=\"band\", col_wrap=10) <p>You should be able to see the green up and green down dynamics of vegetative crop growth in this facet plot.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_ndyi_path = os.path.join(data_path, \"ndyi_2020_bf66_fitted.tif\")\nndyi_rds = rxr.open_rasterio(s2_ndyi_path)\ngndvi_rds.plot.imshow(col=\"band\", col_wrap=10, cmap=\"afmhot\")\n</pre> In\u00a0[\u00a0]: Copied! <pre>elev_gdf_path = os.path.join(os.getcwd(), \"data_lab-2_2\", \"week-2-bf66-elevation.geojson\")\nelev_gdf = gpd.read_file(elev_gdf_path)\nelev_gdf.head()\n</pre> elev_gdf_path = os.path.join(os.getcwd(), \"data_lab-2_2\", \"week-2-bf66-elevation.geojson\") elev_gdf = gpd.read_file(elev_gdf_path) elev_gdf.head() <p>Printing out the <code>head()</code> of the <code>GeoDataFrame</code> <code>elev_gdf</code> clearly illustrates the tabular structure for representing vector data. Attributes are stored in columns, the locational information which is a <code>POINT</code> geometry object is stored in a <code>geometry</code> column, and each row corresponds to one geographic feature.</p> In\u00a0[\u00a0]: Copied! <pre>elev_gdf.explore()\n</pre> elev_gdf.explore() <p>This clearly shows the location of points within the field. However, it is not very informative about each point's elevation value. We can change the colour of each point to represent the variability in elevation across the field. To do this we need to use the <code>column</code> paramter of the <code>explore()</code> method to specify the column in the <code>GeoDataFrame</code> that we wish to represent using colour. We can also specifiy a colour palette to use with the <code>cmap</code> parameter.</p> <p>Executing the following code will render the elevation data with low elevations in blue shades and higher locations in yellow shades.</p> In\u00a0[\u00a0]: Copied! <pre>elev_gdf.explore(column=\"Elevation\", cmap=\"cividis\")\n</pre> elev_gdf.explore(column=\"Elevation\", cmap=\"cividis\") In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020 = gpd.read_file(os.path.join(data_path, \"fao_wheat_crop_yield_2020.geojson\"))\n</pre> gdf_wheat_yield_2020 = gpd.read_file(os.path.join(data_path, \"fao_wheat_crop_yield_2020.geojson\")) In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020.head()\n</pre> gdf_wheat_yield_2020.head() <p>We'll get a simple chloropleth map with the default viridis colour palette if we pass in the column label for wheat yield <code>\"yield_100g_ha\"</code> as an argument to the <code>column</code> parameter.</p> In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\")\n</pre> gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\") <p>You might find that the thick borders for country outlines obscures spotting spatial patterns and trends in wheat crop yields. Let's remove the borders. If you look at the docs for <code>explore()</code> you will see there is a <code>style_kwds</code> parameter we can pass a <code>dict</code> of styling configurations.</p> <p>A <code>dict</code> is a data structure with key:pairs. The <code>dict</code> passed to <code>style_kwds</code> contains a keys that descibe a visual element of the display we wish to adjust and a value that specifies how it should be adjusted. For example, the <code>stroke</code> key determines how the border of geometric features on the map is represented. We can set this to <code>False</code> to remove the border. Note, <code>dict</code> objects are specified by enclosing key:value pairs in braces <code>{}</code>.</p> <pre>{\"stroke\": False}\n</pre> In\u00a0[\u00a0]: Copied! <pre>gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\", style_kwds={\"stroke\": False})\n</pre> gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\", style_kwds={\"stroke\": False}) In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>gdf_wheat_yield_2020.explore(column=\"yield_100g_ha\", style_kwds={\"stroke\": False, \"fillOpacity\": 0.75})\n</pre> In\u00a0[\u00a0]: Copied! <pre>clum_carnarvon_path = os.path.join(data_path, \"clum_land_use_carnarvon.geojson\")\nclum_gdf = gpd.read_file(clum_carnarvon_path)\n</pre> clum_carnarvon_path = os.path.join(data_path, \"clum_land_use_carnarvon.geojson\") clum_gdf = gpd.read_file(clum_carnarvon_path) In\u00a0[\u00a0]: Copied! <pre>clum_gdf.head()\n</pre> clum_gdf.head() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>clum_gdf.explore(column=\"Commod_dsc\", categorical=\"set2\")\n</pre>"},{"location":"notebooks/week-2_2/#geospatial-data-visualisation","title":"Geospatial data visualisation\u00b6","text":"<p>This lab will demonstrate how to generate exploratory and interactive visualisations of geospatial data including multispectral satellite images, point-based samples with in a field, global maps of crop yield, and crop types in orchards and plantations. This lab will provide an introduction to:</p> <ul> <li>raster and vector geospatial data</li> <li>generating interactive visualisations of vector datasets</li> <li>generating static and interactive visualisations of raster data</li> </ul> <p>This week the focus will be on quick, exploratory, and interactive visualisations of geospatial data. For a more detailed discussion of visualising geospatial data please read Wilke (2019). For a focus on generating cartographic quality static maps please see the open course Mapping and Data Visualization with Python.</p>"},{"location":"notebooks/week-2_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-2_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-2_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-2_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-2_2/#geospatial-data","title":"Geospatial data\u00b6","text":"<p>Geospatial data represents geographic features or phenomenon as data in computer program or file.</p> <p>There are two main types of geospatial data:</p> <ul> <li>vector data - point, line, or polygon geometries</li> <li>raster data - images and arrays</li> </ul> <p>There are two components to geospatial data:</p> <ul> <li>Positional information describing location, shape, and extent (e.g. an <code>(x, y)</code> coordinate pair representing the location of a weather station)</li> <li>Attribute information describing characteristics of the phenomenon or entity (e.g. a name:value pair recording the name of the weather station <code>name:'Perth Airport'</code>)</li> </ul>"},{"location":"notebooks/week-2_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-2_2/#raster-data-in-python","title":"Raster data in Python\u00b6","text":""},{"location":"notebooks/week-2_2/#raster-data-model","title":"Raster data model\u00b6","text":"<p>Raster data breaks the Earth's surface up into a grid of cells (pixels). Each pixel is assigned a value that corresponds to the geographic feature or phenomenon of interest. For example, pixels in a raster precipitation dataset would be assigned a numeric value that represents the amount of precipitation that fell at that location. Pixels in a land cover map would have an integer value that corresponds to a land cover class label. The values assigned to pixels in a raster dataset are the attribute information.</p> <p></p> <p>The size of the pixels relative to their position on the Earth's surface determines the spatial detail that can be resolved. A land cover map with pixels that represent a 1 km x 1 km portion of the Earth's surface will not be able to identify features such as individual buildings.</p> <p>The figure below shows the 2018 European Space Agency (ESA) Climate Change Initiative (CCI) land cover map. Each pixel represents a 300 m x 300 m area on the Earth\u2019s land surface and a pixel can only represent a single land cover type. If you look at the bottom two zoomed in maps you can see some limitations of representing land cover using 300 m x 300 m spatial resolution pixels. The shape of land cover features are poorly represented by the \u201cblock-like\u201d arrangement of pixels and there is variation in land cover within a single pixel (a mixed pixel problem).</p> <p></p> <p>Raster data represents geographic features and variables (e.g. elevation, reflectance in UAV images) as a grid of values (pixels). In Python, a data structure called an array is used to store and organise pixels in a raster dataset. Typically, NumPy <code>ndarray</code> objects are used for storing raster data in Python programs.</p>"},{"location":"notebooks/week-2_2/#arrays-numpy-ndarrays","title":"Arrays: NumPy <code>ndarray</code>s\u00b6","text":"<p>NumPy is a library used for scientific and numerical computing and is based around an N-dimensional <code>ndarray</code> object. An <code>ndarray</code> is a grid of elements of the same data type. The dimensions of a NumPy <code>ndarray</code> are called axes. NumPy <code>array</code>s can be created from sequences of values (e.g. stored in lists, tuples, other <code>ndarray</code>s).</p> <p></p> <p>We can create a simple 2-dimensional <code>ndarray</code> using the <code>array()</code> function. A <code>ndarray</code> with 2-dimensions is a matrix with rows arranged on the 0 axis and columns arranged on the 1 axis.</p>"},{"location":"notebooks/week-2_2/#subsetting-numpy-ndarrays","title":"Subsetting NumPy ndarrays\u00b6","text":"<p>A subsetting opertation is when you select values from a NumPy <code>ndarray</code> object based on their index locations. These operations are generally referred to as indexing and slicing when working with NumPy <code>ndarray</code> objects.</p> <p></p> <p>We can extract a value from a NumPy <code>ndarray</code> based on its index location. For example, the first element of a 2-Dimensional <code>ndarray</code> is at location <code>[0, 0]</code> (i.e. the 0th row and 0th column).</p>"},{"location":"notebooks/week-2_2/#xarray","title":"Xarray\u00b6","text":"<p>Xarray is a Python package that builds on top of NumPy's array-based data structures, but provides extra tools and functions that are useful for working with geospatial and Earth Science datasets. For example, <code>xarray.DataArray</code> data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values.</p> <p><code>xarray</code> also provides convenient functions for reading raster data from geospatial data files on disk into memory as <code>xarray.DataArray</code> objects which we can use in our Python programs while retaining geographic and temporal information about the raster values stored in the array.</p> <p>Specifically, while a NumPy <code>ndarray</code> stores just the raster values and has some properties such as the <code>shape</code> (number of elements along each axis) and <code>ndim</code> (the dimensions of the array) it does not explicitly store any geospatial, temporal, or other geographic metadata. <code>xarray</code> solves this problem by reading raster data into an <code>xarray.DataArray</code> object with:</p> <ul> <li><code>values</code>: the multidimensional array of raster values</li> <li><code>dims</code>: a list of names for the dimensions of the array (e.g. instead axis 0 describing the 0th (row) dimension of an array that dimension can have a descriptive label such as longitude)</li> <li><code>coordinates</code>: a <code>list</code> of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)</li> <li><code>attrs</code>: a <code>dict</code> of metadata attributes describing the dataset</li> </ul> <p><code>xarray.DataArray</code> objects can be stored within a larger container called <code>xarray.Dataset</code>. An <code>xarray.Dataset</code> can store many <code>xarray.DataArray</code> objects that share <code>dims</code> and <code>coordinates</code>. This is useful if you have different arrays of different <code>Variables</code> that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single <code>xarray.Dataset</code>).</p> <p></p> <p>Why is <code>xarray</code> useful for geospatial data?</p> <ul> <li>The <code>dims</code> and <code>coordinates</code> of an <code>xarray.DataArray</code> mean we can subset values from an array using latitude, longitude, time, or whatever a coordinate describes; we're not just restricted to subsetting values based on their index location within an array</li> <li><code>xarray.Dataset</code> objects provide a container to store multidimensional arrays (e.g. many variables and time points) that are common in geography, Earth Sciences, meteorology, and agriculture. For example, multispectral satellite images of the same location over time; arrays of different meteorological variables)</li> <li>useful functions for reading, analysing and visualising raster or array-like geospatial data that are common across many spatial data science workflows</li> </ul>"},{"location":"notebooks/week-2_2/#data-input","title":"Data input\u00b6","text":"<p>The <code>rioxarray</code> package provides tools for reading and writing raster geospatial data files into <code>xarray.DataArray</code> objects.</p> <p>Let's pass the path to a GeoTIFF file of raster data into the <code>rioxarray</code> <code>open_rasterio()</code> function:</p>"},{"location":"notebooks/week-2_2/#static-images","title":"Static images\u00b6","text":"<p><code>xarray.DataArray</code> objects have a <code>plot.imshow()</code> method that will render array based data as an image. Band 4 of the <code>xarray.DataArray</code> stores reflectance of red light off the Earth's surface. Let's plot it:</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>The blue band (storing reflectance of blue light) in Sentinel-2 images is band 2 in the <code>xarray.DataArray</code> object referenced by <code>rds</code>. Can you plot blue band reflectance and select a sensible colour palette for visualing the spatial variation in blue reflectance?</p> <p>Hint: you can find a list of colour palettes here.</p>"},{"location":"notebooks/week-2_2/#colour-and-composite-images","title":"Colour and composite images\u00b6","text":"<p>A particular colour is defined by the intensity of light in different parts of the visible spectrum (e.g. yellow is a mixture of light in red and green wavelengths).</p> <p>Colour is represented by combinations (addition) of red, green, and blue light. Red, green, and blue are primary colours and combine to form white. An absence of red, green, and blue is black. Secondary colours can be formed by the addition of primary colours of varying intensities (e.g. yellow is the addition of red and green, magenta is the addition of red and blue, and cyan is the addition of green and blue).</p> <p>Computer displays consist of red, green, and blue sub-pixels, which when activated with different intensities, are perceived as different colours. The range of colours that can be displayed on a computer display is called the gamut. Colour in computer programs is represented as a three byte hexadecimal number with byte 1 corresponding to red, byte 2 corresponding to green, and byte 3 corresponding to blue. Each byte can take the range of 0 to 255 in decimal. 0 indicates the absence of a colour and 255 indicates saturation of that colour:</p> <ul> <li>white: 255 255 255</li> <li>black: 0 0 0</li> <li>red: 255 0 0</li> <li>green: 0 255 00</li> <li>blue: 0 0 255</li> </ul> <p></p> <p>Computer displays represent colour through varying the intensity of sub-pixel displays of red, green, and blue light. Variability in data values in multiband rasters can be visualised by relating data values in one band to the intensity of one of the primary colours on the computer display. Visualising a multiband raster in this way creates an additive RGB or colour composite image - it is called a composite image because each pixel is a composite of red, green, and blue light.</p> <p>Above we rendered the red, green, and blue band reflectance from the Sentinel-2 image separately. However, if we combine these reflectance measures into a composite image (e.g. where red reflectance is represented by sub-pixel intensity of red light) we can create a true colour image as if we were looking down on the Earth's surface with our eyes.</p> <p>We can select multiple bands from the <code>xarray.DataArray</code> object that correspond to red, green, and blue reflectance to render a true colour image. We need to pass a <code>list</code> of bands into the <code>sel()</code> method of the <code>xarray.DataArray</code> object referenced by the variable <code>rds</code>.</p>"},{"location":"notebooks/week-2_2/#interactive-raster-visualisations","title":"Interactive raster visualisations\u00b6","text":""},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Above, we have an interactive image of red band reflectance. However, the image is quite small and the colour palette could be more intuitive to indicate it's an image of red light reflectance. Can you look at the parameters in the <code>px.imshow()</code> docs that we could use to visualise reflectance using a red colour palette and increase the height of the plot?</p>"},{"location":"notebooks/week-2_2/#plotting-raster-time-series","title":"Plotting raster time series\u00b6","text":"<p>In many cases, we'll have raster data that covers the same location on the Earth's surface but is captured on different dates. A good example of this is remotely sensed satellite data which captures spectral reflectance data for the same location each time the satellite overpasses a location. In this instance, each band (or 2D array) might represent an observation for a variable on different dates. For example, green normalised difference vegetation index (GNDVI) values use green and near infrared reflectance to represent the greenness of a location. We can visualise GNDVI through time to represent vegetation growth dynamics (e.g. the green up of a crop after planting).</p> <p>Let's read in a raster dataset of GNDVI values for different dates.</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>There is a GeoTIFF file <code>ndyi_2020_bf66_fitted.tif</code> in the <code>data_lab-2_2</code> folder. It stores normalised difference yellowness index (NDYI) values for each week of the year. Can you read this file into your program and plot each week's NDYI values as an image to visualise change in yellowness of the canola crop canopy through the growing season? Generate this figure as a faceted plot and select a suitable colour map to visualise change in yellowness.</p> <p>Hint: you can find a list of colour palettes here.</p>"},{"location":"notebooks/week-2_2/#vector-data-in-python","title":"Vector data in Python\u00b6","text":""},{"location":"notebooks/week-2_2/#vector-data-model","title":"Vector data model\u00b6","text":"<p>Vector data uses point, line, or polygon geometries to represent geographic features.</p> <p>Coordinate pairs: point locations or the vertices in lines and polygons are represented using coordinate pairs. The coordinate pairs indicate where that feature is located on the Earth's surface (relative to an origin); longitude and latitute are commonly used as coordinate pairs.</p> <p>Attribute information: vector data also stores non-spatial attribute information which describe characteristics of the geographic phenomenon or entity represented by the geometry feature.</p> <p></p>"},{"location":"notebooks/week-2_2/#geopandas-geodataframe","title":"GeoPandas GeoDataFrame\u00b6","text":"<p>A GeoPandas <code>GeoDataFrame</code> is a tabular data structure for storing vector geospatial data and is based on a regular pandas <code>DataFrame</code>.</p> <p>A <code>GeoDataFrame</code> consists of columns of non-spatial attributes similar to a pandas <code>DataFrame</code>. However, a <code>GeoDataFrame</code> also has a <code>geometry</code> column which is a <code>GeoSeries</code> of geometries for the spatial data associated with each row.</p> <p>In Python, geometries are represented as Shapely <code>Geometry</code> objects. The <code>geometry</code> column in a GeoPandas <code>GeoDataFrame</code> is a <code>Series</code> of Shapely <code>Geometry</code> objects. Printing a Shapely <code>Geometry</code> object returns a Well Known Text (WKT) string description of the geometry (e.g. <code>POINT (0, 1)</code>). The <code>geometry</code> column of a <code>GeoDataFrame</code> (or a <code>GeoSeries</code>) can be viewed as a sequence of Shapely <code>Geometry</code> objects:</p> <pre><code>a_geoseries = [POINT (0, 1), POINT (0, 2), POINT (2, 3)]\n</code></pre> <p>Shapely provides tools for representing geometries in Python programs. It does not provide tools for reading geometry data from disk or handling attribute data. GeoPandas <code>GeoDataFrame</code> and <code>GeoSeries</code> combine Shapely's functionality for handling geometries with tools for reading and writing vector data, handling attributes, and visualisation. Therefore, we will focus on using <code>GeoDataFrame</code>s in these labs.</p> <p>Let's read in a GeoJSON file storing the elevation of points sampled across the same field in Western Australia that we have been exploring using raster data.</p>"},{"location":"notebooks/week-2_2/#interactive-vector-visualisations","title":"Interactive vector visualisations\u00b6","text":"<p><code>GeoDataFrame</code>s have a helpful <code>explore()</code> method for rendering spatial data on a \"slippy\" web map.</p>"},{"location":"notebooks/week-2_2/#chloropleth-mapping","title":"Chloropleth mapping\u00b6","text":"<p>Chloropleth maps use a feature's fill colour to visualise spatial variation in a variable. A continuous colour palette is used to represent variation in the values of attributes of a vector spatial dataset. For a more detailed review of chloropleth maps please see Rey et al. (2020) and Wilke (2019).</p> <p>Let's create a chloropleth map of wheat crop yields at the national level in 2020 downloaded from FAOSTAT.</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>The opacity of the fill colour on the chloropleth maps is set to 0.5. Can you find a parameter in the  <code>explore()</code> docs to change the <code>fillOpacity</code> to 0.75?</p> <p>Hint: you can use search tools and find tools to find words on the <code>explore()</code> docs page.</p>"},{"location":"notebooks/week-2_2/#categorical-vector-maps","title":"Categorical vector maps\u00b6","text":"<p>Sometimes the variable that we wish to visualise on a map using colour is not continuous but is categorical. An example could be the crop type associated with a polygon feature of field boundaries. In these cases we're mapping a categorical value to a colour and a change in colour does not represent an increase or decrease in a numeric value, but a change in group or class.</p> <p>In these cases qualitative (or discrete) colour scales should be used to represent groups (i.e. data where there is no logical ordering). Thus, qualitative colour scales should not represent gradients of light to dark or use colours that can be interpreted as having an implied ordering. Often, it is sensible to select colours that relate to the category (e.g. on land cover maps using green for vegetated categories, blue for water etc.).</p> <p>Let's make a categorical map of the crop type in a field for a selection of fields near Canarvon in Western Australia. The data is derived from the Catchment scale land use of Australia product.</p>"},{"location":"notebooks/week-2_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you make a categorical interactive map of the crop type for each field? You will need to use the <code>explore()</code> docs to identify which parameter to use to let <code>explore()</code> know it is visualising categorical data. You should pass in a qualitative colour palette as an argument to the this parameter. A list of qualitative colour palettes can be found here.</p>"},{"location":"notebooks/week-3_1/","title":"Week 3 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-3_1\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-3_1\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-3_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-3_1\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-3_1.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-3_1\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-3_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport plotly.express as px\nimport plotly.io as pio\nimport pprint\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n</pre> import pandas as pd import numpy as np import json import os import plotly.express as px import plotly.io as pio import pprint  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\" In\u00a0[\u00a0]: Copied! <pre># path to csv file\nelev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")\nprint(\"path to elevation csv file:\", elev_csv_path)\n\n# open the file and read the first 1000 characters\nf = open(elev_csv_path, \"r\", encoding=\"utf-8\")\ndata = f.read(1000)\nf.close()\n\nprint(\"\")\nprint(\"The first 1000 characters of the csv file area:\")\nprint(data)\n</pre> # path to csv file elev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\") print(\"path to elevation csv file:\", elev_csv_path)  # open the file and read the first 1000 characters f = open(elev_csv_path, \"r\", encoding=\"utf-8\") data = f.read(1000) f.close()  print(\"\") print(\"The first 1000 characters of the csv file area:\") print(data) In\u00a0[\u00a0]: Copied! <pre>elev_csv_path = os.path.join(os.getcwd(), \"week-2\", \"week-2-bf66-elevation.csv\")\nfile_stats = os.stat(elev_csv_path)\nprint(\"File Size in Bytes is:\",  file_stats.st_size)\n</pre> elev_csv_path = os.path.join(os.getcwd(), \"week-2\", \"week-2-bf66-elevation.csv\") file_stats = os.stat(elev_csv_path) print(\"File Size in Bytes is:\",  file_stats.st_size) In\u00a0[\u00a0]: Copied! <pre>os.getcwd()\n</pre> os.getcwd() <p>We can get a list of files and sub-directories within the current working directory by calling the <code>os.listdir()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>os.listdir()\n</pre> os.listdir() <p>A path describes the location of a file within the computer system's directory structure. We can create paths to files using the <code>os.path.join()</code> function. We pass in string data representing sub-directories and filenames and the <code>os.path.join()</code> function creates a file path.</p> <p>Let's get the file path for this notebook file: <code>week-3_1.ipynb</code>.</p> In\u00a0[\u00a0]: Copied! <pre>os.path.join(os.getcwd(), \"week-3_1.ipynb\")\n</pre> os.path.join(os.getcwd(), \"week-3_1.ipynb\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>print(os.path.join(os.getcwd(), \"data_lab-3-1\", \"week-3-bf66-wheat-yield-max-vi_sampled.geojson\"))\n</pre> <p>Is <code>data_lab-3_1</code> a directory or a file?</p> answer <p><code>data_lab-3_1</code> is a directory which can store files or sub-directories within the file system. You can print out the list of <code>data_lab-3_1</code>'s contents using the <code>os.listdir(os.path.join(os.getcwd(), \"data_lab-3_1\"))</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># open a connection to a csv file\n# path to csv file\nelev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")\n\n# open the file \nf = open(elev_csv_path, \"r\", encoding=\"utf-8\")\n</pre> # open a connection to a csv file # path to csv file elev_csv_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\")  # open the file  f = open(elev_csv_path, \"r\", encoding=\"utf-8\") <p>We've opened a connection to a CSV file in read mode <code>\"r\"</code> and with a <code>utf-8</code> encoding. This has returned to us <code>f</code>, a variable pointing to a <code>file</code> object.</p> <p>What kind of object is <code>f</code> pointing to?</p> In\u00a0[\u00a0]: Copied! <pre>type(f)\n</pre> type(f) <p>Variable <code>f</code> is pointing at a <code>file</code> object, or, more specifically, an <code>_io.TextIOWrapper</code> type object. The <code>IO</code> stands for input / output which is another way of saying reading and writing data. The <code>Text</code> refers to the fact that <code>f</code> will be able to read data to string Python objects or write data from string Python objects to binary in files (i.e. it's reading in text mode). The conversion from string to binary and binary to string is determined by the encoding (e.g. UTF-8 or ASCII).</p> <p>The <code>file</code> object <code>f</code> is a connection to a file and it provides the methods to read (write) data from (to) this file via the connection.</p> In\u00a0[\u00a0]: Copied! <pre># read first 1000 characters from the csv file connected to by f\nchars_1000 = f.read(1000)\nprint(chars_1000)\n</pre> # read first 1000 characters from the csv file connected to by f chars_1000 = f.read(1000) print(chars_1000) In\u00a0[\u00a0]: Copied! <pre># read 10 lines of the csv file \nfor i in range(0, 10):\n    print(f.readline())\n</pre> # read 10 lines of the csv file  for i in range(0, 10):     print(f.readline()) <p>You might notice that the call to <code>readline()</code> does not start with the row of column headers. This is because the <code>file</code> object <code>f</code> keeps a record of a position in the file up to where it has read bytes from. The call to <code>readline()</code> will start reading lines of characters from the file where the previous call to <code>read(1000)</code> finished.</p> <p>We can use the <code>tell()</code> method to see a <code>file</code> object's current position in a file.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"current position in the file is {f.tell()}\")\n</pre> print(f\"current position in the file is {f.tell()}\") <p>The reason that <code>read()</code> or <code>readline()</code> provide the option to read data in <code>n</code> characters at a time or line by line is to help you avoid reading in more data than can fit in your computer's memory. If you had a large file and called <code>read()</code> on it, without specifying the number of characters to read, it could fill up your memory.</p> <p>As we're reading from the file in text mode, the binary data from the file should be converted to Python string objects. We can check this.</p> In\u00a0[\u00a0]: Copied! <pre>type(chars_1000)\n</pre> type(chars_1000) In\u00a0[\u00a0]: Copied! <pre>f2 = open(\"write_demo.csv\", \"w\", encoding=\"utf-8\")\n</pre> f2 = open(\"write_demo.csv\", \"w\", encoding=\"utf-8\") <p>Now, we can call the <code>write()</code> method of the <code>file</code> object <code>f2</code> and pass <code>write()</code> the variable <code>chars_1000</code> which points to 1000 characters stored as a string object. If the write is successful, it should return a number telling us how many characters were written (it should be 1000) and you should be able to see the file write_demo.csv in your current working directory.</p> In\u00a0[\u00a0]: Copied! <pre>f2.write(chars_1000)\n</pre> f2.write(chars_1000) In\u00a0[\u00a0]: Copied! <pre>f.close()\nf2.close()\n</pre> f.close() f2.close() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>f = open(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-terra-precip-monthly.csv\"), \"r\")\n         \nfor line in range(0, 20):\n    print(f.readline())\n\nf.close()\n</pre> In\u00a0[\u00a0]: Copied! <pre>png_file_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-low-res.png\")\n\nwith open(png_file_path, \"rb\") as f:\n    print(f.read(1)) # read and print the first byte\n    print(f.read(3)) # read and print the second, third, and fourth bytes\n</pre> png_file_path = os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-low-res.png\")  with open(png_file_path, \"rb\") as f:     print(f.read(1)) # read and print the first byte     print(f.read(3)) # read and print the second, third, and fourth bytes <p>You will note that we opened a connection to the PNG file in <code>rb</code> mode. This indicates we are reading data in binary mode. Here, instead of reading in the data a character at a time (as we did when reading data in text mode) we are reading in <code>n</code> bytes of data.</p> <p>You will also note when printing the binary data that is read from the PNG file there is a <code>b</code> in front of the text. This indicates the data being printed is of bytes type.</p> <p>Finally, you will notice that we did not need to <code>close()</code> the file connection <code>f</code> as this is handled for us by working within the context of the <code>with</code> statement.</p> <p>This is what the data in the PNG file we have just been reading looks like.</p> <p></p> <p>Imagery (c) 2022 CNES | Airbus, Imagery (c) 2022 | Airbus, Landsat | Copernicus, Maxar Technologies, Map Data (c) 2022</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>with open(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-elevation.csv\"), \"r\") as f:\n    print(f.read())\n</pre> In\u00a0[\u00a0]: Copied! <pre>canola_yield_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\"))\nprint(f\"The shape of the canola yield DataFrame is {canola_yield_df.shape}\")\n</pre> canola_yield_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\")) print(f\"The shape of the canola yield DataFrame is {canola_yield_df.shape}\") In\u00a0[\u00a0]: Copied! <pre>canola_yield_df.head()\n</pre> canola_yield_df.head() <p>Pandas provides a range of convenient functions for reading and writing data - you can find a list of them here.</p> In\u00a0[\u00a0]: Copied! <pre>%%HTML\n&lt;div style=\"padding:110.6% 0 0 0;position:relative;\"&gt;&lt;iframe src=\"https://player.vimeo.com/video/911479339?badge=0&amp;amp;autopause=0&amp;amp;player_id=0&amp;amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"faostat-csv\"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;script src=\"https://player.vimeo.com/api/player.js\"&gt;&lt;/script&gt;\n</pre> %%HTML  In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>You will need to change the filename to match the file that you have downloaded.</p> <pre>df_faostat = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_1\", \"FAOSTAT_data_en_2-9-2024.csv\"))\ndf_faostat.head()\n</pre> In\u00a0[\u00a0]: Copied! <pre>canola_yield_df.to_parquet(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\"))\n</pre> canola_yield_df.to_parquet(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\")) <p>Let's compare the size of the CSV file storing the canola yield data and the parquet file we just saved. The parquet file should be much smaller.</p> In\u00a0[\u00a0]: Copied! <pre>file_stats_csv = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\"))\nprint(\"CSV file size in bytes is:\",  file_stats_csv.st_size)\nfile_stats_parquet = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\"))\nprint(\"Parquet file size in bytes is:\",  file_stats_parquet.st_size)\n</pre> file_stats_csv = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.csv\")) print(\"CSV file size in bytes is:\",  file_stats_csv.st_size) file_stats_parquet = os.stat(os.path.join(os.getcwd(), \"data_lab-3_1\", \"week-3-bf66-canola-yield.parquet\")) print(\"Parquet file size in bytes is:\",  file_stats_parquet.st_size)"},{"location":"notebooks/week-3_1/#data-io-and-file-formats","title":"Data I/O and file formats\u00b6","text":"<p>In this lab we will write Python programs that can read crop yield data collected from harvesters and satellite images of the same field into data structures that we can analyse and visualise.</p> <p>A solid understanding of how to read and write different types of data from and to files is a key skill for data analysis. This week's lab will build these skills and provide an introduction to:</p> <ul> <li>files, directories, and data storage</li> <li>reading and writing files in Python</li> <li>tabular, image, and geospatial file formats</li> <li>specialist file formats for the web, big data, and cloud computing</li> <li>selecting different data formats for specific analysis or storage tasks</li> </ul>"},{"location":"notebooks/week-3_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-3_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-3_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-3_1/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-3_1/#data-io","title":"Data I/O\u00b6","text":"<p>Data analysis tasks involve reading data stored in files on disks, servers in the cloud, or recorded by sensors. Also, we need to save the results of our analysis or datasets we have generated to files.</p> <p>There are a range of data types (e.g. string / text, numeric, datetime) and ways of characterising data such as tabular data, images and arrays, and spatial and non-spatial data. This necessitates storing data with different file formats. It's important to be able to read and write data from and to different file formats into and out of Python data structures that we can analyse in our programs.</p> <p>There are costs involved in storing and transferring data. These can be time costs associated with the time taken to read data from disk into the computer's memory or transferring data from one computer to another over a network. Or, they can be financial costs associated with storing the data (the cost of hard drives increases with storage capacity and cloud storage providers charge by the byte).</p> <p>The term big data refers to the increasing volume, variety, and velocity of data. Larger and more diverse datasets are being generated more quickly. To be able to handle big data it is important to select appropriate file formats for efficient storage and reading / writing (or input / output - I/O).</p>"},{"location":"notebooks/week-3_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-3_1/#a-python-program-to-read-a-file","title":"A Python program to read a file\u00b6","text":"<p>Let's start with a simple program to open a CSV file, read some data from it into memory, and then close the connection to the file.</p>"},{"location":"notebooks/week-3_1/#files-and-io","title":"Files and I/O\u00b6","text":""},{"location":"notebooks/week-3_1/#files","title":"Files\u00b6","text":"<p>A file is data that is stored on a disk. Data in files is stored as a sequence bytes in binary format (values of zero or one).</p> <p></p>"},{"location":"notebooks/week-3_1/#binary","title":"Binary\u00b6","text":"<p>A binary number is represented using only the digits 1 or 0. The binary number system is a base-2 number system (as it has only two symbols).</p> <p>The decimal number <code>0</code> in binary is <code>0</code>:</p> <p>$0 = (0 \\cdot 2^{0})$</p> <p>The decimal number <code>1</code> in binary is <code>1</code>:</p> <p>$1 = (1 \\cdot 2^{0})$</p> <p>The decimal number <code>2</code> in binary is <code>10</code>:</p> <p>$2 = (1 \\cdot 2^{1}) + (0 \\cdot 2^{0})$</p> <p>The decimal number <code>3</code> in binary is <code>11</code>:</p> <p>$3 = (1 \\cdot 2^{1}) + (1 \\cdot 2^{0})$</p> <p>The decimal number <code>4</code> in binary is <code>100</code>:</p> <p>$4 = (1 \\cdot 2^{2}) + (0 \\cdot 2^{1}) + (0 \\cdot 2^{0})$</p> <p>You don't need to know the details of the binary number system here, but the key things to take away are:</p> <ul> <li>numbers are stored in binary using the digits 1 or 0</li> <li>larger numbers require more binary digits</li> <li>larger numbers, therefore, require more storage space</li> </ul> <p>There are plenty of resources online to learn more about number systems. This is a short article on The History of Numbers.</p> <p>Bits</p> <p>A single binary digit is a bit. Looking at the pattern above we can store the numbers 0 and 1 using a 1-bit binary number. We can store the numbers 2 and 3 using a 2-bit binary number. We can store the number 4 using a 3-bit binary number. For every extra bit we double the numbers that can be stored in binary.</p> <ul> <li>1 bits = 2 numbers (0 and 1)</li> <li>2 bits = 4 numbers (0 to 3)</li> <li>3 bits = 8 numbers (0 to 7)</li> <li>4 bits = 16 numbers (0 to 15)</li> <li>5 bits = 32 numbers (0 to 31)</li> <li>6 bits = 64 numbers (0 to 63)</li> <li>7 bits = 128 numbers (0 to 128)</li> <li>8 bits = 256 numbers (0 to 255)</li> </ul> <p>Bytes</p> <p>A byte is an 8-bit binary number. With a single byte we can represent 256 different numbers. Computer storage is measured in bytes:</p> <ul> <li>1 Kilobyte (KB) is about 1,000 bytes.</li> <li>1 Megabyte (MB) is about 1,000,000 bytes.</li> <li>1 Gigabyte (GB) is about 1,000,000,000 bytes.</li> </ul> <p>A greyscale image file can store each pixel's colour as an 8-bit number or as a single byte. Black is represented as the number 0 (00000000), white is the number 255 (11111111), and shades of grey are intermediate numbers (00000001 to 11111110).</p> <p>If this greyscale image has 100x100 pixels (10,000 pixels), how many bytes of storage does this image require?</p> <p>$10000 bytes = 10000 pixels \\cdot 1 byte$ as each pixel requires 1 byte of storage.</p> <p>Generally, a common text character such as upper and lower case letters (A-Z, a-z) and symbols (!, @, # etc.) requires a byte of storage. The CSV file we read above stores text characters. We can use the <code>os.stats()</code> function to inspect the stats of this file in storage. Let's pass the path to the <code>week-2-bf66-elevation.csv</code> file into the <code>os.stats()</code> function and see how many bytes are required to store this file.</p>"},{"location":"notebooks/week-3_1/#file-formats","title":"File formats\u00b6","text":"<p>A file format describes how data is encoded as binary sequences in files (sequences of 1 and 0 digits). The filename's extension indicates the file format used (e.g. .jpg is a JPEG file, .tif is a TIFF or GeoTIFF file, .csv is a CSV file). Some file formats also include a header or magic number inside the file that indicates what the file format is. The header can also include some metadata information about the file.</p> <p></p> <p>A Portable Network Graphics (PNG) file is a common file format for storing image data. It is identified by a .png ending and consists of sections of bytes on disk arranged as:</p> <ol> <li>PNG signature including a magic number for the start of the file and a PNG file identifier.</li> <li>Image header with image metadata such as the size of the image.</li> <li>Image data.</li> <li>Image end to indicate the end of the PNG file on disk.</li> </ol> <p></p> <p>Different file formats encode data in different ways, and, thus, have different strengths and weaknesses. Some file formats prioritise efficient storage (compression) of data on disk (small file sizes and quick transfer), other prioritise quick read and writing of data, and others prioritise cross-platform compatibility or interpretation.</p> <p>The various file formats for storing vector geospatial data provide a good illustration of how different formats store the same data but in ways that are better suited for different applications and uses. This is discussed here and here.</p> <p>Many of you will have used shapefiles as a format for storing vector geospatial data. A strength of shapefiles is the range of software applications that can read and write data from and to them. However, they have drawbacks which mean they're not suited to some use cases:</p> <ul> <li>Multifile format which makes data handling harder (you need a .shp, .shx, .prj, and .dbf file).</li> <li>File size is limited to 2 GB which is prohibits their use for storing large datasets.</li> <li>Attribute names are limited to 10 characters which can preclude using descriptive attribute and column names.</li> <li>Each shapefile can only store one type of geometry (point or line or polygon) which prohibits representing complex geographic features in a single file.</li> <li>No null value which can introduce complications for handling missing data.</li> <li>Data types are limited to 256 characters which precludes storing large numbers / text strings.</li> </ul> <p>It is important to be aware of the characteristics of particular file formats and what their limits or benefits mean for your analysis. A simple example of why this is important: we've processed several harvester yield datasets from different fields into a single dataset and want to save this dataset to file. If this dataset is larger than 2GB and we try and save to a shapefile there will be data loss.</p>"},{"location":"notebooks/week-3_1/#directories-and-file-systems","title":"Directories and file systems\u00b6","text":"<p>Files are organised within a hierarchy of directories and sub-directories (or folders) in a computer system. We're working in a Linux environment so the directory hierachy starts at the root denoted by <code>/</code>. Sub-directories are separated by <code>/</code>.</p> <p>A program has a current working directory which is its current location within the directory hierarchy.</p> <p>We can get the current working directory by calling the <code>os.getcwd()</code> function.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>In the directory <code>data_lab-3_1</code> there is a file named <code>week-3-bf66-wheat-yield-max-vi_sampled.geojson</code>. Can you create a file path for this file using <code>os.path.join()</code> and print the path on the display?</p>"},{"location":"notebooks/week-3_1/#files-in-python","title":"Files in Python\u00b6","text":"<p>In Python, it is possible to read and write files in text and binary modes.</p>"},{"location":"notebooks/week-3_1/#text-mode","title":"Text mode\u00b6","text":"<p>Text mode involves reading and writing string data from and to the file.</p> <p>A text file contains encoded characters. ASCII and Unicode are character sets that define how characters (e.g. 1, 2, 3, 66, A, b, !) are encoded in binary (sequences of 1 and 0 digits) in a file.</p> <p>A character set translates characters into numbers and an encoding translates numbers into binary.</p>"},{"location":"notebooks/week-3_1/#ascii","title":"ASCII\u00b6","text":"<p>ASCII stands for the American Standard Code for Information Interchange and has encodings for 128 English characters, numbers, and some special characters. ASCII characters are encoded using 7-bits. You can see the full ASCII character set here.</p> <p>In ASCII, uppercase G is represented by the number 71, uppercase I is represented by the number 73, and uppcase S is represented by the number 83.</p> <p>Thus, GIS in ASCII is written as <code>71 73 83</code> in its numeric representation and <code>01000111 01001001 01010011</code> in its binary representation (i.e. how it would be stored in a file).</p>"},{"location":"notebooks/week-3_1/#unicode","title":"Unicode\u00b6","text":"<p>Unicode is a more modern and comprehensive character set of text symbols covering modern and ancient languages. Common encodings of the Unicode character set are UTF-8 and UTF-16. The Unicode character set includes over 1,000,000 characters and aims to be a universal system for representing and storing text in computer systems.</p> <p>Unicode characters can be encoded in UTF-8 using one to four bytes. More common symbols (e.g. the ASCII character set) are encoded using one byte for efficient storage.</p> <p>The Python docs suggest UTF-8 is the modern de-facto standard so it is often the default encoding or a good one to choose if you are not sure how your data is encoded.</p>"},{"location":"notebooks/week-3_1/#binary-mode","title":"Binary mode\u00b6","text":"<p>Reading files in binary mode does not assume that specific bytes represent human readable characters. When reading files in binary mode, sequences of bytes are read from the file into Python bytes objects in memory. Images such as JPEG files would be read in binary mode.</p> <p>To be clear, both text and binary data is stored in binary format on disks. However, when reading in text data the binary data on disk is converted to text characters based on the encoding scheme used and read into string objects. When reading data in binary mode the binary data is read straight into memory as bytes objects. Reading and writing data in text mode will be slower because of the encoding overhead.</p>"},{"location":"notebooks/week-3_1/#opening-files","title":"Opening files\u00b6","text":"<p>The <code>open()</code> function opens a connection to a file on disk, or creates a new file if it does not exist, and returns a <code>file</code> connection object.</p> <p>Typically, <code>open()</code> is called by specifying a filename, mode, and encoding as arguments: <code>open(filename, mode, encoding)</code>.</p> <ul> <li>filename: the path and filename of the file to be opened.</li> <li>mode: the mode to open the connection to file in. To open files in text mode use <code>r</code> for read only, <code>w</code> for write only, <code>a</code> for appending data to the file, <code>r+</code> for reading and writing. To open files in binary mode use <code>rb</code> for read only, <code>wb</code> for write only, and <code>rb+</code> for reading and writing.</li> <li>the default is to open connections in text mode - be careful if you are opening a connection to a file that is not text data.</li> <li>encoding: the encoding of the data in the file.</li> </ul> <p>Let's open up a connection to a CSV file and explore the file object that's returned.</p>"},{"location":"notebooks/week-3_1/#reading-files","title":"Reading files\u00b6","text":"<p>A read operation will copy bytes from the file on disk to the computer's memory. The <code>file</code> object,<code>f</code>, has the <code>read()</code> and <code>readline()</code> methods.</p> <p>The <code>read(size=n)</code> method in text mode will read <code>n</code> characters from the file. If <code>n</code> is omitted or is a negative number the <code>read()</code> function will read all of the characters in the file.</p> <p>The <code>readline()</code> will read until a newline in the text file. Text files have newline characters that denote the end of a line. On Windows the newline character is <code>\\r\\n</code> and on Linux / MacOS it is <code>\\n</code>.</p> <p>Let's test out the <code>read()</code> and <code>readline()</code> methods.</p>"},{"location":"notebooks/week-3_1/#writing-files","title":"Writing files\u00b6","text":"<p>The <code>write()</code> method of the <code>file</code> object writes Python objects (e.g. strings) to files. The data will then be stored on disk in a specified format until it is read again by another program.</p> <p>When writing data in text mode Python string objects (characters) are encoded (e.g. using ASCII or UTF-8) and stored as bytes on the disk.</p> <p>Let's demonstrate a write operation by writing the 1000 characters stored in <code>chars_1000</code> to a new file. First, we need to open a connection to the new file in write mode and specify an encoding.</p>"},{"location":"notebooks/week-3_1/#closing-files","title":"Closing files\u00b6","text":"<p>After you have finished reading or writing data from and to the file, it is important to <code>close()</code> the connection to the file. The <code>file</code> object's <code>close()</code> method does this.</p> <p>Once the <code>close()</code> method has been called on a <code>file</code> object it is no longer possible to read or write data from and to the file. This is important to prevent accidental data loss or corruption.</p> <p>We have two open file connections, let's close them.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you open a connection to the file <code>\"week-3-bf66-terra-precip-monthly.csv\"</code>, read and print the first 20 lines, and close the connection the file?</p>"},{"location":"notebooks/week-3_1/#context-managers","title":"Context managers\u00b6","text":"<p>Context managers - the \"correct\" way to read and write data from and to files in Python.</p> <p>Above we have gone through the process of opening connections to files, reading and writing data, and closing connections to files. However, there are lots of moving parts to this approach as you need to keep track of which connections are open to which files and to ensure you close connections when they are no longer needed. As applications grow and work with more data this can require handling many file connections which adds complexity and increases potential for mistakes / errors. Such mistakes / errors can result in data loss, corrupting files, or reduced security if file connections to private data are leaked.</p> <p>There are two \"better\" ways to read and write data in Python:</p> <ol> <li>using context managers.</li> <li>using methods and functions provided by packages (e.g. pandas <code>read_csv()</code> function).</li> </ol> <p>Let's open a file and read data from it using a context manager. A context manager ensures that connections to files are properly closed without explicitly having to code for it.</p> <p>To create a context, use the <code>with</code> statement.</p> <p>We've already demonstrated how to open, read, and write data in text mode from a CSV file. Let's use a context manager to demonstrate how to work with files in binary mode. Above we introduced the PNG file format for storing image data. We have a PNG file showing a Google Earth aerial image of the field we're working in. The first few bytes of the PNG file should be the PNG signature including a magic number for the start of the file and an ASCII representation of the letters PNG.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you read in the file <code>\"week-3-bf66-elevation.csv\"</code> using a context manager?</p>"},{"location":"notebooks/week-3_1/#pandas-io","title":"Pandas I/O\u00b6","text":"<p>Many Python packages provide functions and methods to read and write data that safely open and close connections to files.</p> <p>The pandas <code>read_csv()</code> function reads CSV data from disk into a pandas <code>DataFrame</code> object in a single line of code without us needing to explicitly close a connection to the file.</p> <p>Let's read some canola yield data collected by a harvester and stored as a CSV file into our program using the pandas <code>read_csv()</code> function.</p>"},{"location":"notebooks/week-3_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Go to the FAOSTAT Data Crop and Livestock page and pick a country, element (e.g. yield, area, production), item (e.g. crop - maize), and selection years. Download the data as a CSV file. Then, using <code>pd.read_csv()</code> read the data into a <code>DataFrame</code> object referenced by the variable <code>df_faostat</code>. Print the <code>head()</code> of the <code>DataFrame</code>.</p> <p>Hint: you will need to move the CSV file you download from FOASTAT into your working or data directory.</p>"},{"location":"notebooks/week-3_1/#parquet-files","title":"Parquet files\u00b6","text":"<p>So far we have been working with data in CSV format. The CSV format has many strengths for storing tabular data including:</p> <ul> <li>many software applications provide tools to read and write CSV data.</li> <li>the data structure is relatively intuitive with human readable characters encoded in binary, data values (fields) comprise binary representations of characters and are separated by comma symbols (hence the name), and rows (records) are separated by newline symbols.</li> <li>flexibility to choose different encodings of the text data.</li> </ul> <p>However, CSV files require that each data value is stored even if there is lots of repetition. For example, if there is a column that denotes the field name or id, for every row in the table the field name or id value would be repeated. As datasets get large, this can cause CSV files to increase in size which has subsequent storage costs.</p> <p>An alternative file format for storing tabular is parquet. Parquet files are optimised for storage. This provides more efficient use of hard drives, cheaper cloud storage costs, and quicker transmission of data over the internet.</p> <p>Parquet files have several optimisations for storing tabular data. Whereas CSV files are based around row storage, parquet files are based on column storage.</p> <p></p> <p>Parquet files can optimise storage of tabular data using run length encoding and dictionary encoding.</p> <p>Run length encoding is useful for reducing storage when there are runs of the same value within a column. For example, in the <code>canola_yield_df</code> <code>DataFrame</code> the values in the Crop column repeat. Instead of storing every value, we can store the two values: the value that repeats and the number of repetitions in the column (e.g. <code>(5, 80755)</code> - instead of storing the number 5 80,755 times as would be the case in a CSV file we can just store two numbers 5 and 80,755). Run length encoding is not suited for CSV files as the data is stored by row, and, often, within a row you'll have data of different types (e.g. string / text, numeric, and dates). This structure doesn't lend itself to encoding runs of the same value - repitition in tabular data generally runs down columns not across rows.</p> <p>Dictionary encoding is useful when we need to store large values (e.g. long names or large numbers). Instead of writing the large value repeatedly in the file a smaller value is written and there is a dictionary which acts as a look up table to correspond the small value to the actual large value. This means the large value only needs to be stored once.</p> <p>For a single field, we have 80,755 records (rows) in the <code>DataFrame</code>. If we scaled up this analysis to work with data collected from harvesters across many fields in Western Australia we would quickly accumulate a large volume of data. At some stage we will hit issues with storing and transferring the data and it might be appropriate to switch from CSV to parquet files for data storage.</p>"},{"location":"notebooks/week-3_2/","title":"Week 3 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-3_2\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-3_2\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-3_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-3_2\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-3_2.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-3_2\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-3_2 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import os\nimport pprint\nimport rioxarray as rxr\nimport xarray as xr\nimport pandas as pd\nimport geopandas as gpd\n</pre> import os import pprint import rioxarray as rxr import xarray as xr import pandas as pd import geopandas as gpd In\u00a0[\u00a0]: Copied! <pre># path to the GeoTIFF file\ns2_summer_path = os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-s2-summer-2020.tif\")\n</pre> # path to the GeoTIFF file s2_summer_path = os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-s2-summer-2020.tif\") In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_summer = rxr.open_rasterio(s2_summer_path)\n</pre> <p>Now we've opened the GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code> as an <code>xarray.DataArray</code> referenced by the variable <code>s2_summer</code>, let's explore the dataset using the attributes and methods of the <code>xarray.DataArray</code> object.</p> <p>What are the <code>dims</code> for the <code>xarray.DataArray</code> object storing the raster values from the GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p><code>xarray.DataArray</code> objects store descriptive dimension labels as a tuple under the <code>dims</code> attribute. These dimension labels are more descriptive and informative than the axis numbering of NumPy <code>ndarray</code>s.</p> <pre>s2_summer.dims\n</pre> <p>What is the size, in terms of the number of elements along each dimension, of the <code>xarray.DataArray</code> object storing the raster values from the GeoTIFF file <code>\"week-3-s2-summer-2020.tif\"</code>?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <p>We can return the raster values as a NumPy <code>ndarray</code> object and then access the <code>shape</code> property of the <code>ndarray</code>.</p> <pre>s2_summer.values.shape\n</pre> In\u00a0[\u00a0]: Copied! <pre>s2_summer.rio.crs\n</pre> s2_summer.rio.crs <p>We can see the CRS of the dataset is EPSG 4326, which is representing the raster data using latitude and longitude with the WGS84 ellipsoid and datum. It can be tricky to measure distance or compute area, as the data's positional units are in decimal degrees on 3D surface as opposed to metric units on a 2D surface. Therefore, often, we want to reproject raster data a projected CRS on a 2D surface.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>s2_summer_utm = s2_summer.rio.reproject(\"EPSG:32750\")\n# check it reprojected OK\ns2_summer_utm.rio.crs\n</pre> <p>Above, we knew the EPSG code for the CRS we wished to reproject our <code>xarray.DataArray</code> object to. What happens if we don't have this information? Can you look in the rio accessor docs and see if you can spot a method, and implement it, that will estimate a suitable UTM CRS for an <code>xarray.DataArray</code> object?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>est_utm = s2_summer.rio.estimate_utm_crs()\nest_utm\n</pre> In\u00a0[\u00a0]: Copied! <pre>era5 = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"era-5-western-australia-monthly-2023.nc\"))\nera5\n</pre> era5 = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"era-5-western-australia-monthly-2023.nc\")) era5 <p>You might spot that <code>era5</code> references a <code>xarray.Dataset</code> object with two data variables: <code>tp</code> and <code>t2m</code>. Each of <code>tp</code> and <code>t2m</code> are <code>xarray.DataArray</code> objects that store precipitation and air temperature at 2 m respectively. We can access each of these <code>xarray.DataArray</code> objects by their name. For example, to retrieve the temperature data as a <code>xarray.DataArray</code> we use dot notation to access the <code>t2m</code> variable:</p> In\u00a0[\u00a0]: Copied! <pre>era5.t2m\n</pre> era5.t2m <p>We can select a 2D slice of the multidimensional array of temperature values to visualise. <code>xarray.DataArray</code> objects have a <code>sel()</code> method which lets us conditionally select elements from the array using the <code>coordinates</code>. Let's slice out the 2D array corresponding to the time <code>\"2023-12-01T12:00:00\"</code>. We also need to select the <code>expver=5</code>, this distinguishes between the initial release of the data (<code>expver=5</code>) and a validated release (<code>expver=1</code>). Let's use <code>5</code> here.</p> <p>Finally, let's use <code>plot.imshow()</code> to visualise temperature values across Western Australia for a single time slice.</p> In\u00a0[\u00a0]: Copied! <pre>era5.t2m.sel(time=\"2023-12-01T12:00:00\", expver=5).plot.imshow()\n</pre> era5.t2m.sel(time=\"2023-12-01T12:00:00\", expver=5).plot.imshow() In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>era5.tp.sel(time=\"2023-06-01T18:00:00\", expver=1).plot.imshow(cmap=\"Blues\")\n</pre> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>cmip6_wa = xr.open_dataset(os.path.join(os.getcwd(), \"data_lab-3_2\", \"nuist_cmip6_wa_2100_tasmax.zarr\"), engine=\"zarr\")\nprint(cmip6_wa)\ncmip6_wa.tasmax.sel(time=\"2100-01-01T12:00:00\").plot()\n</pre> In\u00a0[\u00a0]: Copied! <pre>elev_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.csv\"))\nelev_df.head()\n</pre> elev_df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.csv\")) elev_df.head() <p>Now, let's use the longtitude and latitude columns in the <code>DataFrame</code> to convert the elevation data into a GeoPandas <code>GeoDataFrame</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Convert the elevation data to a spatial format\npoints = gpd.points_from_xy(elev_df[\"Lon\"], elev_df[\"Lat\"], crs=\"EPSG:4326\")\nprint(f\"points is of type {type(points)}\")\n\nelev_gdf = gpd.GeoDataFrame(elev_df, geometry=points)\nprint(f\"elev_gdf is of type {type(elev_gdf)}\")\n\nelev_gdf.head()\n</pre> # Convert the elevation data to a spatial format points = gpd.points_from_xy(elev_df[\"Lon\"], elev_df[\"Lat\"], crs=\"EPSG:4326\") print(f\"points is of type {type(points)}\")  elev_gdf = gpd.GeoDataFrame(elev_df, geometry=points) print(f\"elev_gdf is of type {type(elev_gdf)}\")  elev_gdf.head() <p>Let's get the first two rows of the <code>GeoDataFrame</code> and convert them to GeoJSON format. <code>GeoDataFrame</code>s have a <code>to_json()</code> method which can be used to convert the data in the <code>GeoDataFrame</code> into a string object in GeoJSON format.</p> In\u00a0[\u00a0]: Copied! <pre># Get the first two rows of the elevation GeoDataFrame and convert to GeoJSON\nelev_gdf_2 = elev_gdf.iloc[0:2, :]\nelev_gdf_2\n</pre> # Get the first two rows of the elevation GeoDataFrame and convert to GeoJSON elev_gdf_2 = elev_gdf.iloc[0:2, :] elev_gdf_2 In\u00a0[\u00a0]: Copied! <pre>elev_geojson_2 = elev_gdf_2.to_json()\nprint(f\"The GeoJSON data is stored as a {type(elev_geojson_2)} type object\")\nprint(\"\")\npprint.pprint(elev_geojson_2)\n</pre> elev_geojson_2 = elev_gdf_2.to_json() print(f\"The GeoJSON data is stored as a {type(elev_geojson_2)} type object\") print(\"\") pprint.pprint(elev_geojson_2) <p>In Python, the GeoJSON data that we have generated from our <code>GeoDataFrame</code> is stored as a string object. GeoJSON (and JSON) is a text-based data format similar to CSV files. However, unlike the CSV format where data has a tabular structure with records arranged by row the GeoJSON data is based around nested objects of key:value pairs.</p> <p>As we have subsetted the first two rows of our <code>GeoDataFrame</code> and converted them to GeoJSON we have generated a <code>FeatureCollection</code> object with two <code>Feature</code>s.</p> <p>Each row in the <code>GeoDataFrame</code> is converted to a <code>Feature</code> and each <code>Feature</code> has the column values per row stored in a <code>properties</code> object - these are the non-spatial attributes associated with each <code>Point</code> feature. The spatial information is stored in a <code>geometry</code> object which contains two key:value pairs. The value associated with the <code>type</code> key tells us this is a <code>Point</code> geometry and the array value associated with <code>coordinates</code> key defines the location.</p> <p>Compare the tabular display of the <code>GeoDataFrame</code> to the print of the GeoJSON to see how the non-spatial and spatial information in the table structure is converted to the GeoJSON nested format.</p> <p>We can save a <code>GeoDataFrame</code> to GeoJSON using the <code>GeoDataFrame</code>'s <code>to_file()</code> method and setting the driver argument to GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre># Save the elevation GeoDataFrame to a GeoJSON file\nelev_gdf.to_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\"), driver=\"GeoJSON\")\n</pre> # Save the elevation GeoDataFrame to a GeoJSON file elev_gdf.to_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\"), driver=\"GeoJSON\") <p>Check the GeoJSON file has saved to the directory specified. As it is text data, if you click on it you should be able to inspect its format in a text editor.</p> <p>You saved the elevation data to a GeoJSON file at this path: <code>os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\")</code>.</p> <p>Head to the GeoPandas documentation and look up how to read files into <code>GeoDataFrame</code> objects. Read the elevation.geojson file into a <code>GeoDataFrame</code> referenced by the variable <code>elev_from_file</code>.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>elev_from_file = gpd.read_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.geojson\"))\nelev_from_file.head()\n</pre> <p>Note, this answer assumes GeoPandas has been imported as gpd.</p> <p>Write the data referenced by <code>elev_from_file</code> to disk as a GeoPackage.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE\n</pre> ## ADD CODE HERE answer <pre>elev_from_file.to_file(os.path.join(os.getcwd(), \"data_lab-3_2\", \"week-3-bf66-elevation.gpkg\"), driver=\"GPKG\")\n</pre>"},{"location":"notebooks/week-3_2/#geospatial-data-io","title":"Geospatial data I/O\u00b6","text":"<p>Data analysis tasks involve reading geospatial data stored in files on disks, servers in the cloud, or recorded by sensors. Also, we need to save the results of our analysis or datasets we have generated to files.</p> <p>This lab will introduce:</p> <ul> <li>geospatial data file formats</li> <li>techniques for reading and writing from geospatial data from and to files</li> <li>Python data structures for representing vector and raster data</li> </ul>"},{"location":"notebooks/week-3_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-3_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-3_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the data for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-3_2/#install-packages","title":"Install packages\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-3_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-3_2/#raster-data","title":"Raster data\u00b6","text":"<p>Raster data represents geographic features and variables (e.g. elevation, reflectance in UAV images) as a grid of values (pixels).</p>"},{"location":"notebooks/week-3_2/#geotiff","title":"GeoTIFF\u00b6","text":"<p>Many geospatial datasets are based on the raster data model where values are assigned to pixels and pixels represent locations on the Earth's surface.</p> <p>A common source of raster data are remote sensing images captured by sensors on uncrewed aerial vehicles, aircraft, or satellites. Optical remote sensing images store the measured reflectance of light off the Earth's land surface in different wavelenghts. Raster remote sensing images are often stored using the GeoTIFF format.</p> <p>A GeoTIFF file is based on the Tagged Image File Format (or .tiff file) which is a general format for storing image data. A TIFF file comprises:</p> <ul> <li>a TIFF header which includes 8 bytes that tell us that the file is in TIFF format and where in the file (what byte number / byte offset from 0) the first Image File Directory is stored.</li> <li>Image File Directories which contains image metadata, a pointer to where the image data is in the file (what byte number / byte offset from 0), and the location of the next Image File Directory if there is more than one image stored in the TIFF file. Metadata is stored as fields which comprise a TIFF tag and it's corresponding value.</li> <li>Image Data - the values associated with each pixel in the image. A single TIFF file can store multiple images.</li> </ul> <p></p> <p>GeoTIFF files include extra information (metadata) as tags which describe the coordinate reference system (CRS) of the image data (i.e. where on the Earth's surface the image data corresponds to), spatial resolution, no data values, and various other configurations described here.</p> <p>GeoTIFF files can store multiple images (i.e. raster layers) in a single file. This makes them well suited for storing remote sensing image data where each raster layer corresponds to measured reflectance in a particular wavelength.</p> <p>We can use functions provided by the rioxarray package to read and write raster data in GeoTIFF format into Python programs.</p> <p>Using rioxarray's <code>open_rasterio()</code> method we can read raster data stored on disk as a GeoTIFF file into a <code>xarray.Dataset</code> or <code>xarray.DataArray</code> object in our Python program.</p> <p>The GeoTIFF file <code>\"week-2-s2-summer-2020.tif\"</code> in the <code>data_lab-3-2</code> stores remote sensing data covering a field in Western Australia. The remote sensing data was captured by the European Space Agency's Sentinel-2 satellite (10 m spatial resolution for red, green, blue, and near infrared bands). This is the path to the GeoTIFF file:</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>This is the rioxarray's <code>open_rasterio()</code> docs. Can you use this function to read in the GeoTIFF file referenced by <code>s2_summer_path</code> to a <code>xarray.DataArray</code>?</p> <p>Make sure the data is read to an <code>xarray.DataArray</code> object referenced by the variable name <code>s2_summer</code>.</p>"},{"location":"notebooks/week-3_2/#xarray-recap","title":"Xarray recap\u00b6","text":"These are notes repeated from week 2 that provide an overview of xarray and its classes for storing multidimensional arrays as objects in Python programs. (click the arrow to display notes). <p>Xarray is a Python package that builds on top of NumPy's array-based data structures, but provides extra tools and functions that are useful for working with geospatial and Earth Science datasets. For example, <code>xarray.DataArray</code> data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values.</p> <p><code>xarray</code> also provides convenient functions for reading raster data from geospatial data files on disk into memory as <code>xarray.DataArray</code> objects which we can use in our Python programs while retaining geographic and temporal information about the raster values stored in the array.</p> <p>Specifically, while a NumPy <code>ndarray</code> stores just the raster values and has some properties such as the <code>shape</code> (number of elements along each axis) and <code>ndim</code> (the dimensions of the array) it does not explicitly store any geospatial, temporal, or other geographic metadata. <code>xarray</code> solves this problem by reading raster data into an <code>xarray.DataArray</code> object with:</p> <ul> <li><code>values</code>: the multidimensional array of raster values</li> <li><code>dims</code>: a list of names for the dimensions of the array (e.g. instead axis 0 describing the 0th (row) dimension of an array that dimension can have a descriptive label such as longitude)</li> <li><code>coordinates</code>: a <code>list</code> of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)</li> <li><code>attrs</code>: a <code>dict</code> of metadata attributes describing the dataset</li> </ul> <p><code>xarray.DataArray</code> objects can be stored within a larger container called <code>xarray.Dataset</code>. An <code>xarray.Dataset</code> can store many <code>xarray.DataArray</code> objects that share <code>dims</code> and <code>coordinates</code>. This is useful if you have different arrays of different <code>Variables</code> that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single <code>xarray.Dataset</code>).</p> <p></p> <p>Why is <code>xarray</code> useful for geospatial data?</p> <ul> <li>The <code>dims</code> and <code>coordinates</code> of an <code>xarray.DataArray</code> mean we can subset values from an array using latitude, longitude, time, or whatever a coordinate describes; we're not just restricted to subsetting values based on their index location within an array</li> <li><code>xarray.Dataset</code> objects provide a container to store multidimensional arrays (e.g. many variables and time points) that are common in geography, Earth Sciences, meteorology, and agriculture. For example, multispectral satellite images of the same location over time; arrays of different meteorological variables)</li> <li>useful functions for reading, analysing and visualising raster or array-like geospatial data that are common across many spatial data science workflows</li> </ul>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>To answer these questions, you will need to look things up in the xarray docs. The user guide on Data Structures and the <code>xarray.DataArray</code> API reference will be useful.</p> <p>How do <code>xarray.DataArray</code> objects store raster data in Python programs?</p> answer <p><code>xarray.DataArray</code> objects store raster values in a multidimensional NumPy <code>ndarray</code> or array-like <code>values</code> property:</p> <pre>s2_summer.values\n</pre>"},{"location":"notebooks/week-3_2/#rio-accessor","title":"rio accessor\u00b6","text":"<p>The <code>rio</code> accessor from the rioxarray package extends the <code>xarray.DataArray</code> class with extra properties and methods that are useful for retrieving information about an <code>xarray.DataArray</code> object when it contains raster geospatial data.</p> <p>For example, the <code>rio.crs</code> property will return the coordinate reference system (CRS) of the raster data in the <code>xarray.DataArray</code> object which was retrieved when reading the data from the GeoTIFF file:</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you use the rio.reproject() method to reproject the raster data to <code>\"EPSG:32750\"</code> (UTM Zone 50S)?</p> <p>Save the reprojected <code>xarray.DataArray</code> object to a variable referenced by <code>s2_summer_utm</code>.</p>"},{"location":"notebooks/week-3_2/#netcdf","title":"NetCDF\u00b6","text":"<p>Network Common Data Form (NetCDF) is a commonly used file format in climatology, meteorology, oceanography, and geosciences and Earth sciences more broadly, where there is a need to store data as multidimensional arrays. The NetCDF file format is comprised of:</p> <ul> <li>variables - multidimensional arrays of data values (including 1D arrays for the dimensions with the same name as the corresponding dimension)</li> <li>dimensions - have a name and a value and describe the size and shape of the dataset</li> <li>attributes - additional metadata to describe the dataset</li> </ul> <p>The NetCDF format is very similar to the <code>xarray.Dataset</code> class (the NetCDF model was the basis for designing the <code>xarray.Dataset</code> class). However, <code>xarray.Dataset</code> are designed for working with multidimensional arrays in memory from within Python programs. NetCDF is a format for storing multidimensional arrays on disk. This is a useful description of the NetCDF format provided by ESRI.</p> <p>Let's open a NetCDF file which stores some data from ERA5-Land reanalysis weather data covering Western Australia. The data was downloaded from the Copernicus Climate Data Store with the following characteristics:</p> <ul> <li>Product type: Monthly averaged reanalysis by hour of day</li> <li>Variable: 2m temperature (t2m), Total precipitation (tp)</li> <li>Year: 2023</li> <li>Month: January, February, March, April, May, June, July, August, September, October, November, December</li> <li>Time: 00:00, 06:00, 12:00, 18:00</li> <li>Sub-region extraction: North -9\u00b0, West 109\u00b0, South -36\u00b0, East 129\u00b0</li> <li>Format: Zipped NetCDF-3 (experimental)</li> </ul>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you select the precipitation data from the <code>xarray.Dataset</code> <code>era5</code> and visualise precipitation data for a single time slice in June across Western Australia?</p> <p>Use a sensible colour palette for precipitation values. You will need to set <code>expver=1</code>.</p>"},{"location":"notebooks/week-3_2/#zarr","title":"ZARR\u00b6","text":"<p>Zarr is a modern cloud optimised format and specification for storing chunked and compressed multidimensional arrays. It's useful for working with big datasets which have an array-like structure and in cloud computing / web environments.</p> <p>For example, outputs from weather and climate models often comprise a large number of multidimensional arrays (e.g. dimensions for latitude, longitude, and time and arrays for a wide range of variables such as temperature, precipitation, wind speed, pressure, and so on ....). One way of conceptualising a zarr dataset is to think of it as a directory of compressed array files.</p> <p>Compressing arrays means that their storage size is reduced. This reduces the costs associated with storing large datasets and means it is quicker to transfer data over networks.</p> <p>Chunking the arrays in storage means you don't need to be able to read the entire array into memory in your Python programs. The memory limits on personal computers / laptops can prohibit reading in entire datasets stored as arrays as dataset sizes increase.</p> <p>Zarr datasets are well suited to cloud storage buckets (e.g. Google Cloud Storage, Amazon S3) and they also support parallel read and writes. This means that multiple clients (e.g. users, applications) can read data concurrently from the same zarr dataset stored in the cloud.</p> <p>In the <code>data_lab-3_2</code> directory is a zarr dataset named <code>nuist_cmip6_wa_2100_tasmax.zarr</code>. It stores an array of maximum temperature data for the year 2100 covering Western Australia extraced from the Climate Impact Lab Global Downscaled Projections for Climate Impacts Research. This data is from the World Climate Research Programme's 6th Coupled Model Intercomparison Project (CMIP6) and is generated by the NUIST NESM3 model and the ssp585 scenario (this dataset has been made available with a Creative Commons 4.0 International License.</p> <p>xarray provides functionality for reading zarr datasets (see docs here).</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you read in the zarr dataset <code>nuist_cmip6_wa_2100_tasmax.zarr</code> to an <code>xarray.Dataset</code>?</p>"},{"location":"notebooks/week-3_2/#vector-data","title":"Vector data\u00b6","text":"<p>Vector data uses point, line, or polygon geometries to represent geographic features.</p>"},{"location":"notebooks/week-3_2/#geopandas-geodataframe","title":"GeoPandas GeoDataFrame\u00b6","text":"<p>A GeoPandas <code>GeoDataFrame</code> is a tabular data structure for storing vector geospatial data and is based on a regular pandas <code>DataFrame</code>.</p> <p>A <code>GeoDataFrame</code> consists of columns of non-spatial attributes similar to a pandas <code>DataFrame</code>. However, a <code>GeoDataFrame</code> also has a <code>geometry</code> column which is a <code>GeoSeries</code> of geometries for the spatial data associated with each row.</p> <p>In Python, geometries are represented as Shapely <code>Geometry</code> objects. The <code>geometry</code> column in a GeoPandas <code>GeoDataFrame</code> is a <code>Series</code> of Shapely <code>Geometry</code> objects. Printing a Shapely <code>Geometry</code> object returns a Well Known Text (WKT) string description of the geometry (e.g. <code>POINT (0, 1)</code>). The <code>geometry</code> column of a <code>GeoDataFrame</code> (or a <code>GeoSeries</code>) can be viewed as a sequence of Shapely <code>Geometry</code> objects:</p> <pre><code>a_geoseries = [POINT (0, 1), POINT (0, 2), POINT (2, 3)]\n</code></pre> <p>Shapely provides tools for representing geometries in Python programs. It does not provide tools for reading geometry data from disk or handling attribute data. GeoPandas <code>GeoDataFrame</code> and <code>GeoSeries</code> combine Shapely's functionality for handling geometries with tools for reading and writing vector data, handling attributes, and visualisation. Therefore, we will focus on using <code>GeoDataFrame</code>s in these labs.</p> <p>Let's convert a CSV file with longitude, latitude, and elevation columns into a <code>GeoDataFrame</code>. First, let's read the CSV file in as a pandas <code>DataFrame</code>.</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>You will need to refer to the GeoPandas documentation to answer these questions.</p> What does executing the <code>GeoDataFrame</code> method <code>points_from_xy()</code> return? <code>points_from_xy()</code> expects a pandas <code>Series</code> objects for x and y coordinates and coordinate reference system. It will return to a GeoPandas <code>GeometryArray</code> object which stores a POINT geometry object for each x and y pair and can be converted into a <code>GeoSeries</code> object.  <p></p> The <code>GeoDataFrame()</code> constructor function can take three arguments. What are these arguments and how do they enable the creation of a <code>GeoDataFrame</code> object?     The <code>GeoDataFrame()</code> constructor function requires a pandas <code>DataFrame</code> as its first argument. This data is the non-spatial attributes. The second (optional) argument is a GeoPandas  object which stores <code>geometry</code> objects associated with each row (this could also be a string denoting the column of a <code>DataFrame</code> storing geometries. The third (optional) argument is a crs denoting the coordinate reference system for the geometry data."},{"location":"notebooks/week-3_2/#geojson","title":"GeoJSON\u00b6","text":"<p>JSON data (JavaScript Object Notation for its full name) is a widely used format for data interchange (exchanging data between programs, computers, clients, and servers).</p> <p>JSON represents data as key:value pairs enclosed within curly brackets <code>{}</code> (you might notice the similarity with Python's dictionary data structure).</p> <p>This is an example of JSON data:</p> <pre><code>{\n    \"title\": \"Introducing JSON\",\n    \"url\": \"https://www.json.org/json-en.html\"\n}\n</code></pre> <p>The values in JSON data can include text (strings), numbers, arrays (lists), and nested JSON objects. Like the CSV format, JSON is a text based format where human readable characters are encoded in binary using UTF-8 or UTF-16.</p> <p>GeoJSON is an extension of the JSON format for storing and exchanging spatial data. One of GeoJSON's uses is sending spatial data to web browsers to render as layers on web maps.</p> <p>GeoJSON represents geographic features as vector data (points, lines, and polygon geometries) and can also store non-spatial attribute information.</p> <p>Spatial data in GeoJSON are represented using <code>geometry</code> types which include:</p> <p><code>Point</code></p> <pre><code>{\"type\": \"Point\", \"coordinates\": [1, 1]}\n</code></pre> <p><code>LineString</code></p> <pre><code>{\"type\": \"LineString\", \"coordinates\": [[1, 1], [2, 2]]}\n</code></pre> <p><code>Polygon</code></p> <pre><code>{\"type\": \"Polygon\", \"coordinates\": [[[1, 1], [2, 2], [1, 2], [1, 1]]]}\n</code></pre> <p><code>Feature</code> types include attribute data as <code>properties</code> with <code>geometry</code> types.</p> <pre><code>{\n    \"type\": \"Feature\",\n    \"geometry\": {\n        \"type\": \"Point\",\n        \"coordinates\": [0, 0]\n    }, \n    \"properties\": {\n        \"name\": \"Perth Airport\"\n    }\n}\n</code></pre> <p>A <code>FeatureCollection</code> is a collection of <code>Feature</code>s.</p> <pre><code>{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [0, 0]\n            }, \n            \"properties\": {\n                \"name\": \"Perth Airport\"\n            }\n        },\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [10, 1]\n            }, \n            \"properties\": {\n                \"name\": \"Broome Airport\"\n            }\n        }\n        \n    ]\n}\n</code></pre> <p>You can read More than you ever wanted to know about GeoJSON for a description of the GeoJSON format.</p>"},{"location":"notebooks/week-3_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Identify two differences between the GeoJSON file format and a GeoPandas <code>GeoDataFrame</code> <ul> <li>A <code>GeoDataFrame</code> is used to store geospatial data in memory for Python programs. A GeoJSON file format describes how geospatial data should be encoded when it is stored on disk.</li> <li>A <code>GeoDataFrame</code> uses a tabular structure to organise non-spatial and spatial attributes with each row corresponding to a feature. GeoJSON format uses dictionary-like structure of key:value pairs with geographic data (coordinates) stored as values with a <code>geometry</code> key and attribute data stored as values with a <code>properties</code> key.</li> </ul>"},{"location":"notebooks/week-5_1/","title":"Week 5 1","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-5\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-5\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-5\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-5\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>%%HTML\n'&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/mc9QG2R-rf4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt;'\n</pre> %%HTML '' In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import tree\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n\nrng = np.random.RandomState(0)\n</pre> import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import plotly.io as pio import os  from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay from sklearn.model_selection import GroupShuffleSplit from sklearn.inspection import permutation_importance from sklearn import tree  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\"  rng = np.random.RandomState(0) In\u00a0[\u00a0]: Copied! <pre>data_path = os.path.join(os.getcwd(), 'data_lab-5', 'agrifieldnet_processed_adm4.geojson')\ngdf = gpd.read_file(data_path)\n</pre> data_path = os.path.join(os.getcwd(), 'data_lab-5', 'agrifieldnet_processed_adm4.geojson') gdf = gpd.read_file(data_path) In\u00a0[\u00a0]: Copied! <pre>print(f\"The data is of type: {type(gdf)}\")\ngdf.head()\n</pre> print(f\"The data is of type: {type(gdf)}\") gdf.head() <p>Based on the dataset's documentation the below is the mapping between numeric values and crop types in the labels dataset.</p> <ul> <li>1 - Wheat</li> <li>2 - Mustard</li> <li>3 - Lentil</li> <li>4 - No crop/Fallow</li> <li>5 - Green pea</li> <li>6 - Sugarcane</li> <li>8 - Garlic</li> <li>9 - Maize</li> <li>13 - Gram</li> <li>14 - Coriander</li> <li>15 - Potato</li> <li>16 - Bersem</li> <li>36 - Rice</li> </ul> <p>Let's explore how many examples we have of different crop types. We can see that our dataset is dominated by wheat, mustard, and no crop / fallow labels.</p> In\u00a0[\u00a0]: Copied! <pre># make labels categorical for bar plot\nclass_mappings = {\n    \"1\": \"Wheat\",\n    \"2\": \"Mustard\",\n    \"3\": \"Lentil\",\n    \"4\": \"Fallow\",\n    \"5\": \"Green pea\",\n    \"6\": \"Sugarcane\",\n    \"8\": \"Garlic\",\n    \"9\": \"Maize\",\n    \"13\": \"Gram\",\n    \"14\": \"Coriander\",\n    \"15\": \"Potato\",\n    \"16\": \"Bersem\",\n    \"36\": \"Rice\"\n}\n\ngdf[\"labels_cat\"] = gdf[\"labels\"].astype(\"str\")\ngdf.replace({\"labels_cat\": class_mappings}, inplace=True)\n\ngdf.groupby(\"labels_cat\").count().loc[:, \"field_id\"]\n</pre> # make labels categorical for bar plot class_mappings = {     \"1\": \"Wheat\",     \"2\": \"Mustard\",     \"3\": \"Lentil\",     \"4\": \"Fallow\",     \"5\": \"Green pea\",     \"6\": \"Sugarcane\",     \"8\": \"Garlic\",     \"9\": \"Maize\",     \"13\": \"Gram\",     \"14\": \"Coriander\",     \"15\": \"Potato\",     \"16\": \"Bersem\",     \"36\": \"Rice\" }  gdf[\"labels_cat\"] = gdf[\"labels\"].astype(\"str\") gdf.replace({\"labels_cat\": class_mappings}, inplace=True)  gdf.groupby(\"labels_cat\").count().loc[:, \"field_id\"] <p>We can also explore the spatial distribution of the data. Hover over the points on the map with your cursor.</p> In\u00a0[\u00a0]: Copied! <pre>gdf.explore(\"labels_cat\", tiles=\"CartoDB dark_matter\", cmap=\"tab20\", categorical=True)\n</pre> gdf.explore(\"labels_cat\", tiles=\"CartoDB dark_matter\", cmap=\"tab20\", categorical=True) <p>There are some final preprocessing steps required before we are ready to train a model to classify a field's crop type.</p> <p>Scikit-learn models expect the input data and outcomes to be <code>array-like</code>. Generally, this is in the form of NumPy <code>ndarray</code> objects.</p> <p>We want the input data (features or predictors) to be in a separate object to the outcomes (labels). Therefore, we'll subset the <code>GeoDataFrame</code> object and store just the predictor variables in an <code>array-like</code> object <code>X</code> and the outcomes in an object <code>y</code>.</p> <p>Numeric Pandas <code>Series</code> or <code>DataFrame</code> objects are <code>array-like</code> and so we can directly subset columns from the <code>GeoDataFrame</code> to create input and output objects.</p> <p><code>X</code> generally has the shape <code>(n_samples, n_features)</code> where each sample is aligned along the rows dimension (or 0-axis in a rank 2 NumPy <code>ndarray</code>) and the features (or predictors) are aligned along the columns dimension (or 1-axis in a rank 2 NumPy <code>ndarray</code>).</p> In\u00a0[\u00a0]: Copied! <pre>X = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"index_right\", \"village\", \"geometry\"], axis=1)\ny = gdf.loc[:, \"labels\"]\n</pre> X = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"index_right\", \"village\", \"geometry\"], axis=1) y = gdf.loc[:, \"labels\"] In\u00a0[\u00a0]: Copied! <pre>X.head()\n</pre> X.head() <p>For classification tasks the values in <code>y</code> should be integer and for regression tasks the values in <code>y</code> should be floating point. As crop type is a categorical variable values in <code>y</code> should be of integer data type.</p> In\u00a0[\u00a0]: Copied! <pre>y.head()\n</pre> y.head() In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng, test_size=0.3)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng, test_size=0.3) In\u00a0[\u00a0]: Copied! <pre>print(f\"the size of the training features object is {X_train.shape}\") \nprint(f\"the size of the test features object is {X_test.shape}\")\nprint(f\"the size of the training outcomes object is {y_train.shape}\")\nprint(f\"the size of the test features object is {y_test.shape}\")\n</pre> print(f\"the size of the training features object is {X_train.shape}\")  print(f\"the size of the test features object is {X_test.shape}\") print(f\"the size of the training outcomes object is {y_train.shape}\") print(f\"the size of the test features object is {y_test.shape}\") In\u00a0[\u00a0]: Copied! <pre># create and train a random forests model\nrf = RandomForestClassifier(n_estimators=20, random_state=rng)\nrf.fit(X_train, y_train.astype(int))\n</pre> # create and train a random forests model rf = RandomForestClassifier(n_estimators=20, random_state=rng) rf.fit(X_train, y_train.astype(int)) In\u00a0[\u00a0]: Copied! <pre>y_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n</pre> y_pred = rf.predict(X_test)  print(classification_report(y_test, y_pred)) <p>From the classification report, we can ascertain the following:</p> <ul> <li>the model's overall accuracy is 0.68 - 68% of the examples in the test set were classified correctly.</li> <li>the model's performance is better for class labels 1 (wheat), 4 (fallow), and 9 (maize).</li> <li>the performance metric scores for the other classes is lower.</li> <li>the model's performance is best for classes with the most observations in the training dataset.</li> <li>we're getting a warning indicating to us that the precision and f1-score are being set to zero in labels with no predicted samples.</li> </ul> <p>We can also plot a confusion matrix to see if there are patterns of confusion between classes.</p> In\u00a0[\u00a0]: Copied! <pre>labels = [\"Wheat\", \n          \"Mustard\", \n          \"Lentil\", \n          \"Fallow\", \n          \"Green Pea\",\n          \"Sugarcane\",\n          \"Garlic\",\n          \"Maize\",\n          \"Gram\",\n          \"Coriander\",\n          \"Potato\",\n          \"Bersem\",\n          \"Rice\"]\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot(text_kw={\"fontsize\":10}, xticks_rotation=\"vertical\")\nplt.show()\n</pre> labels = [\"Wheat\",            \"Mustard\",            \"Lentil\",            \"Fallow\",            \"Green Pea\",           \"Sugarcane\",           \"Garlic\",           \"Maize\",           \"Gram\",           \"Coriander\",           \"Potato\",           \"Bersem\",           \"Rice\"] cm = confusion_matrix(y_test, y_pred) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(text_kw={\"fontsize\":10}, xticks_rotation=\"vertical\") plt.show() <p>From the confusion matrix, we can see:</p> <ul> <li>there is confusion between the mustard and wheat classes.</li> <li>a large number of minitory classes as misclassified as wheat or mustard (majority classes).</li> <li>there were no successful classifications of coriander, garlic, or bersem.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>## add code here ##\n</pre> ## add code here ## answer <pre># import the tree module from scikit-learn\nfrom sklearn import tree\n\n# create a decision tree classifier object\nclf = tree.DecisionTreeClassifier(random_state=0)\n\n# train the model\nclf.fit(X_train, y_train)\n\n# test model\ny_pred_tree = clf.predict(X_test)\nprint(classification_report(y_test, y_pred_tree))\n</pre> In\u00a0[\u00a0]: Copied! <pre>all_preds = rf.predict(X)\n</pre> all_preds = rf.predict(X) <p>We can then append the predicted crop type labels to our initial <code>GeoDataFrame</code> as a new column and visualise these predictions on a map. Hover over points on the map with your cursor to see the actual (<code>labels_cat</code>) and predicted (<code>predicted</code>) crop types for a field.</p> In\u00a0[\u00a0]: Copied! <pre>gdf[\"predicted\"] = all_preds.astype(\"str\")\ngdf.replace({\"predicted\": class_mappings}, inplace=True)\nbasemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\nattribution = \"Tiles &amp;copy; Esri &amp;mdash; Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\ngdf.explore(column=\"predicted\", cmap=\"tab20\", categorical=True, tiles=basemap, attr=attribution, tooltip=[\"labels_cat\", \"predicted\"])\n</pre> gdf[\"predicted\"] = all_preds.astype(\"str\") gdf.replace({\"predicted\": class_mappings}, inplace=True) basemap = \"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\" attribution = \"Tiles \u00a9 Esri \u2014 Source: Esri, i-cubed, USDA, USGS, AEX, GeoEye, Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\" gdf.explore(column=\"predicted\", cmap=\"tab20\", categorical=True, tiles=basemap, attr=attribution, tooltip=[\"labels_cat\", \"predicted\"]) In\u00a0[\u00a0]: Copied! <pre>px.histogram(gdf, x=\"labels_cat\")\n</pre> px.histogram(gdf, x=\"labels_cat\") In\u00a0[\u00a0]: Copied! <pre>print(\"the number of samples by class in our overall dataset (pre-split) are:\")\ngdf.groupby('labels_cat').count().loc[:, \"field_id\"]\n</pre> print(\"the number of samples by class in our overall dataset (pre-split) are:\") gdf.groupby('labels_cat').count().loc[:, \"field_id\"] <p>Data leakage occurs when information in the test set leaks into the training dataset. This means the test set is not truly independent and does not provide an unbiased assessment of the model's performance on new data.</p> <p>Spatial correlation occurs when observations close to each other are more similar or disimilar than observations further away. This is encapsulated by Tobler's first law of Geography: \"Everything is related to everything else. But near things are more related than distant things.\"</p> <p>Geospatial data is often spatially correlated. This means that data points close to each other are not statistically independent. A random training and test split of spatially correlated data can result in the test dataset not being independent of the training dataset. This is because some of the data in the test set is correlated with data in the training set. Spatial correlation is causing data leakage and the evaluation of model performance using this test set will be biased.</p> In\u00a0[\u00a0]: Copied! <pre>X_sp = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"predicted\", \"index_right\", \"village\", \"geometry\"], axis=1)\ny_sp = gdf.loc[:, \"labels\"]\ngroups = gdf.loc[:, \"village\"]\n</pre> X_sp = gdf.drop([\"field_id\", \"labels\", \"labels_cat\", \"predicted\", \"index_right\", \"village\", \"geometry\"], axis=1) y_sp = gdf.loc[:, \"labels\"] groups = gdf.loc[:, \"village\"] <p>scikit-learn has a <code>GroupShuffleSplit</code> object that has a <code>split()</code> method that can be used to generate splits of the dataset.</p> <p>First, we need to create an instance of the <code>GroupShuffleSplit</code> object specifying the number of different splits of the dataset that we want to create using the <code>n_splits</code> argument. We also use the <code>train_size</code> argument to define how much of the data should be allocated to the training and test sets.</p> <p>Here, we only want to create one split of our dataset at the <code>village</code> level so we set <code>n_splits=1</code>.</p> <p>Then, we call the <code>split()</code> method of <code>gss</code>, our <code>GroupShuffleSplit</code> object, passing in the features (<code>X_sp</code>), outcome labels (<code>y_sp</code>), and the groups (<code>groups</code>). This returns to us a <code>train_index</code> and <code>test_index</code> specifying the index locations of samples allocated to the training and test set. Passing in <code>groups</code> ensures that no samples from the same group (<code>village</code>) are in both the training and test sets.</p> <p>We then use the index locations in <code>train_index</code> and <code>test_index</code> to subset <code>X_sp</code> and <code>y_sp</code> for model training and testing.</p> In\u00a0[\u00a0]: Copied! <pre>gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=0)\n\nfor i, (train_index, test_index) in enumerate(gss.split(X_sp, y_sp, groups)):\n    print(f\"processing split {i}\")\n    X_train_sp = X_sp.iloc[train_index, :]\n    X_test_sp = X_sp.iloc[test_index, :]\n    y_train_sp = y_sp.iloc[train_index]\n    y_test_sp = y_sp.iloc[test_index]\n    print(f\"the size of the training features object is {X_train_sp.shape}\") \n    print(f\"the size of the test features object is {X_test_sp.shape}\")\n    print(f\"the size of the training outcomes object is {y_train_sp.shape}\")\n    print(f\"the size of the test outcomes object is {y_test_sp.shape}\")\n</pre> gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=0)  for i, (train_index, test_index) in enumerate(gss.split(X_sp, y_sp, groups)):     print(f\"processing split {i}\")     X_train_sp = X_sp.iloc[train_index, :]     X_test_sp = X_sp.iloc[test_index, :]     y_train_sp = y_sp.iloc[train_index]     y_test_sp = y_sp.iloc[test_index]     print(f\"the size of the training features object is {X_train_sp.shape}\")      print(f\"the size of the test features object is {X_test_sp.shape}\")     print(f\"the size of the training outcomes object is {y_train_sp.shape}\")     print(f\"the size of the test outcomes object is {y_test_sp.shape}\") <p>Now we're ready to train and test our model.</p> <p>Let's train the model and test its performance.</p> In\u00a0[\u00a0]: Copied! <pre># create and train a random forests model\nrf_sp = RandomForestClassifier(n_estimators=20, random_state=rng)\nrf_sp.fit(X_train_sp, y_train_sp.astype(int))\n</pre> # create and train a random forests model rf_sp = RandomForestClassifier(n_estimators=20, random_state=rng) rf_sp.fit(X_train_sp, y_train_sp.astype(int)) In\u00a0[\u00a0]: Copied! <pre>y_pred_sp = rf.predict(X_test_sp)\n\nprint(classification_report(y_test_sp, y_pred_sp))\n</pre> y_pred_sp = rf.predict(X_test_sp)  print(classification_report(y_test_sp, y_pred_sp))"},{"location":"notebooks/week-5_1/#machine-learning-1","title":"Machine learning 1\u00b6","text":"<p>This lab will provide an introduction to key machine learning concepts and also demonstrate how you can use Scikit-learn to implement machine learning workflows in Python.</p> <p>The focus of this lab will be on supervised machine learning. This lab will develop a machine learning workflow that classifies the crop type of fields in India using spectral reflectance values recorded by the Sentinel-2 satellite. It is based on the AgriFieldNet Competition Dataset (Radiant Earth Foundation and IDinsight, 2022) which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images.</p> <p>The dataset includes spectral reflectance values, crop type labels, field id, and geometry for the field's location from cropping landscapes in the Indian States of Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsights Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation.</p>"},{"location":"notebooks/week-5_1/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-5_1/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-5_1/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-5_1/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-5_1/#machine-learning","title":"Machine learning\u00b6","text":"<p>Machine learning is the process of learning from data to make predictions. Supervised machine learning models are trained to predict an outcome based on input data (predictors or features). The model is trained to minimise the error in predictions using a training set where both the outcome labels and input data are known. If the outcome is categorical (e.g. land cover type, cloud / no-cloud) then it is a classification machine learning task and if the outcome is numeric (e.g. crop yield, temperature) then it is a regression machine learning task.</p> <p>There are also unsupervised machine learning tasks where there are no known outcomes prior to model training. Unsupervised machine learning models typically cluster datasets with similar data points assigned to the same cluster or group.</p> <p>Please watch this introduction to machine learning video from Climate Change AI:</p>"},{"location":"notebooks/week-5_1/#scikit-learn","title":"Scikit-learn\u00b6","text":"<p>Scikit-learn is an open-source machine learning package for Python. It provides a range of tools for preprocessing datasets for machine learning, training machine learning models, evaluating model performance, and using a trained model to make predictions. A range of supervised and unsupervised machine learning algorithms can be used with Scikit-learn.</p>"},{"location":"notebooks/week-5_1/#task","title":"Task\u00b6","text":"<p>The focus of this lab will be on supervised machine learning. This lab will develop a machine learning workflow that classifies the crop type of fields in India using spectral reflectance values recorded by the Sentinel-2 satellite. It is based on the AgriFieldNet Competition Dataset (Radiant Earth Foundation and IDinsight, 2022) which has been published to encourage people to develop machine learning models that classify a field's crop type from satellite images.</p> <p>The dataset includes spectral reflectance values, crop type labels, field id, and geometry for the field's location from cropping landscapes in the Indian States of Odisha, Uttar Pradesh, Bihar, and Rajasthan. The field boundaries and crop type labels were captured by data collectors from IDinsights Data on Demand team and the satellite image preparation was undertaken by the Radiant Earth Foundation.</p>"},{"location":"notebooks/week-5_1/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-5_1/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/week-5_1/#data-pre-processing","title":"Data pre-processing\u00b6","text":"<p>Often, after sourcing data, the first task in a machine learning workflow is data preprocessing - transforming the raw data into a format ready for model training or making predictions. These tasks are often referred to as feature engineering - the process of engineering or creating features or predictor variables.</p> <p>Let's inspect the data. We can see it is a <code>GeoDataFrame</code> with columns corresponding to the <code>field_id</code>, <code>labels</code> (crop type identifier), spectral reflectance in several wavebands (<code>B*</code>) and <code>ndvi</code>, the <code>village</code> where the field is located in India, and <code>geometry</code> <code>POINT</code> object for the field centroid.</p>"},{"location":"notebooks/week-5_1/#train-test-splits","title":"Train-test splits\u00b6","text":"<p>For supervised machine learning tasks we need to create training and test datasets.</p> <p>The model is trained using the training set which consists of matched features and outcomes.</p> <p>The model is then evaluated using the test set. A prediction is made using features in the test set and the prediction is compared with known outcomes for those features. This provides an indication of the model's performance. It is important that the test set is independent from the training set - an important part of machine learning model development is preventing information from the test set leaking into the training set.</p> <p>Scikit-learn provides a useful <code>train_test_split()</code> function which expects <code>X</code> and <code>y</code> <code>array-like</code> objects as inputs and will return 4 <code>array-like</code> objects (<code>X_train</code>, <code>X_test</code>, <code>y_train</code>, <code>y_test</code>).</p> <p>We can provide further arguments to <code>train_test_split()</code>:</p> <ul> <li><code>test_size</code> determines the proportion of the input data that is allocated to the test set</li> <li><code>random_state</code> is a seed that ensures the same random split of the data occurs each time the code is executed. This is important for reproduciblity of results.</li> </ul>"},{"location":"notebooks/week-5_1/#model-training","title":"Model training\u00b6","text":"<p>Scikit-learn provides a range of machine learning algorithms that can be trained for different tasks (e.g. classification, regression, text, images, clustering etc.).</p> <p>In Scikit-learn terminology each of these algorithms is called an <code>estimator</code> - the docs have a useful interactive guide to help you select the right <code>estimator</code> for your machine learning task.</p> <p>Each <code>estimator</code> object has a <code>fit()</code> method. The fit method expects the training data as arguments (<code>X_train</code> and <code>y_train</code>) and when called learns rules that minimise the error in predicting the outcome labels in <code>y_train</code>. This is the learning part of the machine learning workflow.</p> <p>Here, we will demonstrate how to train a tree-based machine learning model: a random forests classifier.</p> <p>First, we create an <code>estimator</code> object for the model. Then, we use the <code>estimator</code>'s <code>fit()</code> method to train the model.</p>"},{"location":"notebooks/week-5_1/#random-forests-classifiers","title":"Random forests classifiers\u00b6","text":"<p>Random forests models are an ensemble and tree-based model. They're a tree-based model as they consist of an ensemble of decision tree classifiers.</p> <p>Please read through this Google Machine Learning Guide on decision trees and random forests.</p> Detailed notes on tree-based models <p>Decision tree classifiers are trained to learn rules that classify outcome labels based on input features by recursively splitting the feature space. The algorithm starts by finding the value of a feature that splits the dataset into two groups which minimise the \"impurity\" of outcome labels in each group. Then, that process is repeated by splitting each of the two groups, again to minimise the \"impurity\" of outcome labels. This process repeats until a stopping criterion is reached. The Gini index is the default metric to measure class impurity in each internal node of the tree.</p> <p>The class label associated with each of the terminal nodes of the tree is based on the most commonly occurring class.</p> <p>Individual decision tree classifiers are relatively quick to train, can learn non-linear and interactive relationships between input features and outcome labels, and are easy to visualise and interpret.</p> <p>However, there are limits to decision tree classifiers. They are often not the most accurate classifiers. They also have high variance; if you train a decision tree classifier on two different samples it will likely learn different relationships and generate different predictions. Large decision trees can also overfit the training data; they can learn to fully represent the structure of the training set but will not generalise well to new and unseen data.</p> <p>Random forests models mitigate the limitations of a single decision tree classifier by:</p> <p>bagging: training a number (ensemble) of decision trees based on bootstrap samples of the training datasets. The average prediction from many decision tree models reduces the variance in predictions.</p> <p>sampling features at each split: when training each of the decision trees in the ensemble, a random selection of features are searched for each split within the tree. This prevents a small number of features from dominating the model, enables the model to learn using all the input features, and reduces overfitting. If there are p features, then often the m \u221ap are considered at each split.</p> <p>majority vote: for classification tasks, the final predicted value from a random forest model is the most common prediction of the outcome label across all trees in the ensemble.</p> <p></p> <p>Let's create a random forest model <code>estimator</code> object using the <code>RandomForestClassifier()</code> function. We'll set the <code>n_estimators</code> parameter to 20 here; this means the random forest will consist of an ensemble of 20 decision tree classifiers. The <code>random_state</code> parameter ensures we learn the same model each time we train it on the same data; this is important for reproducible results.</p>"},{"location":"notebooks/week-5_1/#model-evaluation","title":"Model evaluation\u00b6","text":"<p>After training a model, we need to evaluate it to assess its performance. This evaluation allows us to compare different models and get an indication of how well the model will perform when it is used on new data.</p> <p>After training, the model object, <code>rf</code> in this case stores rules that map input data to output (predicted) labels. In the case of a random forests model, these rules are stored and expressed as a collection of decision trees.</p> <p>It is important that the test data used to evaluate a model is independent of the training data; this is to ensure an unbiased estimate of model performance.</p> <p>There are a range of model evaluation metrics for classification tasks:</p> <ul> <li>accuracy: the proportion of correctly classified examples.</li> <li>recall: the ratio of true positives to true positives and false negatives - the ability of the classifier to capture all positive cases. $recall = \\frac{tp}{tp+fn}$.</li> <li>precision: the ratio of true positives to true positives and false positives - the classifiers ability not to label something as positive when it is not. $precision = \\frac{tp}{tp+fp}$.</li> <li>f1-score: the f1-score combines the recall and precision scores and takes on the range 0 to 1. $F1 = 2\\cdot\\frac{precision\\cdot{recall}}{precision+recall}$</li> </ul> <p>Scitkit-learn provides a <code>classification_report()</code> which can be used to generate performance metrics for each class and the model as a whole.</p> <p>The <code>classification_report()</code> expects known outcome labels and predicted outcome labels. To generate the predicted labels, we can use the <code>predict()</code> method of the <code>estimator</code> object and pass in input test data.</p>"},{"location":"notebooks/week-5_1/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Earlier, we discussed that random forests models should be more accurate than a single decision tree classifier. Can you train a decision tree classifier to test if this the case?</p> <p>The documentation for the decision tree classifier is here.</p> <p>Use <code>X_train</code>, <code>y_train</code>, <code>X_test</code>, and <code>y_test</code> for this task</p>"},{"location":"notebooks/week-5_1/#visualising-predictions","title":"Visualising predictions\u00b6","text":"<p>We have a <code>DataFrame</code> <code>X</code> which stores all the features before we split the data into training and test splits. We can use <code>X</code> to generate a predicted crop type label for each of our data points.</p>"},{"location":"notebooks/week-5_1/#class-imbalance","title":"Class imbalance\u00b6","text":"<p>Our training and test datasets are clearly imbalanced across the outcome class labels. The majoring of samples are of wheat (1), mustard (2), or fallow (4) classes.</p> <p>We can see the imbalance in our dataset using a bar plot.</p>"},{"location":"notebooks/week-5_1/#recap-quiz","title":"Recap quiz\u00b6","text":"How could imbalanced data affect model performance? <ul> <li>The model will not see enough examples of minority classes to learn rules to discriminate them from the input data</li> <li>The model will learn it can achieve good overall accuracy by just predicting majority classes</li> </ul> What could we do to fix the class imbalance problem? <ul> <li>Undersample the majority classes</li> <li>Oversample the minority classes</li> <li>Get more data</li> <li>Pool the minority classes to reduce the total number of classes</li> </ul>"},{"location":"notebooks/week-5_1/#data-leakage","title":"Data leakage\u00b6","text":""},{"location":"notebooks/week-5_1/#recap-quiz","title":"Recap quiz\u00b6","text":"How could you generate training and test splits which are not spatially correlated? First, you could explore the spatial correlation in your dataset using techniques such as Moran's I and Local Moran's I statistics.   <p>If you believe that your samples are not likely to be correlated across administrative boundaries such as villages, counties, states etc. you could randomly split your data at the administrative boundary-level as opposed to the sample-level. That is, instead of taking a random hold-out sample of data points for the test set you would take a random sample of administrative units as the test set and all data points inside those units would be your test set.</p> <p>An alternative strategy if there are no useful administrative boundaries could be to spatially cluster your samples using their coordinates so proximal data points are allocated to the same cluster and randomly hold-out some clusters as the test set.</p> <p></p> <p>Let's use the <code>village</code> column in our dataset <code>gdf</code> as a group to guide generation of training and test sets. We'll ensure that no samples from the same village are in both the training and test sets.</p> What is our assumption when using villages as the grouping variable? We are assuming that data points in neighbouring villages are not spatially correlated, and, therefore, there is no data leakage from the the test set to the training set. Is this a safe assumption? Do you think villages next to each other will have different agricultural contexts?"},{"location":"notebooks/week-5_2/","title":"Week 5 2","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport subprocess\n\nif \"data_lab-5\" not in os.listdir(os.getcwd()):\n    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)\n    if \"data_lab-5\" not in os.listdir(os.getcwd()):\n        print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n    else:\n        print(\"Data download OK\")\n</pre> import os import subprocess  if \"data_lab-5\" not in os.listdir(os.getcwd()):     subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-5.zip\"', shell=True, capture_output=True, text=True)     subprocess.run('unzip \"data_lab-5.zip\"', shell=True, capture_output=True, text=True)     if \"data_lab-5\" not in os.listdir(os.getcwd()):         print(\"Has a directory called data_lab-5 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")     else:         print(\"Data download OK\") In\u00a0[\u00a0]: Copied! <pre>if 'google.colab' in str(get_ipython()):\n    !pip install xarray[complete]\n    !pip install rioxarray\n    !pip install mapclassify\n    !pip install rasterio\n</pre> if 'google.colab' in str(get_ipython()):     !pip install xarray[complete]     !pip install rioxarray     !pip install mapclassify     !pip install rasterio In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn import tree\n\n# setup renderer\nif 'google.colab' in str(get_ipython()):\n    pio.renderers.default = \"colab\"\nelse:\n    pio.renderers.default = \"jupyterlab\"\n\nrng = np.random.RandomState(0)\n</pre> import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import plotly.express as px import plotly.io as pio import os  from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay from sklearn.model_selection import GroupShuffleSplit from sklearn.model_selection import cross_val_score from sklearn.model_selection import cross_validate from sklearn.metrics import mean_absolute_error, mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.inspection import permutation_importance from sklearn import tree  # setup renderer if 'google.colab' in str(get_ipython()):     pio.renderers.default = \"colab\" else:     pio.renderers.default = \"jupyterlab\"  rng = np.random.RandomState(0) In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-5\", \"lobell_2019_maize.csv\"))\nprint(f\"the shape of the DataFrame is {df.shape}\")\ndf.head()\n</pre> df = pd.read_csv(os.path.join(os.getcwd(), \"data_lab-5\", \"lobell_2019_maize.csv\")) print(f\"the shape of the DataFrame is {df.shape}\") df.head() In\u00a0[\u00a0]: Copied! <pre>fig = px.histogram(\n    data_frame=df, \n    x=\"cc_yield\",  \n    marginal=\"box\"\n)\nfig.show()\n</pre> fig = px.histogram(     data_frame=df,      x=\"cc_yield\",       marginal=\"box\" ) fig.show() <p>The <code>DataFrame</code> stores Sentinel-2 derived vegetation indices in the following columns:</p> <ul> <li><code>gcvi_doy_151</code> - average field GCVI on day of year 151.</li> <li><code>gcvi_doy_171</code> - average field GCVI on day of year 171.</li> <li><code>ndvi_doy_151</code> - average field NDVI on day of year 151.</li> <li><code>ndvi_doy_171</code> - average field NDVI on day of year 171.</li> <li><code>mtci_doy_151</code> - average field MTCI on day of year 151.</li> <li><code>mtci_doy_171</code> - average field MTCI on day of year 171.</li> </ul> <p>NDVI is the normalised difference vegetation index. GCVI is the green chlorophyll index. MTCI is the meris terrestrial chlorophyll index. These will be the main features (predictors) in our model.</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>fig = px.scatter(\n    df,\n    x = \"gcvi_doy_151\", ## CHANGE THIS FOR DIFFERENT VEGETATION INDICES\n    y = \"cc_yield\",\n    trendline = \"ols\",\n    opacity=0.25,\n    labels={\"cc_yield\": \"Maize crop yield (Mg/ha)\",\n           \"gcvi_doy_151\": \"GCVI (DOY 151)\"} ## CHANGE THIS FOR DIFFERENT VEGETATION INDICES\n)\nfig.show()\n</pre> In\u00a0[\u00a0]: Copied! <pre># get X and y data\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n</pre> # get X and y data  # drop nas as Linear Regression object cannot be trained on datasets with missing data df_linear_reg = df.loc[: , [     \"cc_yield\",     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\"]].dropna()  # get X X = df_linear_reg.loc[:, [     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\"]]  # get Y y = df_linear_reg.loc[:, \"cc_yield\"] In\u00a0[\u00a0]: Copied! <pre># create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_val_score(reg, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n</pre> # create a LinearRegression estimator object reg = LinearRegression()  # evaluate using 5-fold cross validation cv_scores = cross_val_score(reg, X, y, cv=5, scoring=\"neg_mean_absolute_error\") <p><code>cv_scores</code> should reference an array of values recording the mean absolute error for the predictions of maize crop yield for each fold. Scikit-learn returns negative mean absolute error values (becauase their convention is that a higher metric values are better than lower metric values which holds for metrics for categorical outcomes such as accuracy). Therefore, we'll want to convert negative mean absolute error values to positive.</p> In\u00a0[\u00a0]: Copied! <pre># print cross validation test scores\nfor i, mae in enumerate(cv_scores):\n    print(f\"the mae for the {i}th fold is {round(abs(mae), 4)}\")\n</pre> # print cross validation test scores for i, mae in enumerate(cv_scores):     print(f\"the mae for the {i}th fold is {round(abs(mae), 4)}\") <p>If we want to use more than one metric to evaluate the model, we can pass in a list of metrics to the <code>scoring</code> argument. Let's also estimate the mean squared error value as well as the mean absolute error. The mean squared error penalises the model more for predictions with larger error.</p> <p>To use multiple metrics we need to use the <code>cross_validate()</code> function instead.</p> In\u00a0[\u00a0]: Copied! <pre># evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"])\ncv_scores\n</pre> # evaluate using 5-fold cross validation cv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"]) cv_scores In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>print(f\"mean mae: {abs(cv_scores['test_neg_mean_absolute_error'].mean())}\")\nprint(f\"mean mse: {cv_scores['test_neg_mean_squared_error'].mean()}\")\n</pre> <p></p> <p>Can you train and evaluate a random forests model using 5-fold cross-validation to see if it improves the predictive performance?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>rf = RandomForestRegressor(n_estimators=20, random_state=rng)\nrf_cv_scores = cross_validate(rf, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\nrf_cv_scores\n</pre> In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() <p>Now, let's one hot encode the <code>slope_sr</code> variable and see how it is represented as numeric data. (scroll to the far right of the displayed <code>DataFrame</code>).</p> In\u00a0[\u00a0]: Copied! <pre>df_cat = pd.get_dummies(df, columns=[\"slope_sr\"])\n</pre> df_cat = pd.get_dummies(df, columns=[\"slope_sr\"]) In\u00a0[\u00a0]: Copied! <pre>df_cat.head()\n</pre> df_cat.head() In\u00a0[\u00a0]: Copied! <pre>df_cat.columns\n</pre> df_cat.columns <p>Now, let's retrain our linear regression model using slope as a feature.</p> In\u00a0[\u00a0]: Copied! <pre># get X and y data\n\n## NOTE WE ARE USING df_cat here!!\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_cat.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"])\ncv_scores\n</pre> # get X and y data  ## NOTE WE ARE USING df_cat here!! # drop nas as Linear Regression object cannot be trained on datasets with missing data df_linear_reg = df_cat.loc[: , [     \"cc_yield\",     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",     \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]].dropna()  # get X X = df_linear_reg.loc[:, [     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",     \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]]  # get Y y = df_linear_reg.loc[:, \"cc_yield\"]  # create a LinearRegression estimator object reg = LinearRegression()  # evaluate using 5-fold cross validation cv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"]) cv_scores In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre>df_2 = pd.get_dummies(df, columns=[\"slope_sr\", \"soiltype_sr\", \"soilqual_sr\"])\ndf_2.head()\n</pre> <p>Can you use <code>df_2</code> with <code>soiltype_sr</code>, <code>slope_sr</code>, and <code>soilqual_sr</code> as training data in a random forests model?</p> In\u00a0[\u00a0]: Copied! <pre>## ADD CODE HERE ##\n</pre> ## ADD CODE HERE ## answer <pre># get X and y data\n\n# NOTE WE USE df_2 here!!\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_2.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\", \n    \"soiltype_sr_CLAY\", \n    \"soiltype_sr_LOAM\",\n    \"soiltype_sr_OTHER (SPECIFY)\", \n    \"soiltype_sr_SANDY\", \n    \"soilqual_sr_FAIR\",\n    \"soilqual_sr_GOOD\", \n    \"soilqual_sr_POOR\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\", \n    \"soiltype_sr_CLAY\", \n    \"soiltype_sr_LOAM\",\n    \"soiltype_sr_OTHER (SPECIFY)\", \n    \"soiltype_sr_SANDY\", \n    \"soilqual_sr_FAIR\",\n    \"soilqual_sr_GOOD\", \n    \"soilqual_sr_POOR\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\nrf = RandomForestRegressor(n_estimators=20, random_state=rng)\nrf_cv_scores = cross_validate(rf, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\nrf_cv_scores\n</pre> In\u00a0[\u00a0]: Copied! <pre># NOTE we use df_cat here!!\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_cat.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\", \n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# fit the model\nreg.fit(X, y)\n</pre> # NOTE we use df_cat here!!  # drop nas as Linear Regression object cannot be trained on datasets with missing data df_linear_reg = df_cat.loc[: , [     \"cc_yield\",     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",      \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]].dropna()  # get X X = df_linear_reg.loc[:, [     \"gcvi_doy_151\",      \"gcvi_doy_171\",      \"ndvi_doy_151\",      \"ndvi_doy_171\",      \"mtci_doy_151\",      \"mtci_doy_171\",     \"slope_sr_FLAT\",     \"slope_sr_MODERATE SLOPE\",      \"slope_sr_SLIGHT SLOPE\",     \"slope_sr_STEEP SLOPE\"]]  # get Y y = df_linear_reg.loc[:, \"cc_yield\"]  # create a LinearRegression estimator object reg = LinearRegression()  # fit the model reg.fit(X, y) <p>Now we compute permuation importance using 30 shuffles of each feature. The results referenced by <code>p_imp</code> is a dictionary object with arrays showing the model error when each feature was randomly shuffled and for each repeat of the random shuffling. It also has a property <code>importances_mean</code> which is the mean increase in error across all iterations when a feature was randomly shuffled.</p> <p>We use the <code>permutation_importance()</code> function and pass in the <code>LinearRegression()</code> estimator, the features and labels data (<code>X</code> and <code>y</code>), the metric to evaluate model performance to the <code>scoring</code> argument, and specify the number of repeats with the <code>n_repeats</code> argument.</p> In\u00a0[\u00a0]: Copied! <pre>p_imp = permutation_importance(reg, X, y, scoring=\"neg_mean_absolute_error\", n_repeats=30, random_state=rng)\n</pre> p_imp = permutation_importance(reg, X, y, scoring=\"neg_mean_absolute_error\", n_repeats=30, random_state=rng) In\u00a0[\u00a0]: Copied! <pre>p_imp[\"importances_mean\"]\n</pre> p_imp[\"importances_mean\"] <p>Let's use this data to make a permutation importance plot that visualises the increase in error when a feature is randomly shuffled.</p> <p>First, let's get a list of column headings for each feature and convert the negative mean absolute error values to positive.</p> In\u00a0[\u00a0]: Copied! <pre>columns = X.columns\np_imp = abs(p_imp[\"importances_mean\"])\n</pre> columns = X.columns p_imp = abs(p_imp[\"importances_mean\"]) <p>Now, let's combine the feature labels and importance values into a <code>DataFrame</code>, sort the <code>DataFrame</code> by the importance scores, and generate a bar plot.</p> In\u00a0[\u00a0]: Copied! <pre>p_imp_df = pd.DataFrame({\"feature\": columns, \"importance\": p_imp})\np_imp_df = p_imp_df.sort_values(by=[\"importance\"], ascending=True)\np_imp_df\n</pre> p_imp_df = pd.DataFrame({\"feature\": columns, \"importance\": p_imp}) p_imp_df = p_imp_df.sort_values(by=[\"importance\"], ascending=True) p_imp_df In\u00a0[\u00a0]: Copied! <pre>fig = px.bar(p_imp_df, y=\"feature\", x=\"importance\", height=600)\nfig.show()\n</pre> fig = px.bar(p_imp_df, y=\"feature\", x=\"importance\", height=600) fig.show() answer 1 <p>Here, we filter out any data points representing mixed cropping fields using the condition <code>df[purestand] == 1</code> where a value of 1 in the <code>purestand</code> column indicates pure maize cropping.</p> <p>This approach means our vegetation indices should just be capturing information about maize crop condition.</p> <pre># get X and y data\n\n# drop mixed cropping fields\ndf_pure = df.loc[df[\"purestand\"] == 1, :]\n\n# one hot encode categorical predictors\ndf_pure = pd.get_dummies(df_pure, columns=[\"slope_sr\", \"soiltype_sr\", \"soilqual_sr\"])\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_pure.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\ncv_scores\n</pre> answer 2 <p>Here, we control for the presence of mixed cropping by using intercropping and crop rotation variables as features in the model.</p> <p>This approach might be suited to generating a maize crop prediction model that's applicable to the Ugandan context where mixed cropping is prevalent and pure maize fields are uncommon.</p> <pre># get X and y data\n\n# one hot encode categorical predictors\ndf_2 = pd.get_dummies(df, columns=[\"slope_sr\", \"soiltype_sr\", \"soilqual_sr\"])\n\n# drop nas as Linear Regression object cannot be trained on datasets with missing data\ndf_linear_reg = df_2.loc[: , [\n    \"cc_yield\",\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\",\n    \"intercrop_legume\",\n    \"intercrop_cassava\",\n    \"crop_rotation\"]].dropna()\n\n# get X\nX = df_linear_reg.loc[:, [\n    \"gcvi_doy_151\", \n    \"gcvi_doy_171\", \n    \"ndvi_doy_151\", \n    \"ndvi_doy_171\", \n    \"mtci_doy_151\", \n    \"mtci_doy_171\",\n    \"slope_sr_FLAT\",\n    \"slope_sr_MODERATE SLOPE\",\n    \"slope_sr_SLIGHT SLOPE\",\n    \"slope_sr_STEEP SLOPE\",\n    \"intercrop_legume\",\n    \"intercrop_cassava\",\n    \"crop_rotation\"]]\n\n# get Y\ny = df_linear_reg.loc[:, \"cc_yield\"]\n\n# create a LinearRegression estimator object\nreg = LinearRegression()\n\n# evaluate using 5-fold cross validation\ncv_scores = cross_validate(reg, X, y, cv=5, scoring=[\"neg_mean_absolute_error\"])\ncv_scores\n</pre>"},{"location":"notebooks/week-5_2/#machine-learning-2","title":"Machine Learning 2\u00b6","text":"<p>Machine learning is the process of learning from data to make predictions. Supervised machine learning models are trained to predict an outcome based on input data (predictors or features). The model is trained to minimise the error in predictions using a training set where both the outcome labels and input data are known. A key part of the machine learning model development workflow is evaluating model performance. This lab will introduce a technique for evaluating model performance in the context of limited training and test data: cross-validation.</p> <p>Previously, we demonstrated a workflow to develop a machine learning model for a classification task: predicting a field's crop type. In this lab we will develop a machine learning model for a regression task (predicting a continuous number) and evaluate the model using k-fold cross-validation.</p> <p>The task for this lab is to develop and evaluate a machine learning model that can predict smallholder farm maize crop yields in Uganda using remotely sensed vegetation indices as input features.</p> <p>In this lab you'll learn how to use Scikit-learns tools for evaluating models using cross-validation. We'll also introduce approaches for pre-processing categorical data to be used as features in machine learning models and techniques for interpreting the model and exploring how the model is making predictions.</p>"},{"location":"notebooks/week-5_2/#setup","title":"Setup\u00b6","text":""},{"location":"notebooks/week-5_2/#run-the-labs","title":"Run the labs\u00b6","text":"<p>You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.</p>"},{"location":"notebooks/week-5_2/#download-data","title":"Download data\u00b6","text":"<p>If you need to download the date for this lab, run the following code snippet.</p>"},{"location":"notebooks/week-5_2/#working-in-colab","title":"Working in Colab\u00b6","text":"<p>If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment.</p>"},{"location":"notebooks/week-5_2/#cross-validation","title":"Cross-validation\u00b6","text":"<p>A key part of the machine learning model development workflow is evaluating model performance. This should provide an assessment of how well a model will perform in making predictions on new or unseen data. Machine learning models are data hungry, the more examples the model sees during training the better it will be able to learn mappings that relate input features to outcome labels. However, we also want to test our model on a dataset that is representative of conditions the model might encounter \"in-the-wild\"; this results in setting aside a chunk of our ground truth dataset that cannot be used for model training. Thus, our model is not trained using all available ground truth data.</p> <p>One strategy that is deployed to maximise data available for model training and to provide an assessment of model performance is cross-validation.</p> <p>Before we explore our ground truth dataset for model development, let's quickly introduce cross-validation. Previously, we evaluated the model's performance by removing a random sample of the data prior to model training to use as a test set.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Why is it important for the test dataset to be randomly sampled from the ground truth data? We want the test dataset to be representative and unbiased to provide as realistic assessment of the model's performance on new data as possible.   What is a potential limitation of using a single hold-out randomly sampled test set for evaluating model performance? With a randomly sampled test set, each time the machine learning model development workflow is repeated new training and test sets would be generated and the model will have different performance scores. Using a single test set means, that by chance, the model could have an overly optimistic or pessimistic assessment of its performance.  <p>Further, by withholding a test set we reduce the amount of data available to train the model. A smaller training dataset can reduce the model's performance. Thus, as we're removing data to form the test set we'd expect the model's error to be larger than if we'd trained the model on the entire dataset.</p> <p></p> <p>In k-fold cross-validation there is not a single test set. Instead, the ground truth dataset is randomly split into $k$ folds. For example, if $k=5$ the ground truth dataset would be randomly split into 5 groups. Then, in turn, each fold is held out as a test set and the model is trained using data from the remaining four folds. Each fold takes a turn at being the test set. The model performance can be summarised using the average of the performance metrics generated using each fold. This means the model's performance is less susceptible to being influenced by a chance split of the ground truth data into training and test splits. It also means we can use the whole dataset to train the model and evaluate its performance.</p>"},{"location":"notebooks/week-5_2/#maps-2016-data","title":"MAPS 2016 Data\u00b6","text":"<p>The dataset we're using is the data from Lobell et al. (2019). Their analysis compared different approaches to estimating smallholder maize crop yields in Uganda: farmer reported yields, subplot crop cut samples, full plot crop cut samples, and satellite-based crop yield esimates.</p> <p>Boosting agricultural productivity in smallholder landscapes is important for improving a range of livelihood outcomes including food security and poverty alleviation. Accurate data on smallholder farmer crop production is a key ingredient to guiding development initiatives, government policies / programs, agricultural management and input use, and monitoring progress towards several Sustainable Development Goals.</p> <p>Traditionally, agricultural productivity in smallholder landscapes has been measured using farmer reported crop yields via surveys after harvests. These estimates are subject to considerable error impacting the quality of the data.</p> <p>More accurate measures of crop yield include physically harvesting a sub plot or full plot - crop cutting. However, crop cutting is more costly, time consuming, and requires liaising with farmers to generate large datasets of yield measurements.</p> <p>Lobell et al. (2019) explore the potential for using satellite data to measure crop yields in smallholder fields to address i) the issue of error in farmer reported yields, and ii) the cost of crop cutting.</p> <p>We're going to use the replication data from their paper and see if we can develop a machine learning model to accurately predict smallholder maize crop yield using satellite data as inputs. As this dataset only has a few hundred data points, we'll use all the data to train the model and test its performance using cross-validation</p>"},{"location":"notebooks/week-5_2/#import-modules","title":"Import modules\u00b6","text":""},{"location":"notebooks/week-5_2/#load-data","title":"Load data\u00b6","text":""},{"location":"notebooks/week-5_2/#explore-data","title":"Explore data\u00b6","text":"<p>There data that we have read into <code>df</code> includes a range of variables related to crop yield outcomes, farm management and farm type, and satellite-derived vegetation indices from the Sentinel-2 sensor.</p> <p>We'll be using the crop cut maize yield measures from sample sub plots in the fields as our outcome variable here - this is referenced by the column <code>cc_yield</code>. The units are Mg/ha.</p> <p>Let's look at the distribution of yield values.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you generate scatter plots to explore the correlation between vegetation index values and maize crop yield?</p>"},{"location":"notebooks/week-5_2/#cross-validation","title":"Cross-validation\u00b6","text":"<p>We'll start by training a linear regression model that predicts maize crop yield as a function of vegetation indices and evaluate its performance using cross validation.</p> <p>Maize crop yield is a continuous variable and we need a metric to evaluate model performance. We'll use the <code>mean_absolute_error</code> as the metric which is the mean absolute difference between predicted and observed crop yields. In this case, the mean absolute error will be computed using observations in the held out fold in cross-validation.</p> <p>We'll need to create a linear regression estimator object that we can train (using its <code>fit()</code> method). To evaluate the model using cross validation you call the <code>cross_val_score()</code> function on the dataset and the estimator. You pass in the metric you wish to use to evaluate the model to the <code>scoring</code> argument.</p> <p>We'll also need to use the <code>dropna()</code> method of pandas <code>DataFrame</code>s to remove missing data before training the model. Scikit-learn models cannot be trained on datasets with <code>NaN</code> values.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you estimate the mean mean absolute error and mean mean squared error across the five test folds using the <code>cv_scores</code> dictionary object?</p>"},{"location":"notebooks/week-5_2/#categorical-features","title":"Categorical features\u00b6","text":"<p>In our <code>DataFrame</code> there are some categorical variables that could help improve our predictions of crop yield. These include the slope, soil type, and soil quality variables. These variables are <code>str</code> type. We can only pass numeric data into our models; therefore, we'll need to recode the text data to a numeric representation.</p> <p>One approach for recoding categorical data is one hot encoding. Each unique value in a one hot encoded categorical variable is assigned a new column in the <code>DataFrame</code>. For rows when this value is present a value of one is assigned and zero otherwise.</p> <p>Let's one hot encode the slope variable <code>slope_sr</code> to illustrate this concept.</p> <p>The pandas <code>get_dummies()</code> function can be used to one hot encode a column in a <code>DataFrame</code>. The <code>get_dummies()</code> function has a <code>columns</code> argument that takes a list of column names that will be one hot encoded.</p> <p>First, let's visualise our <code>DataFrame</code> <code>df</code> and inspect the values in the <code>slope_sr</code> column. You should see the values \"FLAT\", \"MODERATE SLOPE\", \"SLIGHT SLOPE\", \"STEEP SLOPE\" as <code>str</code> data.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"<p>Can you also recode the soil type <code>soiltype_sr</code> and <code>soilqual_sr</code> variables from categorical to numeric using one hot encoding? Reference the result with the variable <code>df_2</code>.</p>"},{"location":"notebooks/week-5_2/#controlling-randomness","title":"Controlling randomness\u00b6","text":"<p>Some elements of the machine learning workflow are inherently random. For example, allocating data points to folds in k-fold cross-validation and bootstrap sampling of data to train decision trees in random forests.</p> <p>While this randomness is important (e.g. to ensure unbiased estimates of model performance when using cross-validation) it presents a challenge for reproducible results. The randomness of estimators (e.g. an instance of <code>RandomForestsRegressor()</code> or a cross-validation splitter is controlled by a <code>random_state</code> parameter.</p> <p>Some general tips on setting the <code>random_state</code> parameter:</p> <ul> <li>Never set <code>random_state</code> to <code>None</code> for reproducible results.</li> <li>Create a <code>RandomState</code> variable at the start of your program and pass it to all functions that accept a <code>random_state</code> argument. Look at the start of this notebook and see if you can sport where we create a <code>RandomState</code> variable just after we import the modules.</li> <li>If you're generating cross validation splits, use an integer value instead of a <code>RandomState</code> instance.</li> </ul> <p>This is quite an advanced topic, but important to ensure your results are reproducible. Generally, following the guidelines above is the best way to go. However, you can read more about this topic here.</p>"},{"location":"notebooks/week-5_2/#feature-importance","title":"Feature importance\u00b6","text":"<p>Machine learning models are often considered \"black boxes\". That is, it is not clear how the model is using input features to make predictions and what relationships it has learnt to relate features to outcomes.</p> <p>One strategy to make machine learning models more interpretable is to compute feature importance (or permutation importance). The feature importance is a measure of how much the error in a model's prediction increases when a feature is omitted from the model. Features with larger importance scores are therefore more important for making accurate predictions.</p> <p>The permutation feature importance score is computed as the decrease in a model's performance when a feature is randomly shuffled (permuted). This should ensure there is no relationship between the feature and the outcome.</p> <p>You can read more about feature importance in the Interpretable Machine Learning book and in the Scikit-learn docs.</p> <p>First, let's set up and fit a linear regression model that predicts maize crop yield using vegetation indices and field characteristics.</p>"},{"location":"notebooks/week-5_2/#recap-quiz","title":"Recap quiz\u00b6","text":"Do the feature importance results make sense? Can you explain them? The most important features for predictive importance are the vegetation indices. There is an established literature that vegetation indices are correlated with, and predictive of, crop yields.   <p>However, we should be cautious in interpreting the differences between vegetation indices as it is likely that the vegetation indices are correlated (even if they're designed to capture different information about vegetation growth and condition). When one of the vegetation indices is permuted (shuffled), it is likely the model will still have access to information about this feature through other features in the model which it is correlated with. You can read more about this here.</p> <p></p> Here, we computed the feature importance scores using the training data. Can you think of a limit to computing feature importance with the training set compared to using the test set? Computing feature importance using a held-out test set would indicate which features are important for the model's capacity to generalise well to unseen data. Features that are important for the training set migh be causing the model to overfit. You can read more about this here."},{"location":"notebooks/week-5_2/#final-activity","title":"Final activity\u00b6","text":"<p>You will notice in <code>df</code> that there are some columns related to mixed cropping in some of the maize fields (e.g.<code>intercrop_legume</code>, <code>intercrop_cassava</code>, <code>crop_rotation</code>, <code>purestand</code>). One issue that could be affecting model performance is that we're using average vegetation indices across the whole field but not all of the field is maize cropping. This means that our vegetation index data is not purely capturing a maize crop signal but also the condition of other crops. We might be able to improve the model's performance if we restrict our analysis to pure maize fields or control for the effect of mixed cropping.</p> <p>Can you create a training set that will use one or more of the <code>intercrop_legume</code>, <code>intercrop_cassava</code>, <code>crop_rotation</code>, and <code>purestand</code> variables to train a model to predict maize yield that accounts for the mixed cropping practices inherent in smallholder systems in Uganda. Evaluate your model using cross-validation and justify the rationale for your approach.</p>"}]}