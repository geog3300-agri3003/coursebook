{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using LLMs in Google Colab\n",
        "\n",
        "This notebook demonstrates how you can use LLMs programmatically within a Jupyter notebook environment on Google Colab.\n",
        "\n",
        "While you can easily use LLMs via their apps or website chat interfaces, it's also possible to programmatically pass prompts to these models and capture their responses. You can also download the actual LLM to your environment and pass prompts to it directly (i.e. you are not passing information to a model hosted by Google or OpenAI or Meta) There are some advantages to doing this:\n",
        "\n",
        "* Access LLMs directly within your coding environment\n",
        "* Run LLMs on your own hardware or within a personal environment, this is useful when you are working with private data that you cannot share or upload to hosted LLMs\n",
        "* Demonstrates how you can build custom apps and workflows on top of LLM technologies\n",
        "\n",
        "To use LLMs within Jupyter notebooks on Google Colab, there are two initial setup steps:\n",
        "\n",
        "1. Create a <a href=\"\" target=\"_blank\">Hugging Face</a> account and add your account token to the Google Colab Secrets. Then restart your Google Colab session.\n",
        "2. Ensure that you are using the GPU Google Colab runtime.\n",
        "\n",
        "## Hugging Face\n",
        "\n",
        "<a href=\"https://huggingface.co/huggingface\" target=\"_blank\">Hugging Face</a> are a machine learning and AI technology company that host trained models and develop software tools for working with AI models.\n",
        "\n",
        "### Hugging Face accounts and tokens\n",
        "\n",
        "To use many of the models on Hugging Face you need to create an account and store your accounts API key in Google Colab's secrets. This lets you make authenticated calls to Hugging Face.\n",
        "\n",
        "### Hugging Face pipelines\n",
        "\n",
        "A <a href=\"https://huggingface.co/docs/transformers/en/pipeline_tutorial\" target=\"_blank\">pipeline</a> is a Python object that provides a wrapper around the code required to use LLMs and other AI tools. In short, pipelines take away a lot of the work of setting up AI models within your Python environment and the associated data processing. You simply create a `pipeline` object, pass your prompt in, and `print()` the response."
      ],
      "metadata": {
        "id": "0MrIj1EPU4fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "### Setup Hugging Face token\n",
        "\n",
        "1. Create a Hugging Face account:\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-1-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "Once, you have created a Hugging Face account you can gain access to models. Many LLMs require you accept terms and conditions. For this exercise we will be working with Google's Gemma 2 2b it model (2b represents 2 billion parameters and it represents instruction tuned). Agree to the model's terms and conditions and usage policy <a href=\"https://huggingface.co/google/gemma-2-2b-it\" target=\"_blank\">here</a>.\n",
        "\n",
        "2. Create an *Access Token*:\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-2-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "3. Click on *Create new token*\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-3-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "4. Set the token permissions\n",
        "\n",
        "You can initially set token permissions to *Read*, which has read access to all your resources. This is a good option for getting started.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-4-read-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "However, as you start developing resources such as models and datasets and using Hugging Face in different environments, it's a good idea to create access tokens with fine-grained permissions with just enough permissions to complete tasks associated with the token.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-4-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "If you have selected fine-grained permissions, you will need to add repositories (models) that you want that token to grant permission to.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-5-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "6. Click *Create token* to generate the access token.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-6-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "7. Copy the access token. **This is your only opportunity to do this - keep a record of the token (in a secure location)**. If you lose your token, it's easy to generate a new one.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-7-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "8. In Google Colab, click on the *key* icon in the left-hand sidebar.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-8-annotated.png\" alt=\"HF account\" width=\"75%\">\n",
        "\n",
        "9. Add your Hugging Face access token with the name `HF_TOKEN` and make sure the Notebook access it checked. **Restart your Google Colab session to load the token into your environment**.\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/hf-9-annotated.png\" alt=\"HF account\" width=\"50%\">\n",
        "\n",
        "### Setup runtime\n",
        "\n",
        "**This lab will only work (quickly) using Google Colab with a *T4 GPU* runtime type.**\n",
        "\n",
        "**Before running any code, set the runtime type to *T4 GPU*.**\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/week-1-colab-runtime-1.jpg\" alt=\"colab runtime menu\" width=\"50%\">\n",
        "\n",
        "<p></p>\n",
        "\n",
        "<img src=\"https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/week-1-colab-runtime-2.jpg\" alt=\"colab runtime menu\" width=\"50%\">\n",
        "\n",
        "### Install packages"
      ],
      "metadata": {
        "id": "peRxOvC0TQ5h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VfMacu5BNYpG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `pipeline` objects\n",
        "\n",
        "To create a `pipeline` object call the `pipeline()` instantiator function as pass in the task (`\"text-generation\"` here), the model (`\"google/gemma-2-2b-it\"`), and some additional arguments. Set the `device_map` argument to `\"auto\"` to run the model on Colab's GPU when it is available.\n",
        "\n",
        "This will download the model to your Colab environment, which will take a moment or two."
      ],
      "metadata": {
        "id": "RCyHwhBYTPer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/gemma-2-2b-it\",\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "w2AoSpwySUk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can pass in a list of prompts to `pipe` and specify the number of tokens you want generated (`max_new_tokens`).\n",
        "\n",
        "Each prompt is a dictionary object with keys for `\"role\"` and `\"content\"`. The text values associated with the `\"content\"` key is the prompt provided to the model.\n",
        "\n",
        "`pipe` returns a list of dictionary objects storing the conversation between the user and the AI assistant."
      ],
      "metadata": {
        "id": "KXwJbP93lDFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the distance between two points?\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(prompts, max_new_tokens=512)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "LzzMIrULTlqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's a little fiddly to extract the generated response, but it's a good exercise to practice extracting information from lists and dictionary objects:\n",
        "\n",
        "1. subset the first element in the list object `outputs[0]` (remember Python indexes from 0)\n",
        "2. get the data in the dict under the `\"generated_text\"` key\n",
        "3. get the last element in that list (which stores the user-assistant conversation); `-1` means index the last element\n",
        "4. subset the text stored against the `\"content\"` key\n",
        "\n",
        "Work through this to make sure you understand the different ways of subsetting data from a list (subsetting by index position) and a dict (by key).\n",
        "\n",
        "Calling `strip()` on the string text data removes trailing and leading whitespace.\n",
        "\n",
        "Finally, `print()` the response from the AI assistant."
      ],
      "metadata": {
        "id": "c2WatIugo1Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Q8IuMuiinDK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Recap quiz\n",
        "\n",
        "<b>Understanding a Python program, can you identify:\n",
        "    <ul>\n",
        "        <li>What is the purpose of the program?</li>\n",
        "        <li>What input data does the program require?</li>\n",
        "        <li>What data type is the input data?</li>\n",
        "        <li>What user defined functions does the program use?</li>\n",
        "        <li>What built in operations does the program use?</li>\n",
        "        <li>What data type is the data returned by the function?</li>\n",
        "    </ul>\n",
        "</b>\n",
        "\n",
        "We can copy and paste the Python code out of the printed response by the LLM and execute it (the Python code is contained within ``` marks)."
      ],
      "metadata": {
        "id": "aoC_iWCGrS4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTE! If you run the code above different Python code could be generated as LLMs are probabilistic.\n",
        "\n",
        "import math\n",
        "\n",
        "def distance(x1, y1, x2, y2):\n",
        "  \"\"\"Calculates the distance between two points.\n",
        "\n",
        "  Args:\n",
        "    x1: The x-coordinate of the first point.\n",
        "    y1: The y-coordinate of the first point.\n",
        "    x2: The x-coordinate of the second point.\n",
        "    y2: The y-coordinate of the second point.\n",
        "\n",
        "  Returns:\n",
        "    The distance between the two points.\n",
        "  \"\"\"\n",
        "  return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
        "\n",
        "# Example usage\n",
        "x1 = 1\n",
        "y1 = 2\n",
        "x2 = 4\n",
        "y2 = 6\n",
        "\n",
        "distance_between_points = distance(x1, y1, x2, y2)\n",
        "print(f\"The distance between the points ({x1}, {y1}) and ({x2}, {y2}) is: {distance_between_points}\")"
      ],
      "metadata": {
        "id": "HHrNPhh-JpHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs are helpful as assistants when trying to solve more complex coding or spatial analysis tasks.\n",
        "\n",
        "#### Activities\n",
        "\n",
        "</b>\n",
        "Create prompts and generate responses for each of the following tasks. Use the code snippets above as an example of how to use <code>pipeline</code> objects.\n",
        "\n",
        "* Computing the distance between two points on a sphere.\n",
        "* Computing the shortest line that joins three points.\n",
        "* Computing the length of all the edges of a polygon.\n",
        "</b>\n",
        "\n",
        "<details>\n",
        "    <summary><b>How can you verify the response from the LLM is correct?</b></summary>\n",
        "<ul>\n",
        "<li>Ask the LLM to generate test examples (check the tests are correct).</li>\n",
        "<li>Look at how the LLM has solved the problem and compare these solutions with what you find from independent research.</li>\n",
        "<li>Generate a suite of cases where you know the correct answer and compare these ground truth values with what is returned when executing the LLM generated solution.</li>\n",
        "</ul>\n",
        "</details>"
      ],
      "metadata": {
        "id": "5UkQ0XlhJxoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "    <summary><b>Example prompts with code snippets (note, you might need to change the number of tokens you want generated to capture the full response).</b></summary>\n",
        "\n",
        "```python\n",
        "prompts = [\n",
        "    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the distance between two points on a sphere? The points are represented as latitude and longitude pairs.\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(prompts, max_new_tokens=512)\n",
        "\n",
        "response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "print(response)\n",
        "```\n",
        "\n",
        "```python\n",
        "prompts = [\n",
        "    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the shortest line between three points?\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(prompts, max_new_tokens=1024)\n",
        "\n",
        "response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "print(response)\n",
        "```\n",
        "\n",
        "```python\n",
        "prompts = [\n",
        "    {\"role\": \"user\", \"content\": \"Can you generate a short Python program that computes the length of all the edges of a Polygon?\"},\n",
        "]\n",
        "\n",
        "outputs = pipe(prompts, max_new_tokens=1024)\n",
        "\n",
        "response = outputs[0][\"generated_text\"][-1][\"content\"].strip()\n",
        "print(response)\n",
        "```\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "yMxzt2LdEuYq"
      }
    }
  ]
}