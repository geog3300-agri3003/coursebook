{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49a8ad52-bc4f-46a8-b4e4-54069c40787f",
   "metadata": {},
   "source": [
    "# Data wrangling: raster data \n",
    "\n",
    "This lab will demonstrate a range of techniques for wrangling (transforming) raster data. This will include techniques to tidy, check, and visualise data; data subsetting; and various operations to transform image pixel values or summarise raster datasets. \n",
    "\n",
    "Here, you will work with two Sentinel-2 satellite images captured before and after <a href=\"https://emergency.copernicus.eu/mapping/list-of-components/EMSR489\" target=\"_blank\">Tropical Cyclone Yasa struck Fiji in December 2020</a>. Tropical Cyclone Yasa made landfall in Fiji on 17-18 December with heavy rain and storm surges causing flooding. You will convert these images into datasets that reflect the presence of water and moisture on the land surface and conduct a change detection exercise comparing pre and post event images to estimate the area of cyclone induced flooding. Here, we'll be focusing on flood impacts on croplands surrounding Labasa on the island of Viti Levu. \n",
    "\n",
    "![](https://github.com/geog3300-agri3003/coursebook/raw/main/docs/img/week-4-disaster-charter-sentinel-1-labasa.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb9568-ba7a-4b1d-a8b2-d82f48d8af1a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Run the labs\n",
    "\n",
    "You can run the labs locally on your machine or you can use cloud environments provided by Google Colab. **If you're working with Google Colab be aware that your sessions are temporary and you'll need to take care to save, backup, and download your work.**\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/geog3300-agri3003/coursebook/blob/main/docs/notebooks/week-4_1.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### Download data\n",
    "\n",
    "If you need to download the data for this lab, run the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea23dc4c-38d8-40e4-a3bb-eb524767613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"data_lab-4_1\" not in os.listdir(os.getcwd()):\n",
    "    subprocess.run('wget \"https://github.com/geog3300-agri3003/lab-data/raw/main/data_lab-4_1.zip\"', shell=True, capture_output=True, text=True)\n",
    "    subprocess.run('unzip \"data_lab-4_1.zip\"', shell=True, capture_output=True, text=True)\n",
    "    if \"data_lab-4_1\" not in os.listdir(os.getcwd()):\n",
    "        print(\"Has a directory called data_lab-4_1 been downloaded and placed in your working directory? If not, try re-executing this code chunk\")\n",
    "    else:\n",
    "        print(\"Data download OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d5f96-c643-4545-a155-198129691c6e",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "\n",
    "If you're working in Google Colab, you'll need to install the required packages that don't come with the colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf884833-952f-4db9-bdea-5a015eda8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install xarray[complete]\n",
    "    !pip install rioxarray\n",
    "    !pip install mapclassify\n",
    "    !pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ffa198-a062-4e9c-971f-9218a10ceda6",
   "metadata": {},
   "source": [
    "## What is data wrangling?\n",
    "\n",
    "<a href=\"https://r4ds.had.co.nz/wrangle-intro.html\" target=\"_blank\">Wickham and Grolemund (2017)</a> and <a href=\"https://wesmckinney.com/book/\" target=\"_blank\">McKinney (2022)</a> state that data wrangling consists of data import, data cleaning, and data transformation. \n",
    "\n",
    "#### Data import\n",
    "\n",
    "Data import was covered in week 3 with examples of how to read tabular, vector, and raster data into Python programs. \n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "Data cleaning includes handling outliers and missing data. Here, we'll cloud mask Sentinel-2 remote sensing images, which is a data cleaning exercise. \n",
    "\n",
    "#### Data transformation\n",
    "\n",
    "<a href=\"https://wesmckinney.com/book/\" target=\"_blank\">McKinney (2022)</a> define data transformation as mathematical or statistical operations applied to data to generate new datasets. Data transformation can also include operations that reshape datasets or combine two or more datasets.\n",
    "\n",
    "<details>\n",
    "    <summary><b>Detailed notes on data transformation for spatial and non-spatial data</b></summary>\n",
    "<p></p>\n",
    "As we're working with spatial and non-spatial data we can categorise data transformation operations as attribute operations, spatial operations, geometry operations, and raster-vector  operations (<a href=\"https://geocompr.robinlovelace.net/index.html\" target=\"_blank\">Lovelace et al. (2022)</a>).\n",
    "<p></p>\n",
    "    \n",
    "**Attribute operations** are applied to non-spatial (attribute data). This could be a tabular dataset without any spatial information, the attribute table of a vector dataset, or the pixel values of a raster dataset. Common attribute operations include:\n",
    "\n",
    "* Selecting columns from a table based on a condition. \n",
    "* Selecting (subsetting) pixels from a raster based on a condition.\n",
    "* Filtering rows from a table based on a condition. \n",
    "* Creating a new column of values using a function applied to existing data.\n",
    "* Computing summary statistics of columns in a table or of pixel values in a raster.\n",
    "* Joining datasets based on matching values in columns (keys).\n",
    "\n",
    "**Spatial operations** transform data using the data's geographic information including shape and location. Vector spatial operations include:\n",
    "\n",
    "* Spatial subsetting by selecting data points based on a geographic condition (e.g. selecting all fields in Western Australia).\n",
    "* Spatial joins where datasets are combined based on their relationship in space. \n",
    "* Spatial aggregation where summaries are produced for regions (e.g. the average crop yield for all fields in a region).\n",
    "\n",
    "Spatial operations on raster data are based on map algebra concepts and include:\n",
    "\n",
    "* Local operations which are applied on a pixel by pixel basis (e.g. converting a raster of temperature values in °F to °C).\n",
    "* Focal operations which summarise or transform a raster value using the values of neihbouring pixels (e.g. computing the average value within a 3 x 3 pixel moving window).\n",
    "* Zonal operations which summarise or transform raster values using values inside an irregular shaped zone.\n",
    "* Global operations which summarise the entire raster (e.g. computing the minimum value in the raster dataset). \n",
    "\n",
    "**Geometry operations** transform a dataset's geographic information. Common geometry operations for vector data include:\n",
    "\n",
    "* Simplification of shapes.\n",
    "* Computing the centroid of polygons.\n",
    "* Clipping (subsetting) of geometries based on their intersection or relationship with another geometry. \n",
    "\n",
    "and geometry operations on raster data typically involve changing the spatial resolution and include:\n",
    "\n",
    "* Aggregation or dissagregation.\n",
    "* Resampling.\n",
    "\n",
    "**Raster-vector operations** involve both raster and vector datasets and include:\n",
    "\n",
    "* Cropping or masking raster data using a vector geometry.\n",
    "* Extracting raster values that intersect with a vector geometry.\n",
    "* Rasterisation where a vector dataset is transformed to a raster layer.\n",
    "* Vectorisation where a raster dataset is transformed to a vector layer.\n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "In this lab we're focusing on data transformation operations applied to raster data. Raster data breaks the Earth's surface up into a grid of cells (pixels). Each pixel is assigned a value that corresponds to the geographic feature or phenomenon of interest. In particular, we'll be working with remote sensing images where each pixel in a two dimensional raster stores a reflectance value (i.e. how much incoming light was reflected off the portion of the Earth's land surface that the pixel represents). \n",
    "\n",
    "To complete the task of creating a map of flooding following Tropical Cyclone Yasa, we will apply a range of subsetting, map algebra, and geometry operations to transform mutlispectral Sentinel-2 remote sensing images into a flood map.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b20318c-ebd1-47d1-ace5-c51a5cb2d488",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e9966-ecfe-4c85-ba68-7b787c71402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import stackstac\n",
    "from collections import OrderedDict\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "# setup renderer\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f980c-8123-47ee-8ce0-96820bf9622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data_lab-4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dab49d-41a3-4194-9c5b-69313520a171",
   "metadata": {},
   "source": [
    "### Lab data\n",
    "\n",
    "Let's quickly inspect the files that we have downloaded for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949022a-679b-4d13-9d8a-a6089303be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27bc98-655f-4a30-b0b0-3fa573adf702",
   "metadata": {},
   "source": [
    "* `s2_tc_yasa_pre_event.tif` is a four band Sentinel-2 image from 25 October 2020. The four bands are blue, green, red, and near infrared (NIR).\n",
    "* `s2_tc_yasa_pre_event_cloud_probability.tif` is a cloud probability raster for the Sentinel-2 image on the 25 October 2020. Each pixel has a value between 0 and 100 indicating the probability of that pixel being cloud covered.\n",
    "* `s2_tc_yasa_post_event.tif` is a four band Sentinel-2 image from 19 December 2020. The four bands are blue, green, red, and near infrared (NIR).\n",
    "* `s2_tc_yasa_post_event_cloud_probability.tif` is a cloud probability raster for the Sentinel-2 image on the 19 December 2020. Each pixel has a value between 0 and 100 indicating the probability of that pixel being cloud covered.\n",
    "\n",
    "The cloud probability rasters are generated using <a href=\"https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_CLOUD_PROBABILITY#description\" target=\"_blank\">Sentinel Hub's sentinel2-cloud-detector machine learning algorithm</a>.\n",
    "\n",
    "## Xarray refresher\n",
    "\n",
    "<a href=\"\" target=\"_blank\">Xarray</a> `DataArray` data structures are objects that store multidimensional arrays of raster values and also store metadata information that describe the raster values. `xarray.DataArray` objects have the following properties:\n",
    "\n",
    "* `values`: the multidimensional array of raster values\n",
    "* `dims`: a list of names for the dimensions of the array (e.g. instead of axis 0 describing the 0th (row) dimension of an array, that dimension can have a descriptive label such as longitude)\n",
    "* `coordinates`: a `list` of array-like objects that describe the location of an array element along that dimension (e.g. a 1D array of longitude values describing the location on the Earth's surface for each row in the array)\n",
    "* `attrs`: a `dict` of metadata attributes describing the dataset\n",
    "\n",
    "`xarray.DataArray` objects can be stored within a larger container called an `xarray.Dataset`. An `xarray.Dataset` can store many `xarray.DataArray` objects that share `dims` and `coordinates`. This is useful if you have arrays of different `Variables` that correspond to the same locations and time-periods (e.g. you could have a separate array for temperature and precipitation values organised within a single `xarray.Dataset`).\n",
    "\n",
    "![](https://docs.xarray.dev/en/stable/_images/dataset-diagram.png)\n",
    "\n",
    "*Schematic of an xarray.Dataset (source: xarray Getting Started)*\n",
    "\n",
    "### Data input\n",
    "\n",
    "The `rioxarray` package provides tools for reading and writing raster geospatial data files into `xarray.DataArray` objects.\n",
    "\n",
    "Let's pass the path to a GeoTIFF file of the pre Tropical Yasa Sentinel-2 image into the `rioxarray` `open_rasterio()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46c75d-8a79-4f98-ace0-2742a2aeda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tc_yasa_s2_path = os.path.join(data_path, \"s2_tc_yasa_pre_event.tif\")\n",
    "pre_s2 = rxr.open_rasterio(pre_tc_yasa_s2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67b450-b229-4aa8-97a7-dfac9a64fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f500a-e0e6-4ded-9530-294f1234de64",
   "metadata": {},
   "source": [
    "## Subsetting data\n",
    "\n",
    "<a href=\"https://docs.xarray.dev/en/latest/user-guide/indexing.html#indexing-and-selecting-data\" target=\"_blank\">Subsetting data (or selecting data)</a> refers to operations that extract a subset of data from a larger dataset. Subsetting operations can be spatial or non-spatial. \n",
    "\n",
    "**Spatial subsetting** operations select data based on their location in space (e.g. extracting all paddocks that intersect with a farm boundary, or subsetting all remote sensing image pixels within a geographic region). \n",
    "\n",
    "**Non-spatial subsetting** operations select data based upon their attribute values or another non-spatial condition. For example, we could subset all values in a column of temperature measurements where the temperature was greater than 30 °C or all temperature measurements that match a date range. We can also subset data based on their position within a dataset (e.g. select the first 10 rows of a DataFrame or columns by column name). Subsetting data based on their position within the dataset is often referred to as indexing. \n",
    "\n",
    "`xarray.DataArray` objects support label-based and positional subsetting:\n",
    "\n",
    "* With positional subsetting, we can access `xarray.DataArray` values by selecting them using `[]` and index positions. For example, `demo_ds[0:4, 0:3, 0:3]` would select the first four bands (along the 0th dimension), first three rows (along the 1st dimension), and first three columns of array (along the 2nd dimension) of the array.\n",
    "* With label-based subsetting we can use the `sel()` method to select elements from the array using dimension names. For example, `demo_ds.sel(time=\"2020-01-31, bands=[1, 2])` would select array values that correspond to the the 31st January 2020 and have band labels 1 and 2. \n",
    "\n",
    "**It is recommended that you read the xarray guide on <a href=\"https://docs.xarray.dev/en/latest/user-guide/indexing.html#indexing-and-selecting-data\" target=\"_blank\">Subsetting data (or selecting data)</a>.**\n",
    "\n",
    "To visualise the pre Tropical Cyclone Yasa Sentinel-2 image as an RGB image on our display, we need to select the red, green, and blue bands. This is a good use case for the `sel()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19875e0d-2726-4d32-b321-4348edd7e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2.sel(band=[3, 2, 1]).plot.imshow(vmin=0, vmax=2000, add_labels=False, aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f641cd-85e8-401b-a792-bddd550ee6c5",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you visualise the `pre_s2` image as a false colour composite where near infrared is rendered as red, red is rendered as green, and green is rendered as blue on the display?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbee345-f00a-4bf4-922e-e35ce3602169",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f3833-711c-45b1-b91d-ee90aa702a12",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "# Data inspection\n",
    "pre_s2.sel(band=[4, 3, 2]).plot.imshow(vmin=0, vmax=2000, add_labels=False, aspect=3, size=4)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb15d2-298b-4029-a38d-78e32a9750c5",
   "metadata": {},
   "source": [
    "As discussed in previous labs, one of the nice features of `xarray.DataArray` objects is the ability to store information about the array data as dimensions with associated coordinates. Let's add a time dimension to store the date of image capture. This is important as our workflow involves change detection, so it's useful to keep track of which array corresponds to remote sensing images before the Tropical Cyclone event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a51a2-a593-4708-87fc-c577204f58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time dimension and coords\n",
    "pre_s2 = pre_s2.expand_dims(dim={\"time\": [pd.to_datetime(\"2020-10-25\")]}, axis=0)\n",
    "pre_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7942f-f14b-4fc4-8752-4014c371e54c",
   "metadata": {},
   "source": [
    "## Map algebra\n",
    "\n",
    "Following <a href=\"https://geocompr.robinlovelace.net/index.html\" target=\"_blank\">Lovelace et al. (2022)</a>, we refer to map algebra as operations that transform raster pixel values via statistical or mathematical operations which can involve combining pixel values from different raster layers or using neighbouring raster values. \n",
    "\n",
    "<a href=\"https://geocompr.robinlovelace.net/index.html\" target=\"_blank\">Lovelace et al. (2022)</a> outline four different types of map algebra operations:\n",
    "\n",
    "### Local operations\n",
    "\n",
    "**Local** map algebra operations operate on a pixel by pixel basis; the mathematical operation is applied independently to each pixel without reference to neighbouring pixel values. For example, addition, subtraction, multiplication, and logical operations can all be applied on a pixel by pixel basis. \n",
    "\n",
    "Commonly used local operations when working with remote sensing data are computing spectral indices or masking out cloudy pixels. Spectral indices are pixel by pixel mathematical combinations of spectral reflectance in different wavelengths that are used to monitor vegetation or land surface conditions. Read <a href=\"https://doi.org/10.1038/s43017-022-00298-5\" target=\"_blank\">Zeng et al. (2022)</a> for a review of vegetation indices.\n",
    "\n",
    "The normalised difference vegetation index (NDVI) is used for tracking vegetation condition and representing the greenness of vegetation in a remote sensing image. \n",
    "\n",
    "The NDVI is computed as:\n",
    "\n",
    "$NDVI=\\frac{NIR-red}{NIR+red}$\n",
    "\n",
    "Thus, the NDVI is computed via division, subtraction, and addition operations computed on a pixel by pixel basis using raster data corresponding to red and near infrared reflectance. \n",
    "\n",
    "### Focal / neighbourhood operations\n",
    "\n",
    "Focal operations update a pixel's value using a combination of values from a regular shaped neighbourhood centred on the focal pixel (e.g. 3 x 3 or 5 x 5 pixel neighbourhood). An example of focal operations are dilation operations, which assign the focal pixel the maximum value found within the neighbourhood. This is often used to extend the coverage of cloud masks to conservatively remove thin clouds / poor atmospheric conditions in remote sensing images. \n",
    "\n",
    "### Zonal operations\n",
    "\n",
    "Zonal operations summarise raster values in a target layer using a categorical zones raster layer to identify zones. For example, if we have a raster layer of NDVI values and a zones layer where each pixel value represents a crop type. A zonal operation computes the mean NDVI for for each crop type. \n",
    "\n",
    "### Global operations\n",
    "\n",
    "Global operations compute summary statistics for an entire raster layer. For example, we could have a raster layer indicating the presence of forest or non-forest areas. Counting the number of forest pixels and multiplying the count by the pixel area creates global summary statistic of the area of forest cover. \n",
    "\n",
    "## Local operations\n",
    "\n",
    "As mentioned above, local operations perform mathematical operations on arrays on a pixel by pixel basis. To identify and compute the area of cropland flooded by Tropical Cyclone Yasa, we will need to perform a range of local operations:\n",
    "\n",
    "1) Convert a cloud probability raster into a binary cloud mask (i.e. set all pixels to `True` where cloud probability is less than 50%).\n",
    "2) Use the cloud mask to mask all cloudy pixels in Sentinel-2 multispectral satellite images (i.e. set all pixels to no data where the cloud mask is `False` - cloudy pixels).\n",
    "3) Compute a spectral index, the normalised difference water index (NDWI), using bands of green and near infrared reflectance to highlight water and moisture presence.\n",
    "4) Compute a difference image using the before and after Tropical Cyclone Yasa NDWI images to identify locations of change which are indicative of flood impacts.\n",
    "5) Threshold the difference image to identify pixels where a large change in NDWI occurred to represent an estimate of flood extent.\n",
    "6) Mask out non-cropland pixels using a land cover map to return an array of flooded cropland pixels.\n",
    "\n",
    "An example of a local operation is dividing each pixel value by 10000 (or multiply by 0.0001). This is necessary because the data is spectral reflectance so should have a value between 0 and 1 (i.e. the ratio of incoming light to reflected light off the land surface in a spectral band). If you look at the <a href=\"https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED#bands\" target=\"_blank\">information page for the Sentinel-2 remote sensing data</a> we are using here, you can see it has a scale factor of 0.0001. This means the reflectance data has been scaled by multiplying it by 10000 so it can be stored as integer type. We need to convert it back to reflectance units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97581e71-7d32-40f7-87a0-d348139def2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2 = pre_s2 / 10000\n",
    "pre_s2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ba341-795a-43d4-8a9f-69a4a241dea9",
   "metadata": {},
   "source": [
    "### Cloud masking\n",
    "\n",
    "As you can see above, there are clouds obscuring parts of the land surface in the pre Tropical Cyclone Yasa Sentinel-2 image. We'll need to mask out those clouds. Let's start by loading a cloud probablity raster layer where each pixel value is a cloud probability score for the Sentinel-2 image taken before Tropical Cyclone Yasa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296b059-6d84-4082-a020-f9d9228b6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2_cloud_prob_path = os.path.join(data_path, \"s2_tc_yasa_pre_event_cloud_probability.tif\")\n",
    "pre_s2_cloud_prob = rxr.open_rasterio(pre_s2_cloud_prob_path)\n",
    "pre_s2_cloud_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fb517-e494-43e6-aa68-dba22433d070",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you use the `plot.imshow()` method of an `xarray.DataArray` object to visualise the cloud probability raster?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4f34b-9010-4b6c-adf7-365076765ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8851360-b161-48b4-b147-a7f7e504ae0a",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "pre_s2_cloud_prob.sel(band=1).plot.imshow(aspect=3, size=4)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f155ec-f8d6-4d61-a528-cb6be46ffa96",
   "metadata": {},
   "source": [
    "Next, we need to convert the `pre_s2_cloud_prob` `xarray.DataArray` of cloud probability scores (i.e. values between 0 and 100) to a binary array where pixels have the value of `True` if they're not cloudy. We can use a cloud probability threshold of 50, where a value less than 50 indicates no cloud. This involves applying a less than (`<`) operation to every pixel in `pre_s2_cloud_prob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e0e14-5c20-4425-a346-97c65f97009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2_cloud_mask = pre_s2_cloud_prob < 50\n",
    "pre_s2_cloud_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e65c1-36d6-4485-af16-7ef41ed5ee22",
   "metadata": {},
   "source": [
    "We can plot the cloud mask `pre_s2_cloud_mask` to see which pixels are clear and which are cloudy. Note, that `True` values will be rendered as 1 and `False` as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad207c40-975a-44d6-9b3b-7675d81f0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2_cloud_mask.sel(band=1).plot.imshow(aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0926bb3-f585-412b-8055-03afc97920e4",
   "metadata": {},
   "source": [
    "Now use the cloud mask to set all pixels in the `pre_S2` `xarray.DataArray` of Sentinel-2 multispectral data to NaN (not a number - a no data indicator). We can use the <a href=\"https://docs.xarray.dev/en/stable/user-guide/indexing.html#masking-with-where\" target=\"_blank\">`where()` method</a> of `xarray.DataArray` objects to do this. \n",
    "\n",
    "The `where()` method takes in an array of bool type values (`True`, `False`) and sets all pixels to NaN (no data) where the value passed into `where()` is `False`. Or, `where()` can take a conditional or comparison statement that evaluates to `True` or `False`.\n",
    "\n",
    "Let's apply the cloud mask to `pre_s2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2643e8-e0f8-4fa7-8ecf-56567cd76f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2_cm = pre_s2.where(pre_s2_cloud_mask.sel(band=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb74aa3-f127-4aed-ae74-99a0e840711c",
   "metadata": {},
   "source": [
    "We also need to update the metadata for the `pre_s2` `xarray.DataArray` object to identify the no data value. This is important for when we save the array data to a file (e.g. GeoTIFF file) to keep a record of which pixels have no data. We can do this using the `rio` accessor from <a href=\"https://corteva.github.io/rioxarray/stable/getting_started/nodata_management.html\" target=\"_blank\">rioxarray</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe037621-26fc-40b1-92bc-a52aa5c76ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2_cm.rio.write_nodata(np.nan, encoded=True, inplace=True)\n",
    "# check no data value has been set\n",
    "print(f\"nodata: {pre_s2_cm.rio.nodata}\")\n",
    "print(f\"encoded_nodata: {pre_s2_cm.rio.encoded_nodata}\")\n",
    "pre_s2_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f6545-ac84-43a9-a8a4-cd823d3bc96c",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**How could you check that the cloud mask has been successfully applied?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b47de1-1381-4d0f-871e-29bb92cd8c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb32939-48af-4907-9682-a1e47affa983",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "You could print <code>pre_s2_cm</code> and inspect the array values to see if any have been converted to NaN. Or, better, you could plot the RGB image and see if the clouds have been masked out.\n",
    "\n",
    "```python\n",
    "pre_s2_cm.sel(time=\"2020-10-25\", band=[3, 2, 1]).plot.imshow(vmin=0, vmax=0.2, add_labels=False, aspect=3, size=4)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d17bf-e968-45c0-983c-0d70808c2cce",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you repeat the process we have gone through above to cloud mask the post Tropical Cyclone Yasa Sentinel-2 image?**\n",
    "\n",
    "**First, you will need to create a path to the file `s2_tc_yasa_post_event.tif`, and then you will need to read the file into an `xarray.DataArray` object using rioxarray's `open_rasterio()` method. Use the variable name `post_s2` as a reference for the `xarray.DataArray` object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73cb4c-46e2-4946-aafe-f3325516465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf79d9-0f60-4f9d-b86b-5d78b982e7ef",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "post_tc_yasa_s2_path = os.path.join(data_path, \"s2_tc_yasa_post_event.tif\")\n",
    "post_s2 = rxr.open_rasterio(post_tc_yasa_s2_path)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84201859-b5b1-4d43-a66c-aacb2ec62ab4",
   "metadata": {},
   "source": [
    "**Next, you need to add a time dimension as the first (0th) axis. Set the time value to `\"2020-12-19\"`, which corresponds to the date of the Sentinel-2 image capture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b64b7-e955-4989-a0cf-81bf70dab2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d61e22-e2bf-4e1d-9abb-b3c61f41dc76",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "# add time dimension and coords\n",
    "post_s2 = post_s2.expand_dims(dim={\"time\": [pd.to_datetime(\"2020-12-19\")]}, axis=0)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee8b1b-dff1-44fd-8cc3-2f41235ff1af",
   "metadata": {},
   "source": [
    "**Now, can you rescale the Sentinel-2 multispectral reflectance values to be between 0 and 1 by dividing all array values by 10000?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b79f4-1707-4ac3-80c3-a6f0a8e23cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abb7e9-f01f-4148-a3f4-6f2719cf2bd3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "# rescale reflectance to 0 and 1\n",
    "post_s2 = post_s2 / 10000\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00934c-6c7c-4789-94e7-88cb5f48a739",
   "metadata": {},
   "source": [
    "**Can you read in the cloud probability raster and set all values less than 50 to `True`? Use the variable name `post_s2_cloud_mask` to refer to the cloud mask array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbebc42-e4eb-408c-8b4c-644144ede193",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b2d310-2cef-4b88-8773-ca86f8c4f106",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "post_s2_cloud_prob_path = os.path.join(data_path, \"s2_tc_yasa_post_event_cloud_probability.tif\")\n",
    "post_s2_cloud_prob = rxr.open_rasterio(post_s2_cloud_prob_path)\n",
    "post_s2_cloud_mask = post_s2_cloud_prob < 50\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54710e02-bdf2-482d-a99a-dd91a0623150",
   "metadata": {},
   "source": [
    "**Finally, can you apply the cloud mask to `post_s2` using the `where()` method to set all cloudy pixels to NaN (no data)?**\n",
    "\n",
    "**Remember to update the no data value in the `post_s2` `xarray.DataArray` objects metadata.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b751ff-512f-4199-b8bb-7f422f532851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcacfe05-ba9f-45d0-8e11-f8c3d6a02ac3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "# apply cloud mask\n",
    "post_s2_cloud_mask = post_s2_cloud_prob < 50\n",
    "post_s2_cm = post_s2.where(post_s2_cloud_mask.sel(band=1))\n",
    "post_s2_cm.rio.write_nodata(np.nan, encoded=True, inplace=True)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a3d42-5e41-4e8d-b0f5-be1abaf17460",
   "metadata": {},
   "source": [
    "### Spectral indices\n",
    "\n",
    "Spectral indices are mathematical combinations of spectral bands from remote sensing images to highlight features of interest on the land surface. To detect flooding associated with Tropical Cyclone Yasa we need to use a spectral index that's sensitive to the presence of water and moisture. We will compute the normalised difference water index (NDWI).\n",
    "\n",
    "The NDWI is computed as:\n",
    "\n",
    "$NDWI=\\frac{Green-NIR}{Green+NIR}$\n",
    "\n",
    "The NDWI ranges from -1 to 1, with positive values corresponding to the presence of water. The NDWI uses reflectance in the green and near infrared (NIR) portions of the electromagnetic spectrum. NIR light is absorbed by water and relatively more green light is reflected. It is this contrast between green and NIR reflectance that highlights water and moisture in images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5bd1a-3e13-4f33-a866-722af010ae40",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Computing the NDWI is a local map algebra operation, the NDWI is computed for each pixel in turn. Can you compute the NDWI using the arrays referenced by `pre_s2_cm` and `post_s2_cm`?**\n",
    "\n",
    "**Assign the results to the variables `pre_s2_ndwi` and `post_s2_ndwi`. You will need to use the <a href=\"https://docs.xarray.dev/en/latest/generated/xarray.DataArray.sel.html\" target=\"_blank\">`.sel()` method</a> to select the bands that correspond to green and NIR reflectance. Green is band 2 and NIR is band 4.**\n",
    "\n",
    "**If you need help using `sel()`, look at how it is used earlier in this notebook or refer to the xarray documentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee5ba7-9497-424f-b897-d383d643f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec651900-2890-41c0-8065-ed532c8d6d01",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "pre_s2_ndwi = (pre_s2_cm.sel(band=2) - pre_s2_cm.sel(band=4)) / (pre_s2_cm.sel(band=2) + pre_s2_cm.sel(band=4))\n",
    "post_s2_ndwi = (post_s2_cm.sel(band=2) - post_s2_cm.sel(band=4)) / (post_s2_cm.sel(band=2) + post_s2_cm.sel(band=4))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24debfc0-8631-40ce-867c-7145af104d77",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Why do we need to use the `sel()` method to select the green and NIR bands?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "The NDWI equation subtracts, adds, and divides two dimensional arrays (or rasters) (i.e. pixel wise subtracting the NIR array from the green array, pixel wise adding the green array and NIR array). The Sentinel-2 images that we're working with here have four bands (blue, green, red, and NIR). We need to convert the three dimensional four band arrays into two, two dimensional arrays to compute the NDWI. The `sel()` method lets us select a band to subset returning a two dimensional array. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dac365-bba8-4fb0-a24d-f49f3a2ea66e",
   "metadata": {},
   "source": [
    "Let's check the NDWI was computed correctly by visualising the data. The values should be between -1 and 1, and higher values should correspnd to the presence of water. The ocean and rivers should appear in blue shades as we're using a green to blue colour palette here (low NDWI values are mapped to green shades and high NDWI values are mapped to blue shades). \n",
    "\n",
    "Note, here we use the `sel()` method again, but we're subsetting along the time dimension. As there is only one period along the time dimension, we're effectively removing the time informaton from the `xarray.DataArray` object to leave a two dimensional array for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012a7d0-04de-469a-8b3d-02a6a6819f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s2_ndwi.sel(time=\"2020-10-25\").plot.imshow(cmap=\"GnBu\", robust=True, aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92304a-bc37-42d4-af26-3580c1845ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_s2_ndwi.sel(time=\"2020-12-19\").plot.imshow(cmap=\"GnBu\", robust=True, aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37747c8e-566c-4e49-ac19-8c2293d7615a",
   "metadata": {},
   "source": [
    "#### Change detection \n",
    "\n",
    "Change detection in the context of remote sensing image analysis is comparing two or more remote sensing images from different dates to detect change on the Earth's land surface. Change detection can be implemented as a local map algebra operation -  comparing change in pixels values across time where a large change in values indicates change in land surface conditions. \n",
    "\n",
    "We can detect flooded locations following Tropical Cyclone Yasa using a change detection analysis by comparing the pre and post event NDWI images. A large increase in NDWI values would indicate flooding. \n",
    "\n",
    "#### Recap quiz\n",
    "\n",
    "**A simple change detection technique is to compute the difference in NDWI values for the pre and post event images. Can you do this and assign the difference image to the variable `diff_ndwi`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1d1d2-161a-41dc-b2b3-5e35d9b7366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2f259-f709-4025-a291-bbef0487d8ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "diff_ndwi = post_s2_ndwi.sel(time=\"2020-12-19\") - pre_s2_ndwi.sel(time=\"2020-10-25\")\n",
    "diff_ndwi\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9523717-19c7-42ad-b2a3-2471f1fa7fbb",
   "metadata": {},
   "source": [
    "Let's plot the difference image. We'll use the `\"Blues\"` colour palette so areas of an increase in water should appear as darker blue shades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f96b7-8d15-4586-8c6b-63fefd88ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ndwi.plot.imshow(cmap=\"Blues\", robust=True, aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cecab8-29fd-418d-b399-1537e7a6cbc5",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**Can you consider a strength and limitation of using the `\"Blues\"` colour palette to viualise change in NDWI values?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "<p><b>strength:</b> change indicating increasing NDWI values are rendered in darker bluer shades which intuitively looks wetter.</p>\n",
    "<p><b>weakness:</b> it is a sequential colour palette of blue shades from decreasing to increasing change in NDWI values. The sequential palette does not obviously convey both increasing and decreasing change values around zero. We could use a diverging palette with increasing change in NDWI represented by blue shades and decreasing change in NDWI represented by red shades.</p>\n",
    "\n",
    "<h4>Recap quiz</h4>\n",
    "\n",
    "<b>Here are the docs for <a href=\"https://docs.xarray.dev/en/stable/generated/xarray.plot.imshow.html\" target=\"_blank\"><code>plot.imshow()</code></a> and <a href=\"https://matplotlib.org/stable/gallery/color/colormap_reference.html\" target=\"_blank\"><code>Matplotlib's colour palettes</code></a>. Can you use these docs to render the NDWI difference image with a diverging colour palette centred on zero?</b>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94528609-de40-427f-8907-99b94799c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1288f6f-3838-4a41-90d3-c8ac0a3e5839",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "diff_ndwi.plot.imshow(cmap=\"RdBu\", robust=True, center=0, aspect=3, size=4)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35c24a-3fdf-4681-acf2-807cc0b71ba1",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**What are some of the challenges / limitations to using the difference as a change detection metric?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "The magnitude can be influenced by how reflective a surface is or the illumination conditions when the image was captured. For example, a small absolute difference in spectral reflectance for a surface that does not reflect much might be a large relative change. Similarly, if one of the pre or post image capture conditions was brighter, then that could amplify or attenuate the difference signal. Therefore, often relative change is used as a change detection metric. \n",
    "</details>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<a href=\"https://pro.arcgis.com/en/pro-app/latest/tool-reference/image-analyst/compute-change-raster.htm\" target=\"_blank\">ESRI provide a formula for computing the relative change</a> between two images:\n",
    "\n",
    "$relative change=\\frac{Post - Pre}{max(Post, Pre)}$\n",
    "\n",
    "The difference between the post event and pre event NDWI images is divided by the maximum NDWI pixel value considering both the pre and post images. Let's compute the relative change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ab553-f228-4371-9606-201f06614f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_change_ndwi = diff_ndwi / np.maximum(post_s2_ndwi.sel(time=\"2020-12-19\"), pre_s2_ndwi.sel(time=\"2020-10-25\"))\n",
    "rel_change_ndwi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8db290-b5b1-46d1-bf7f-011bbbb7e49d",
   "metadata": {},
   "source": [
    "Let's plot the relative change map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241a473-e4c7-4314-ac8e-7ecdde74c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_change_ndwi.plot.imshow(cmap=\"RdBu\", robust=True, center=0, aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2cd4d-ce88-4567-921e-b736354793ac",
   "metadata": {},
   "source": [
    "### Flood extent maps\n",
    "\n",
    "Here, we'll use a simple threshold to classify pixels as flooded or not-flooded. To start with, let's use a simple threshold from visual inspection of the data.\n",
    "\n",
    "Let's use a change threshold of an increase in NDWI of greater than 1 to represent flooded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4271ca2-95de-4f21-81d0-9f7a03c486a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_map = rel_change_ndwi > 1\n",
    "flood_map.plot.imshow(cmap=\"Blues\", aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e16e6-dbef-45f6-9569-34e046894114",
   "metadata": {},
   "source": [
    "## Geometry operations\n",
    "\n",
    "### Vector geometry operations\n",
    "\n",
    "*Vector geometry operations transform the shape, size, and / or projection of vector datasets. They're applied to the geometries as opposed to the dataset attributes.*. \n",
    "\n",
    "We can load in a \"ground truth\" flood map prepared by UNITAR and compare their flood maps to our change image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e6f51-15ec-4c04-b5f3-2ff3afa41bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unitar_yasa_gdf = gpd.read_file(os.path.join(data_path, \"st1_20201219_floodextent_labasa.geojson\"))\n",
    "unitar_yasa_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab768ded-3374-4cb9-ae3c-e0485e1ca1e3",
   "metadata": {},
   "source": [
    "The `xarray.DataArray` object `flood_map` stores a raster representation of the flooded extents (i.e. areas where there was a large relative change in NDWI after Tropical Cyclone Yasa made landfall). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0196e4-17e7-451d-8d56-3b995ce0ebff",
   "metadata": {},
   "source": [
    "However, it would be good to overlay the vector geometries of \"ground truth\" flood extents on our NDWI difference image see if our choice of threshold is sensible. To do this we'll need to make sure the coordinate reference systems (CRS) for our raster data and vector data match. Let's check their CRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a139763-3f00-46cc-9044-cae0e83f85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The CRS of the raster data is: {rel_change_ndwi.rio.crs}\")\n",
    "print(f\"The CRS of the vector data is: {unitar_yasa_gdf.crs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbcde5-047c-4fe0-9763-75cdfbb05109",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**By executing `unitar_yasa_gdf.crs`, are we accessing an attribute or a method?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "An attribute. Specifically we are accessing the <code>crs</code> attribute of a <code>GeoDataFrame</code> object. The <code>crs</code> attribute is self describing, it stores the coordinate reference system that the geometry information in the <code>GeoDataFrame</code> corresponds to.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f1108-1794-460e-9793-8052bf5661da",
   "metadata": {},
   "source": [
    "The CRS of the vector data is EPSG:4326 which uses latitude and longitude to identify locations on the Earth's surface. The CRS of the raster data is EPSG:32760 which is UTM zone 60S - a projected CRS. This means the raster data has been projected onto a flat surface. Let's convert the vector data to match the raster data's CRS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31cca6-0932-4a5f-8a4b-441f248627cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unitar_yasa_gdf_epsg_32760 = unitar_yasa_gdf.to_crs(\"EPSG:32760\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3510f7-bb8a-4c5d-8231-5b998c7e5dce",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**By executing `unitar_yasa_gdf.to_crs(\"EPSG:32760\")`, are we accessing an attribute or a method?**\n",
    "\n",
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "A method. We can see it is a method by the parentheses following <code>to_crs()</code> and the fact \"to\" implies that we are doing something. Specifically, here, we are converting our geometry data to another CRS. \n",
    "</details>\n",
    "\n",
    "By executing `to_crs()` we are performing a **geometry data transformation** operation. We are converting the geometry information (i.e. coordinates defining the polygons of flood extents) from one CRS to another. \n",
    "\n",
    "Now, let's plot our raster NDWI difference image and vector ground truth flood extents on the same map. You can see the correspondance between the dark blue shades of flooding detected using our remote sensing data and the light blue polygons which are the \"ground truth\" flood extents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98971957-bd95-4bab-9792-d63bd9a6ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "flood_map.plot.imshow(\n",
    "    cmap=\"Blues\",\n",
    "    ax=ax,\n",
    ")\n",
    "    \n",
    "unitar_yasa_gdf_epsg_32760.plot(\n",
    "    color=\"None\",\n",
    "    edgecolor=\"cyan\",\n",
    "    linewidth=0.25,\n",
    "    ax=ax,\n",
    "    zorder=4,\n",
    ")\n",
    "    \n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562056f5-b588-4651-9a2f-68e795b7c865",
   "metadata": {},
   "source": [
    "## Raster geometry operations\n",
    "\n",
    "*Raster geometry operations transform the shape, resolution, size, and / or projection of raster datasets. They're applied to the pixel geometries as opposed to the dataset attributes (i.e. pixel values).* \n",
    "\n",
    "The final task we need to perform is to estimate the area of cropland that was flooded. To do this we need a map of cropland extents. We can read in a 10 m spatial resolution land cover map extracted from the <a href=\"https://gee-community-catalog.org/projects/esrilc2020/\" target=\"\">ESRI 2020 Global Land Use Land Cover from Sentinel-2 dataset</a>. \n",
    "\n",
    "In the ESRI land cover map, each pixel is assigned an integer value that corresponds to a land cover class. Cropland is class value 5. Let's read in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149c059-bf5a-4ae5-aad5-7321b0a2ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "esri_lulc_path = os.path.join(data_path, \"esri_lulc_2020.tif\")\n",
    "esri_lulc = rxr.open_rasterio(esri_lulc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8c4a8-24cc-481e-b08d-5bb80c9cca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "esri_lulc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd95c4-c513-4255-b4bf-99078f3ab168",
   "metadata": {},
   "source": [
    "Let's visualise the ESRI land cover map. Don't worry about the fiddly code to draw the legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420389e-cfc5-4a70-910a-bd3d09e999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "esri_lulc_colours = [\n",
    "    \"#1A5BAB\",\n",
    "    \"#358221\",\n",
    "    \"#A7D282\",\n",
    "    \"#87D19E\",\n",
    "    \"#FFDB5C\",\n",
    "    \"#EECFA8\",\n",
    "    \"#ED022A\",\n",
    "    \"#EDE9E4\",\n",
    "    \"#F2FAFF\",\n",
    "    \"#C8C8C8\",\n",
    "  ]\n",
    "\n",
    "esri_lulc_classes = [\n",
    "    \"water\", \n",
    "    \"trees\", \n",
    "    \"grass\", \n",
    "    \"flooded vegetation\", \n",
    "    \"crops\", \n",
    "    \"scrub/shrub\", \n",
    "    \"build area\", \n",
    "    \"bare ground\", \n",
    "    \"snow/ice\", \n",
    "    \"clouds\",\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "cmap = matplotlib.colors.ListedColormap(esri_lulc_colours)\n",
    "esri_lulc.sel(band=1).plot.imshow(cmap=cmap, ax=ax, add_colorbar=False)\n",
    "\n",
    "legend_labels = [f'{i+1}: {esri_lulc_classes[i]}' for i in range(len(esri_lulc_classes))]\n",
    "ax.legend(handles=[plt.Line2D([0], [0], marker=\"s\", color=\"w\", markerfacecolor=f\"{esri_lulc_colours[i]}\", markersize=10, label=label) for i, label in enumerate(legend_labels)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a165f-dae2-480b-bff7-200b3a48b74b",
   "metadata": {},
   "source": [
    "To identify flooded croplands, we'll need to create a cropland mask that we can apply to our flood map (a similar process to cloud masking). This will require us to create a binary cropland `True` or `False` layer that has the same shape and CRS as our flood map. First, let's check the shape of our flood map and our land cover map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f098b-ff06-47a2-a13f-375b8535e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the flood map is: {flood_map.shape}\")\n",
    "print(f\"The shape of the land cover map is: {esri_lulc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06aefc7-50dd-4c11-b3c5-1de43024049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The CRS of the flood map is: {flood_map.rio.crs}\")\n",
    "print(f\"The CRS of the land cover map is: {esri_lulc.rio.crs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8b6fd-acae-4b0f-804d-00706e45f1f5",
   "metadata": {},
   "source": [
    "We can see that the shape and CRS of the land cover map does not match the flood map. Therefore, we'll need to perform a range of raster geometry operations to change the shape and dimensions of the land cover data. First, we'll need to reproject the land cover map to the CRS EPSG:32760 which can change the shape of the array (here we're moving from a spherical to a flat projection). Then, we need to clip the land cover map to the extent of the flood map. And, finally, we need to resample the land cover map so it has the same shape as the flood map. Helpfully, the rioxarray package has a <a href=\"\" target=\"_blank\">`reproject_match()` method</a> that handles all of these steps for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fb549-4d69-4174-8441-102445ea987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "esri_lulc_match = esri_lulc.rio.reproject_match(flood_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b908a28-354c-4011-ac42-a6126c87b7e2",
   "metadata": {},
   "source": [
    "Let's check this worked OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce18ba5-5e0c-43b0-be83-533db718b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the flood map is: {flood_map.shape}\")\n",
    "print(f\"The shape of the land cover map is: {esri_lulc_match.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03f82e-61bb-483b-849b-deb7eb7c6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The CRS of the flood map is: {flood_map.rio.crs}\")\n",
    "print(f\"The CRS of the land cover map is: {esri_lulc_match.rio.crs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1330a14-d15e-4fdc-942f-4db98ba1688d",
   "metadata": {},
   "source": [
    "Now, let's create a cropland mask. Cropland is represented by the pixel value 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ed44e-d44d-4ec7-b77e-3f4de20ac6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland = esri_lulc_match == 5\n",
    "cropland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02b140-a118-4f96-ae5a-a816c48936b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropland.sel(band=1).plot.imshow(cmap=\"Greens\", aspect=3, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b48aa3-45e5-4864-9d67-6d4cb34d6297",
   "metadata": {},
   "source": [
    "#### Recap quiz\n",
    "\n",
    "**As a final exercise, can you use the cropland mask to create a flood map of only flooded croplands? Use the cloud masking example as a template for how to do this.**\n",
    "\n",
    "**Can you compute the area of cropland that is flooded? You can do this by summing the number of flooded cropland pixels and multiplying the result by 100 (each pixel is 10 m x 10 m).**\n",
    "\n",
    "You can find information aggregation of `xarray.DataArray` objects <a href=\"https://docs.xarray.dev/en/latest/user-guide/computation.html#agg\" target=\"_blank\">here</a>. Is recommended you use the <a href=\"https://docs.xarray.dev/en/latest/generated/xarray.DataArray.sum.html\" target=\"_blank\">`sum()` method</a> for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4faab-d54d-4851-a1c9-3cfb6cc1c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e77f4-e559-420d-bcd2-3ba0bb435785",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>answer</b></summary>\n",
    "\n",
    "```python\n",
    "flooded_croplands = flood_map.where(cropland)\n",
    "print(f\"The area of flooded croplands is {(flooded_croplands.sum() * 100).values} square metres\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2751c-88d0-4a0f-8bae-d8c5e75aed18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
