# Verifying and checking AI outputs

It's important to verify and check the responses generated by AI tools, including large language models (LLMs). LLMs are probabilistic; they can generate different responses to the same prompt which may be more or less correct. LLMs can also hallucinate and <a href="https://www.promptingguide.ai/risks/factuality" target="_blank">confidently give you wrong answers</a>. 

There are various techniques you can deploy to improve the quality (correctness and relevance) of AI generated responses. Many of these are outlined in the notes on [prompt engineering](prompt-engineering.md). However, you will still need to check the responses generated by AI before you use them. The following sections outline some strategies and techniques you can use to verify AI generated responses.  

### Iteratively work with AI to develop solutions

You will always need to implement some form of final checking of generated responses (as you should with all workflows), ultimately this will come down to a form of manual verification. However, you should not be passively generating AI content in your studies. You should be iteratively working with the AI as assistant to solve problems, to further your understanding of topics and concepts and to generate new ideas. Thus, through your active engagement with the AI tool as you solve problem you should be familiar with the LLM's response and be refining it to be more accurate and relevant for your task. 

### Code tests

In programming, tests verify that code behaves as expected and generates correct answers. It is best practice to implement some form of testing strategy for any code you write or use. This principle extends to working with AI generated outputs; you can manually create tests for code generated by AI tools. For example, if your code comprises a function that computes monthly precipitation totals your test could compare the function's result to a known value using a small curated dataset. Read more about testing <a href="https://realpython.com/python-testing" target="_blank">here</a>.

A more efficient, but potentially riskier approach, is to prompt the LLM to generate a comprehensive list of tests for code it has generated. Use [prompt engineering](prompt-engineering.md) techniques to help the model generate good quality and relevant tests (this could be a good task for "thinking" models that encourage the LLM to think carefully about generating effective tests for the specific code in question). This technique has the advantage of quickly generating a large number of tests, which can be necessary for complex programs or analyses. The obvious disadvantage is that the tests are generated by an LLM, and so are susceptible to error too.

### Search 

As discussed in the notes on [AI tools](overview-and-tools.md#search), many LLMs are integrated with web search. This can improve response quality by allowing the LLM to combine knowledge from its internal weights (i.e. what it learnt during training) with information it retrieves from the internet while handling your prompt. The information from online sources can help keep the generated responses grounded with factual content, mitigating hallucinations, and aligned with up-to-date information. The LLMs provide links to the sources used; this makes it easy for you to click through and manually review the LLM generated response against these sources (and also undertake your own appraisal of the source quality).

Google's Gemini app gives you a *Double-check response* option (found by clicking the three vertical dots at the bottom of the response). This will run a Google search and highlight your response in green, orange or red depending on whether supporting content was found online. <a href="https://chatgpt.com" target="_blank">ChatGPT</a> and <a href="https://www.perplexity.ai" target="_blank">Perplexity</a> also both integrate search with LLM prompts and providing grounding and links to help you check the response.

### Grounding

Integrating LLM response generation with search is one form of <a href="https://techcommunity.microsoft.com/blog/fasttrackforazureblog/grounding-llms/3843857" target="_blank">grounding</a>. LLMs are general reasoning, problem solving and text generation tools. While these models will have picked up information during their training, <a href="https://techcommunity.microsoft.com/blog/fasttrackforazureblog/grounding-llms/3843857" target="_blank">they are not databases, have not been optimised for your specific task and may not have access to information relevant to the task (e.g. private or more recent data)</a>. 

When creating your prompts, consider how you can provide the model with good quality and relevant information to ground its generated response. This could be through document uploads, dataset uploads, incorporating example datasets, sharing videos or uploading images. **Check the copyright and licensing of any datasets you use for grounding**.

### Picking the "right" model

Generally, the most commonly used models, such as the LLMs released by OpenAI, Google or Meta, are highly performant across a range of tasks. However, they're constantly improving and adding new features (e.g. the <a href="https://huggingface.co/blog/Kseniase/testtimecompute" target="_blank">"thinking" capabilities is a relatively new release for these models, as of February 2025</a>). However, it's important to be aware of the different features associated with using different models and using the right model for your task to get the best outcomes. 

LLMs are great base models, they can be fine-tuned for different downstream tasks such as <a href="https://huggingface.co/stabilityai/stable-code-instruct-3b" target="_blank">coding</a> or <a href="https://openai.com/index/openai-o3-mini/" target="_blank">reasoning</a> or question-answering. You can find a comprehensive list of models on <a href="https://huggingface.co/models" target="_blank">Hugging Face</a> and you can review their model cards to see what tasks they're well suited for. 

There are a range of benchmark datasets used for evaluating and comparing LLMs across different tasks (e.g. maths, reasoning, conversation, visual understanding, coding). You can check a model's score on benchmarks relevant for the task in question or use <a href="https://lmarena.ai" target="_blank">leaderboards</a> to compare models. 
